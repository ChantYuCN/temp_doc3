[2020-06-11 08:36:13,191] 1544 WARNING MainThread ssh.get_active_controller:: No ssh client found for no_name
[2020-06-11 08:36:13,191] 151  INFO  MainThread ssh.connect :: Attempt to connect to host - 172.16.130.249
[2020-06-11 08:36:14,727] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:36:14,814] 426  DEBUG MainThread ssh.expect  :: Output:  
controller-0:~$ 
[2020-06-11 08:36:14,814] 183  INFO  MainThread ssh.connect :: Login successful!
[2020-06-11 08:36:14,814] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:36:14,865] 426  DEBUG MainThread ssh.expect  :: Output: [Kcontroller-0:~$ 
[2020-06-11 08:36:14,936] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:36:17,940] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:36:17,940] 304  DEBUG MainThread ssh.send    :: Send 'unset PROMPT_COMMAND'
[2020-06-11 08:36:18,026] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:36:18,026] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:36:18,026] 304  DEBUG MainThread ssh.send    :: Send 'export TMOUT=0'
[2020-06-11 08:36:18,146] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:36:18,147] 1637 INFO  MainThread ssh.set_active_controller:: Active controller client for no_name is set. Host ip/name: 172.16.130.249
[2020-06-11 08:36:18,147] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:36:18,147] 304  DEBUG MainThread ssh.send    :: Send 'cat /etc/platform/openrc'
[2020-06-11 08:36:18,234] 426  DEBUG MainThread ssh.expect  :: Output: 
unset OS_SERVICE_TOKEN

export OS_ENDPOINT_TYPE=internalURL
export CINDER_ENDPOINT_TYPE=internalURL

export OS_USERNAME=admin
export OS_PASSWORD=`TERM=linux /opt/platform/.keyring/19.12/.CREDENTIAL 2>/dev/null`
export OS_AUTH_TYPE=password
export OS_AUTH_URL=http://192.168.104.1:5000/v3

export OS_PROJECT_NAME=admin
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_IDENTITY_API_VERSION=3
export OS_REGION_NAME=RegionOne
export OS_INTERFACE=internal

if [ ! -z "${OS_PASSWORD}" ]; then
    export PS1='[\u@\h \W(keystone_$OS_USERNAME)]\$ '
else
    echo 'Openstack Admin credentials can only be loaded from the active controller.'
    export PS1='\h:\w\$ '
fi
controller-0:~$ 
[2020-06-11 08:36:18,234] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:36:18,342] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:36:18,343] 304  DEBUG MainThread ssh.send    :: Send 'source /etc/platform/openrc'
[2020-06-11 08:36:18,769] 426  DEBUG MainThread ssh.expect  :: Output: 
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:18,769] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:36:18,886] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:18,887] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:36:18,887] 304  DEBUG MainThread ssh.send    :: Send 'openstack --os-interface internal --os-region-name RegionOne endpoint list'
[2020-06-11 08:36:20,106] 426  DEBUG MainThread ssh.expect  :: Output: 
+----------------------------------+-----------+--------------+-----------------+---------+-----------+-------------------------------+
| ID                               | Region    | Service Name | Service Type    | Enabled | Interface | URL                           |
+----------------------------------+-----------+--------------+-----------------+---------+-----------+-------------------------------+
| 1334d559895748059dfa0e84fdc3bfe3 | RegionOne | fm           | faultmanagement | True    | admin     | http://192.168.104.1:18002    |
| 47a8a60bd0a04fa49afe6b5d9c3b40db | RegionOne | fm           | faultmanagement | True    | internal  | http://192.168.104.1:18002    |
| e924ed747c06477e8389884c11f27a7b | RegionOne | fm           | faultmanagement | True    | public    | http://172.16.130.249:18002   |
| 8b0c51548386427f97c32c9db5fb4db6 | RegionOne | patching     | patching        | True    | admin     | http://192.168.104.1:5491     |
| 9ca25e3f2ea046cf9a8c334611d5e017 | RegionOne | patching     | patching        | True    | internal  | http://192.168.104.1:5491     |
| 36c7f73f53544dc6bdf7c640ae9f5c54 | RegionOne | patching     | patching        | True    | public    | http://172.16.130.249:15491   |
| 03d275a9f1f44aa2bd36e51d53061b65 | RegionOne | vim          | nfv             | True    | admin     | http://192.168.104.1:4545     |
| 695d33221f1d4c97bcb7d29f2f40a2c9 | RegionOne | vim          | nfv             | True    | internal  | http://192.168.104.1:4545     |
| c0e36c6346894085b48b9fb3452e2f93 | RegionOne | vim          | nfv             | True    | public    | http://172.16.130.249:4545    |
| 2dc18f72771b4a6db6fe5e724858d19e | RegionOne | smapi        | smapi           | True    | admin     | http://192.168.104.1:7777     |
| 846e05377eed49079080bcad1a9ec83f | RegionOne | smapi        | smapi           | True    | internal  | http://192.168.104.1:7777     |
| fe8028a9ef9c4c71aa3f0a775f640de3 | RegionOne | smapi        | smapi           | True    | public    | http://172.16.130.249:7777    |
| a5bf601b82a84199ab0bf2d828a16b17 | RegionOne | keystone     | identity        | True    | admin     | http://192.168.104.1:5000/v3  |
| 8e98b4a9bc774980a58f00764ca454dd | RegionOne | keystone     | identity        | True    | internal  | http://192.168.104.1:5000/v3  |
| bfc26918df33404cb71b6dcbc7b4d419 | RegionOne | keystone     | identity        | True    | public    | http://172.16.130.249:5000/v3 |
| 3ef8fe1a9009441084fe6a80356e9c57 | RegionOne | barbican     | key-manager     | True    | admin     | http://192.168.104.1:9311     |
| 7d8525c308924e0baafd70209d4330f7 | RegionOne | barbican     | key-manager     | True    | internal  | http://192.168.104.1:9311     |
| 87f32559b7794042818113b52627ef02 | RegionOne | barbican     | key-manager     | True    | public    | http://172.16.130.249:9311    |
| f0c3484970154b9b9a8933c9bf094e21 | RegionOne | sysinv       | platform        | True    | admin     | http://192.168.104.1:6385/v1  |
| 7be457b874574dcca9962744444535cc | RegionOne | sysinv       | platform        | True    | internal  | http://192.168.104.1:6385/v1  |
| b7ae36db3f1643728b45b07cc6f0e01a | RegionOne | sysinv       | platform        | True    | public    | http://172.16.130.249:6385/v1 |
+----------------------------------+-----------+--------------+-----------------+---------+-----------+-------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:20,107] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:36:20,226] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:20,227] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:36:20,227] 304  DEBUG MainThread ssh.send    :: Send 'unset OS_REGION_NAME'
[2020-06-11 08:36:20,336] 426  DEBUG MainThread ssh.expect  :: Output: 
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:20,336] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:36:20,415] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:20,415] 575  DEBUG MainThread table_parser.get_values:: Returning matching URL value(s): ['http://172.16.130.249:5000/v3']
[2020-06-11 08:36:20,939] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/horizon/test_hosts.py::test_horizon_host_inventory_display
[2020-06-11 08:36:20,940] 19   INFO  MainThread conftest.setup_test_session:: 
====================== Setup Step 1: (session) Setting up test session...
[2020-06-11 08:36:20,940] 67   INFO  MainThread setups.setup_primary_tenant:: Primary Tenant for test session is set to admin
[2020-06-11 08:36:20,941] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:36:20,941] 304  DEBUG MainThread ssh.send    :: Send 'cat /etc/build.info'
[2020-06-11 08:36:21,018] 426  DEBUG MainThread ssh.expect  :: Output: 
###
### StarlingX
###     Release 19.12
###

OS="centos"
SW_VERSION="19.12"
BUILD_TARGET="Host Installer"
BUILD_TYPE="Formal"
BUILD_ID="r/stx.3.0"

JOB="STX_BUILD_3.0"
BUILD_BY="starlingx.build@cengn.ca"
BUILD_NUMBER="21"
BUILD_HOST="starlingx_mirror"
BUILD_DATE="2019-12-13 02:30:00 +0000"
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:21,018] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:36:21,116] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:21,116] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:36:21,218] 426  DEBUG MainThread ssh.expect  :: Output: 
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:21,218] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 08:36:21,218] 89   DEBUG MainThread local.connect :: Attempt to connect to localhost - forstarlingxtest
[2020-06-11 08:36:21,225] 304  DEBUG MainThread ssh.send    :: Send 'export PS1="\u@\h\$ "'
[2020-06-11 08:36:21,278] 426  DEBUG MainThread ssh.expect  :: Output: 
kroot@forstarlingxtest:~/yuchengde/test/automated-pytest-suite\root@forstarlingxtest$ 
[2020-06-11 08:36:21,278] 101  DEBUG MainThread local.connect :: Connected to localhost!
[2020-06-11 08:36:21,278] 1477 INFO  MainThread ssh.set_natbox_client:: NatBox localhost ssh client is set
[2020-06-11 08:36:21,278] 1425 INFO  MainThread ssh.get_natbox_client:: Getting NatBox Client...
[2020-06-11 08:36:21,279] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:36:21,279] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 08:36:22,166] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:22,167] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:36:22,286] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:22,287] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 08:36:22,287] 102  INFO  MainThread setups.setup_keypair:: stx-openstack is not applied. Skip nova keypair config.
[2020-06-11 08:36:22,287] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:36:22,287] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:36:23,184] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:35:50.735721+00:00                                     |
| uptime                | 4763                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:23,184] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:36:23,269] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:23,270] 124  INFO  MainThread system_helper.is_aio_system:: This is small footprint system.
[2020-06-11 08:36:23,270] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:36:23,270] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 08:36:24,203] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:24,204] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:36:24,289] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:24,289] 244  DEBUG MainThread system_helper.get_hosts:: Filtered hosts: ['controller-0']
[2020-06-11 08:36:24,290] 48   INFO  MainThread system_helper.get_sys_type:: ============= System type: AIO-SX ==============
[2020-06-11 08:36:24,290] 1536 INFO  MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:36:24,290] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:36:24,290] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 08:36:25,216] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:25,216] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:36:25,303] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:25,304] 244  DEBUG MainThread system_helper.get_hosts:: Filtered hosts: ['controller-0']
[2020-06-11 08:36:25,304] 324  INFO  MainThread setups._rsync_files_to_con1:: Less than two controllers on system. Skip copying file to controller-1.
[2020-06-11 08:36:25,305] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:36:25,305] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:36:25,305] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 08:36:26,263] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:26,264] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:36:26,348] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:26,349] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 08:36:26,349] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 2: (session) Gathering system health info before test session begins.
[2020-06-11 08:36:26,349] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:36:26,349] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:36:26,349] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 08:36:27,511] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 9d88562e-b36d-45b3-8c7a-bd21c75eae20 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T07:20:46.822110 |
| b9633fd5-c9bf-46d7-a693-ff9cd8649142 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T07:20:46.741178 |
| 6def9f78-10cd-4959-9871-626e8dafb455 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T07:20:46.579245 |
| 016eee9c-b552-4d9e-837a-b861ccde27b6 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T07:20:45.615108 |
| 60914477-4b83-4666-9b6c-211a4ab6f027 | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T07:20:45.291215 |
| c7a56ae5-5589-48f4-bb00-201a699e4aab | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T07:20:44.309158 |
| 8b9709d8-14ce-4767-80fb-510886dc0867 | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T07:20:44.228143 |
| 68dbcfeb-a80d-49a6-9e08-1262cc80f912 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T07:20:44.147103 |
| d908b5f8-9f50-46ef-bd9d-00ff2fbd880f | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T07:20:44.066161 |
| 2484cf4d-2ef4-460d-b21f-bb59950f1100 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T07:20:43.983141 |
| 4e1981c4-057f-4ff2-9fe2-adb3348de1da | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T07:20:43.902160 |
| a4aebb54-c257-4df8-b43b-7d28a8151a12 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T07:20:43.483143 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:27,511] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:36:27,611] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:36:27,612] 634  DEBUG MainThread conftest.config_host_module:: Empty config host module
[2020-06-11 08:36:27,613] 628  DEBUG MainThread conftest.config_host_class:: Empty config host class
[2020-06-11 08:36:27,613] 57   INFO  MainThread helper.get_driver:: Setting Firefox download preferences
[2020-06-11 08:37:28,234] 54   DEBUG MainThread conftest.update_results:: ***Failure at test setup: /root/yuchengde/yuchengde_py36/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py:242: selenium.common.exceptions.WebDriverException: Message: connection refused
***Details: request = <SubRequest 'driver' for <Function 'test_horizon_host_inventory_display'>>

    @fixture(scope="session")
    def driver(request):
>       driver_ = HorizonDriver.get_driver()

testfixtures/horizon.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
utils/horizon/helper.py:76: in get_driver
    driver_ = webdriver.Firefox(firefox_profile=profile)
../../yuchengde_py36/lib/python3.6/site-packages/selenium/webdriver/firefox/webdriver.py:174: in __init__
    keep_alive=True)
../../yuchengde_py36/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py:157: in __init__
    self.start_session(capabilities, browser_profile)
../../yuchengde_py36/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py:252: in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
../../yuchengde_py36/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py:321: in execute
    self.error_handler.check_response(response)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <selenium.webdriver.remote.errorhandler.ErrorHandler object at 0x7feae0d8d908>
response = {'status': 500, 'value': '{"value":{"error":"unknown error","message":"connection refused","stacktrace":"stack backtra...            at /buildslave/rust-buildbot/slave/stable-dist-rustc-musl-linux/build/src/libstd/sys/unix/thread.rs:84"}}'}

    def check_response(self, response):
        """
        Checks that a JSON response from the WebDriver does not have an error.
    
        :Args:
         - response - The JSON response from the WebDriver server as a dictionary
           object.
    
        :Raises: If the response contains an error message.
        """
        status = response.get('status', None)
        if status is None or status == ErrorCode.SUCCESS:
            return
        value = None
        message = response.get("message", "")
        screen = response.get("screen", "")
        stacktrace = None
        if isinstance(status, int):
            value_json = response.get('value', None)
            if value_json and isinstance(value_json, basestring):
                import json
                try:
                    value = json.loads(value_json)
                    if len(value.keys()) == 1:
                        value = value['value']
                    status = value.get('error', None)
                    if status is None:
                        status = value["status"]
                        message = value["value"]
                        if not isinstance(message, basestring):
                            value = message
                            message = message.get('message')
                    else:
                        message = value.get('message', None)
                except ValueError:
                    pass
    
        exception_class = ErrorInResponseException
        if status in ErrorCode.NO_SUCH_ELEMENT:
            exception_class = NoSuchElementException
        elif status in ErrorCode.NO_SUCH_FRAME:
            exception_class = NoSuchFrameException
        elif status in ErrorCode.NO_SUCH_WINDOW:
            exception_class = NoSuchWindowException
        elif status in ErrorCode.STALE_ELEMENT_REFERENCE:
            exception_class = StaleElementReferenceException
        elif status in ErrorCode.ELEMENT_NOT_VISIBLE:
            exception_class = ElementNotVisibleException
        elif status in ErrorCode.INVALID_ELEMENT_STATE:
            exception_class = InvalidElementStateException
        elif status in ErrorCode.INVALID_SELECTOR \
                or status in ErrorCode.INVALID_XPATH_SELECTOR \
                or status in ErrorCode.INVALID_XPATH_SELECTOR_RETURN_TYPER:
            exception_class = InvalidSelectorException
        elif status in ErrorCode.ELEMENT_IS_NOT_SELECTABLE:
            exception_class = ElementNotSelectableException
        elif status in ErrorCode.ELEMENT_NOT_INTERACTABLE:
            exception_class = ElementNotInteractableException
        elif status in ErrorCode.INVALID_COOKIE_DOMAIN:
            exception_class = InvalidCookieDomainException
        elif status in ErrorCode.UNABLE_TO_SET_COOKIE:
            exception_class = UnableToSetCookieException
        elif status in ErrorCode.TIMEOUT:
            exception_class = TimeoutException
        elif status in ErrorCode.SCRIPT_TIMEOUT:
            exception_class = TimeoutException
        elif status in ErrorCode.UNKNOWN_ERROR:
            exception_class = WebDriverException
        elif status in ErrorCode.UNEXPECTED_ALERT_OPEN:
            exception_class = UnexpectedAlertPresentException
        elif status in ErrorCode.NO_ALERT_OPEN:
            exception_class = NoAlertPresentException
        elif status in ErrorCode.IME_NOT_AVAILABLE:
            exception_class = ImeNotAvailableException
        elif status in ErrorCode.IME_ENGINE_ACTIVATION_FAILED:
            exception_class = ImeActivationFailedException
        elif status in ErrorCode.MOVE_TARGET_OUT_OF_BOUNDS:
            exception_class = MoveTargetOutOfBoundsException
        elif status in ErrorCode.JAVASCRIPT_ERROR:
            exception_class = JavascriptException
        elif status in ErrorCode.SESSION_NOT_CREATED:
            exception_class = SessionNotCreatedException
        elif status in ErrorCode.INVALID_ARGUMENT:
            exception_class = InvalidArgumentException
        elif status in ErrorCode.NO_SUCH_COOKIE:
            exception_class = NoSuchCookieException
        elif status in ErrorCode.UNABLE_TO_CAPTURE_SCREEN:
            exception_class = ScreenshotException
        elif status in ErrorCode.ELEMENT_CLICK_INTERCEPTED:
            exception_class = ElementClickInterceptedException
        elif status in ErrorCode.INSECURE_CERTIFICATE:
            exception_class = InsecureCertificateException
        elif status in ErrorCode.INVALID_COORDINATES:
            exception_class = InvalidCoordinatesException
        elif status in ErrorCode.INVALID_SESSION_ID:
            exception_class = InvalidSessionIdException
        elif status in ErrorCode.UNKNOWN_METHOD:
            exception_class = UnknownMethodException
        else:
            exception_class = WebDriverException
        if value == '' or value is None:
            value = response['value']
        if isinstance(value, basestring):
            if exception_class == ErrorInResponseException:
                raise exception_class(response, value)
            raise exception_class(value)
        if message == "" and 'message' in value:
            message = value['message']
    
        screen = None
        if 'screen' in value:
            screen = value['screen']
    
        stacktrace = None
        if 'stackTrace' in value and value['stackTrace']:
            stacktrace = []
            try:
                for frame in value['stackTrace']:
                    line = self._value_or_default(frame, 'lineNumber', '')
                    file = self._value_or_default(frame, 'fileName', '<anonymous>')
                    if line:
                        file = "%s:%s" % (file, line)
                    meth = self._value_or_default(frame, 'methodName', '<anonymous>')
                    if 'className' in frame:
                        meth = "%s.%s" % (frame['className'], meth)
                    msg = "    at %s (%s)"
                    msg = msg % (meth, file)
                    stacktrace.append(msg)
            except TypeError:
                pass
        if exception_class == ErrorInResponseException:
            raise exception_class(response, message)
        elif exception_class == UnexpectedAlertPresentException:
            alert_text = None
            if 'data' in value:
                alert_text = value['data'].get('text')
            elif 'alert' in value:
                alert_text = value['alert'].get('text')
            raise exception_class(message, screen, stacktrace, alert_text)
>       raise exception_class(message, screen, stacktrace)
E       selenium.common.exceptions.WebDriverException: Message: connection refused

../../yuchengde_py36/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py:242: WebDriverException
[2020-06-11 08:37:31,252] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 08:37:31,253] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:37:31,348] 426  DEBUG MainThread ssh.expect  :: Output: 
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:31,348] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 08:37:34,352] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 08:37:34,352] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:37:34,404] 426  DEBUG MainThread ssh.expect  :: Output: 
kroot@forstarlingxtest:~/yuchengde/test/automated-pytest-suite\root@forstarlingxtest$ 
[2020-06-11 08:37:34,404] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 08:37:34,404] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/horizon/test_hosts.py::test_horizon_host_inventory_display
[2020-06-11 08:37:34,405] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/horizon/test_hosts.py::test_horizon_host_inventory_display - Test Failed at test setup

[2020-06-11 08:37:34,407] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_active_controller_reject
[2020-06-11 08:37:34,408] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:37:34,408] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:37:34,408] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 08:37:35,274] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:35,275] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:37:35,397] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:35,398] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 08:37:35,399] 58   INFO  MainThread pre_checks_and_configs.no_simplex:: 
====================== Setup Step 1: (Session) Skip if Simplex
[2020-06-11 08:37:38,405] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 08:37:38,405] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:37:38,520] 426  DEBUG MainThread ssh.expect  :: Output: 
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:38,520] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 08:37:41,524] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 08:37:41,524] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:37:41,575] 426  DEBUG MainThread ssh.expect  :: Output: 
kroot@forstarlingxtest:~/yuchengde/test/automated-pytest-suite\root@forstarlingxtest$ 
[2020-06-11 08:37:41,576] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 08:37:41,576] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_active_controller_reject
[2020-06-11 08:37:41,577] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_active_controller_reject - Test Skipped
Reason: Not applicable to Simplex system

[2020-06-11 08:37:41,578] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[controller]
[2020-06-11 08:37:41,581] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 08:37:41,581] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:37:41,581] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:37:41,581] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 08:37:42,723] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 9d88562e-b36d-45b3-8c7a-bd21c75eae20 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T07:20:46.822110 |
| b9633fd5-c9bf-46d7-a693-ff9cd8649142 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T07:20:46.741178 |
| 6def9f78-10cd-4959-9871-626e8dafb455 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T07:20:46.579245 |
| 016eee9c-b552-4d9e-837a-b861ccde27b6 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T07:20:45.615108 |
| 60914477-4b83-4666-9b6c-211a4ab6f027 | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T07:20:45.291215 |
| c7a56ae5-5589-48f4-bb00-201a699e4aab | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T07:20:44.309158 |
| 8b9709d8-14ce-4767-80fb-510886dc0867 | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T07:20:44.228143 |
| 68dbcfeb-a80d-49a6-9e08-1262cc80f912 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T07:20:44.147103 |
| d908b5f8-9f50-46ef-bd9d-00ff2fbd880f | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T07:20:44.066161 |
| 2484cf4d-2ef4-460d-b21f-bb59950f1100 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T07:20:43.983141 |
| 4e1981c4-057f-4ff2-9fe2-adb3348de1da | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T07:20:43.902160 |
| a4aebb54-c257-4df8-b43b-7d28a8151a12 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T07:20:43.483143 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:42,723] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:37:42,811] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:42,812] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:37:42,812] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:37:42,812] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:37:43,059] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d5h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d5h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          94m    <none>         <none>     <none>           <none>
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:43,060] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:37:43,150] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:43,151] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:37:43,151] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:37:43,151] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 08:37:44,067] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:44,067] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:37:44,154] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:44,156] 213  DEBUG MainThread conftest.testcase_log:: 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test steps started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[controller]
[2020-06-11 08:37:44,157] 60   INFO  MainThread test_lock_unlock_host.test_lock_unlock_host:: 
====================== Test Step 1: Select a controller node from system if any
[2020-06-11 08:37:44,157] 82   INFO  MainThread test_lock_unlock_host.test_lock_unlock_host:: 
====================== Test Step 2: Lock controller host - controller-0 and ensure it is successfully locked
[2020-06-11 08:37:44,157] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:37:44,158] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:37:44,158] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:37:45,146] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:36:50.788188+00:00                                     |
| uptime                | 4763                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:45,146] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:37:45,232] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:45,233] 654  INFO  MainThread host_helper.lock_host:: Locking controller-0...
[2020-06-11 08:37:45,233] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:37:45,233] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:37:45,233] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-lock controller-0'
[2020-06-11 08:37:47,786] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+-------------------------------------------+
| Property              | Value                                     |
+-----------------------+-------------------------------------------+
| action                | none                                      |
| administrative        | unlocked                                  |
| availability          | available                                 |
| bm_ip                 | None                                      |
| bm_type               | none                                      |
| bm_username           | None                                      |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1 |
| capabilities          | {u'stor_function': u'monitor'}            |
| clock_synchronization | ntp                                       |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2      |
| config_status         | None                                      |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2      |
| console               | tty0                                      |
| created_at            | 2017-01-01T20:01:48.619012+00:00          |
| hostname              | controller-0                              |
| id                    | 1                                         |
| install_output        | text                                      |
| install_state         | None                                      |
| install_state_info    | None                                      |
| inv_state             | inventoried                               |
| invprovision          | provisioned                               |
| location              | {}                                        |
| mgmt_ip               | 192.168.104.2                             |
| mgmt_mac              | 54:b2:03:04:b9:13                         |
| operational           | enabled                                   |
| personality           | controller                                |
| reserved              | False                                     |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1 |
| serialid              | None                                      |
| software_load         | 19.12                                     |
| subfunction_avail     | available                                 |
| subfunction_oper      | enabled                                   |
| subfunctions          | controller,worker                         |
| task                  | Locking                                   |
| tboot                 | false                                     |
| ttys_dcd              | None                                      |
| updated_at            | 2020-06-11T08:36:50.788188+00:00          |
| uptime                | 4763                                      |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650      |
| vim_progress_status   | services-enabled                          |
+-----------------------+-------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:47,786] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:37:47,879] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:47,879] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'task': ''}
[2020-06-11 08:37:47,880] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:37:47,880] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:37:47,880] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:37:48,913] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking-                                                             |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:37:48.155224+00:00                                     |
| uptime                | 4763                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:48,913] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:37:48,997] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:48,998] 3593 INFO  MainThread system_helper.wait_for_host_values:: controller-0 task is Locking-.
[2020-06-11 08:37:52,001] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:37:52,002] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:37:52,002] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:37:52,927] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking-                                                             |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:37:50.862933+00:00                                     |
| uptime                | 4763                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:52,927] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:37:53,015] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:56,019] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:37:56,019] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:37:56,019] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:37:56,912] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking-                                                             |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:37:50.862933+00:00                                     |
| uptime                | 4763                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:37:56,913] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:37:57,041] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:00,045] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:38:00,045] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:38:00,045] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:38:00,933] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking-                                                             |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:37:50.862933+00:00                                     |
| uptime                | 4763                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:00,933] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:38:01,018] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:04,022] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:38:04,022] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:38:04,022] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:38:04,936] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking-                                                             |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:37:50.862933+00:00                                     |
| uptime                | 4763                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:04,936] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:38:05,057] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:08,061] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:38:08,062] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:38:08,062] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:38:08,977] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking-                                                             |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:37:50.862933+00:00                                     |
| uptime                | 4763                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:08,977] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:38:09,063] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:12,067] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:38:12,068] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:38:12,068] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:38:12,993] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking-                                                             |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:37:50.862933+00:00                                     |
| uptime                | 4763                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:12,994] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:38:13,080] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:16,084] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:38:16,085] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:38:16,085] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:38:16,976] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking-                                                             |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:37:50.862933+00:00                                     |
| uptime                | 4763                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:16,976] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:38:17,065] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:20,069] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:38:20,070] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:38:20,070] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:38:20,953] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking-                                                             |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:37:50.862933+00:00                                     |
| uptime                | 4763                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:20,953] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:38:21,038] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:24,042] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:38:24,042] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:38:24,042] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:38:24,972] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking-                                                             |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:37:50.862933+00:00                                     |
| uptime                | 4763                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:24,973] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:38:25,058] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:28,062] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:38:28,062] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:38:28,063] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:38:28,987] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking-                                                             |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:37:50.862933+00:00                                     |
| uptime                | 4763                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:28,987] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:38:29,074] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:32,077] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:38:32,078] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:38:32,078] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:38:33,020] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:38:32.467729+00:00                                     |
| uptime                | 4973                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:33,021] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:38:33,152] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:33,153] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 task has reached: 
[2020-06-11 08:38:33,153] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'task': ''}
[2020-06-11 08:38:33,153] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'administrative': 'locked'}
[2020-06-11 08:38:33,153] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:38:33,153] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:38:33,153] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:38:34,104] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:38:32.467729+00:00                                     |
| uptime                | 4973                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:34,105] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:38:34,190] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:34,190] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 administrative has reached: locked
[2020-06-11 08:38:34,191] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'administrative': 'locked'}
[2020-06-11 08:38:34,191] 714  INFO  MainThread host_helper.lock_host:: controller-0 is locked. Waiting for it to go Online...
[2020-06-11 08:38:34,191] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'availability': 'online'}
[2020-06-11 08:38:34,191] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:38:34,191] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:38:34,191] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:38:35,103] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:38:32.467729+00:00                                     |
| uptime                | 4973                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:35,104] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:38:35,190] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:35,191] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 availability has reached: online
[2020-06-11 08:38:35,191] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'availability': 'online'}
[2020-06-11 08:38:40,196] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'availability': 'online'}
[2020-06-11 08:38:40,197] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:38:40,197] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:38:40,197] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:38:41,157] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:38:32.467729+00:00                                     |
| uptime                | 4973                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:41,157] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:38:41,250] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:41,250] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 availability has reached: online
[2020-06-11 08:38:41,251] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'availability': 'online'}
[2020-06-11 08:38:41,251] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'task': ''}
[2020-06-11 08:38:41,251] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:38:41,251] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:38:41,251] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:38:42,177] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:38:32.467729+00:00                                     |
| uptime                | 4973                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:42,177] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:38:42,264] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:38:42,264] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 task has reached: 
[2020-06-11 08:38:42,264] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'task': ''}
[2020-06-11 08:38:42,265] 732  INFO  MainThread host_helper.lock_host:: Host is successfully locked and in online state.
[2020-06-11 08:39:02,285] 92   INFO  MainThread test_lock_unlock_host.test_lock_unlock_host:: 
====================== Test Step 3: Unlock controller host - controller-0 and ensure it is successfully unlocked
[2020-06-11 08:39:02,285] 827  INFO  MainThread host_helper.unlock_host:: Unlocking controller-0...
[2020-06-11 08:39:02,286] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:39:02,286] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:39:02,286] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:39:03,185] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:38:50.843296+00:00                                     |
| uptime                | 4973                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:39:03,185] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:39:03,310] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:39:03,311] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:39:03,311] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:39:04,200] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:38:50.843296+00:00                                     |
| uptime                | 4973                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:39:04,201] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:39:04,295] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:39:04,296] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:39:04,296] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 08:39:05,254] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:39:05,254] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:39:05,342] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:39:05,342] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 08:39:05,343] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:39:05,343] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:39:05,512] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg            0/1     ImagePullBackOff   0          2d5h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw            0/1     ImagePullBackOff   0          2d5h   172.16.43.10   worker-0   <none>           <none>
kube-system   calico-kube-controllers-855577b7b5-2jwg2   0/1     Pending            0          78s    <none>         <none>     <none>           <none>
kube-system   coredns-6bc668cd76-88snz                   0/1     Pending            0          95m    <none>         <none>     <none>           <none>
kube-system   coredns-6bc668cd76-ls69s                   0/1     Pending            0          78s    <none>         <none>     <none>           <none>
kube-system   rbd-provisioner-7484d49cf6-n8s9r           0/1     Pending            0          78s    <none>         <none>     <none>           <none>
kube-system   tiller-deploy-d6b59fcb-qfm76               0/1     Pending            0          78s    <none>         <none>     <none>           <none>
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:39:05,513] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:39:05,610] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:39:05,611] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:39:05,611] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-unlock controller-0'
[2020-06-11 08:39:10,374] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+-------------------------------------------+
| Property              | Value                                     |
+-----------------------+-------------------------------------------+
| action                | none                                      |
| administrative        | locked                                    |
| availability          | online                                    |
| bm_ip                 | None                                      |
| bm_type               | none                                      |
| bm_username           | None                                      |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1 |
| capabilities          | {u'stor_function': u'monitor'}            |
| clock_synchronization | ntp                                       |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2      |
| config_status         | None                                      |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2      |
| console               | tty0                                      |
| created_at            | 2017-01-01T20:01:48.619012+00:00          |
| hostname              | controller-0                              |
| id                    | 1                                         |
| install_output        | text                                      |
| install_state         | None                                      |
| install_state_info    | None                                      |
| inv_state             | inventoried                               |
| invprovision          | provisioned                               |
| location              | {}                                        |
| mgmt_ip               | 192.168.104.2                             |
| mgmt_mac              | 54:b2:03:04:b9:13                         |
| operational           | disabled                                  |
| personality           | controller                                |
| reserved              | False                                     |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1 |
| serialid              | None                                      |
| software_load         | 19.12                                     |
| subfunction_avail     | online                                    |
| subfunction_oper      | disabled                                  |
| subfunctions          | controller,worker                         |
| task                  | Unlocking                                 |
| tboot                 | false                                     |
| ttys_dcd              | None                                      |
| updated_at            | 2020-06-11T08:38:50.843296+00:00          |
| uptime                | 4973                                      |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650      |
| vim_progress_status   | services-disabled                         |
+-----------------------+-------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:39:10,375] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:39:10,497] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 08:41:40,629] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:41:43,684] 278  INFO  MainThread ssh.wait_for_disconnect:: ssh session to 172.16.130.249 disconnected
[2020-06-11 08:42:13,697] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:42:16,751] 151  INFO  MainThread ssh.connect :: Attempt to connect to host - 172.16.130.249
[2020-06-11 08:42:20,141] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:42:20,142] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:42:20,142] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:42:26,190] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:42:26,191] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:42:26,191] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:42:32,351] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:42:32,351] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:42:32,351] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:42:37,139] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:42:37,140] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:42:37,140] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:42:43,202] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:42:43,203] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:42:43,203] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:42:49,242] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:42:49,243] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:42:49,243] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:42:55,300] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:42:55,300] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:42:55,301] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:43:01,369] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:43:01,369] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:43:01,370] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:43:07,432] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:43:07,432] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:43:07,432] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:43:13,473] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:43:13,474] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:43:13,474] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:43:19,550] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:43:19,550] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:43:19,550] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:43:25,624] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:43:25,625] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:43:25,625] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:43:31,673] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:43:31,674] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:43:31,674] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:43:37,727] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:43:37,728] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:43:37,728] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:43:43,787] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:43:43,787] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:43:43,787] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:43:49,834] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:43:49,835] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:43:49,835] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:43:55,883] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:43:55,883] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:43:55,884] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:44:01,934] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:44:01,935] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:44:01,935] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:44:08,009] 231  DEBUG MainThread ssh.connect :: Login failed due to error: Could not establish connection to host
[2020-06-11 08:44:08,010] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:44:08,010] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:44:12,501] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:44:12,588] 426  DEBUG MainThread ssh.expect  :: Output:  
controller-0:~$ 
[2020-06-11 08:44:12,588] 183  INFO  MainThread ssh.connect :: Login successful!
[2020-06-11 08:44:12,588] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:44:12,639] 426  DEBUG MainThread ssh.expect  :: Output: [Kcontroller-0:~$ 
[2020-06-11 08:44:12,691] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:44:15,695] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:44:15,695] 304  DEBUG MainThread ssh.send    :: Send 'unset PROMPT_COMMAND'
[2020-06-11 08:44:15,788] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:44:15,789] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:44:15,789] 304  DEBUG MainThread ssh.send    :: Send 'export TMOUT=0'
[2020-06-11 08:44:15,887] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:44:15,887] 1637 INFO  MainThread ssh.set_active_controller:: Active controller client for no_name is set. Host ip/name: 172.16.130.249
[2020-06-11 08:44:15,888] 1296 INFO  MainThread host_helper._wait_for_openstack_cli_enable:: Waiting for system cli and subfunctions to be ready and nova cli (if stx-openstack applied) to be enabled on active controller
[2020-06-11 08:44:15,888] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:44:15,888] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 08:44:16,751] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 08:44:16,751] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:44:16,837] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 08:44:16,837] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:44:16,937] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:44:26,947] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:44:26,948] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 08:44:27,562] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 08:44:27,562] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:44:27,644] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 08:44:27,645] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:44:27,762] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:44:37,773] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:44:37,773] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 08:44:38,391] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 08:44:38,392] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:44:38,477] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 08:44:38,477] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:44:38,566] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:44:48,577] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:44:48,577] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 08:44:49,162] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 08:44:49,162] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:44:49,247] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 08:44:49,248] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:44:49,346] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:44:59,357] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:44:59,357] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 08:44:59,946] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 08:44:59,946] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:45:00,033] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 08:45:00,033] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:45:00,117] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:45:10,128] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:45:10,129] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 08:45:10,735] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 08:45:10,735] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:45:10,858] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 08:45:10,859] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:45:10,954] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:45:20,965] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:45:20,965] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 08:45:21,620] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 08:45:21,620] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:45:21,729] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 08:45:21,729] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:45:21,815] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:45:31,826] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:45:31,826] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 08:45:32,922] 426  DEBUG MainThread ssh.expect  :: Output: 
+----------------------+--------------------------------------+
| Property             | Value                                |
+----------------------+--------------------------------------+
| contact              | None                                 |
| created_at           | 2017-01-01T20:00:35.851917+00:00     |
| description          | None                                 |
| https_enabled        | False                                |
| location             | None                                 |
| name                 | 03a8f8c4-9883-4c0d-a430-63370202a9d0 |
| region_name          | RegionOne                            |
| sdn_enabled          | False                                |
| security_feature     | spectre_meltdown_v1                  |
| service_project_name | services                             |
| software_version     | 19.12                                |
| system_mode          | duplex                               |
| system_type          | All-in-one                           |
| timezone             | UTC                                  |
| updated_at           | 2020-05-12T07:22:44.454374+00:00     |
| uuid                 | 3f620fb2-4ceb-441a-b7d7-8800c6c585f3 |
| vswitch_type         | none                                 |
+----------------------+--------------------------------------+
controller-0:~$ 
[2020-06-11 08:45:32,922] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:45:33,005] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:45:43,010] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:45:43,010] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne servicegroup-list'
[2020-06-11 08:45:44,828] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+-----------------------------+--------------+--------+
| uuid                                 | service_group_name          | hostname     | state  |
+--------------------------------------+-----------------------------+--------------+--------+
| d90e8718-a42a-40e8-b4b3-868f5946be64 | cloud-services              | controller-0 | active |
| f9f2a936-48cc-427c-9486-23f51425df38 | controller-services         | controller-0 | active |
| 4d06986d-84a9-45a3-8f97-5b99663e4fe7 | directory-services          | controller-0 | active |
| 4a92b36e-dfe0-464c-b6ef-ce7786ef23f8 | oam-services                | controller-0 | active |
| c7487169-1218-4a1f-bc6c-8aa049310d1f | patching-services           | controller-0 | active |
| 3c53f730-31ef-49f5-818d-0e0453e6db21 | storage-monitoring-services | controller-0 | active |
| daf9b589-f138-4bda-93b7-fc0139d43bec | storage-services            | controller-0 | active |
| fc7baef7-fb37-44fa-a17b-30418aabbc35 | vim-services                | controller-0 | active |
| f84b88fc-4f6d-437c-bae7-025d143fd541 | web-services                | controller-0 | active |
+--------------------------------------+-----------------------------+--------------+--------+
controller-0:~$ 
[2020-06-11 08:45:44,828] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:45:44,914] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:45:44,915] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): ['controller-0']
[2020-06-11 08:45:44,915] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): []
[2020-06-11 08:45:44,916] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:45:44,916] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:45:44,916] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:45:45,902] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:45:31.757354+00:00                                     |
| uptime                | 310                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:45:45,902] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:45:45,998] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:45:45,999] 1273 INFO  MainThread host_helper.check_sysinv_cli:: Simplex system in locked state. Wait for task to clear only
[2020-06-11 08:45:46,000] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'task': ''}
[2020-06-11 08:45:46,000] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:45:46,000] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:45:47,081] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:45:31.757354+00:00                                     |
| uptime                | 310                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:45:47,082] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:45:47,166] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:45:47,166] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 task has reached: 
[2020-06-11 08:45:47,167] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'task': ''}
[2020-06-11 08:45:47,167] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:45:47,167] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 08:45:48,128] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 08:45:48,129] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:45:48,214] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:45:48,215] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 08:45:48,215] 1284 INFO  MainThread host_helper.check_sysinv_cli:: system cli and subfunction enabled
[2020-06-11 08:45:58,225] 772  INFO  MainThread host_helper._wait_for_simplex_reconnect:: Re-connected via ssh and openstack CLI enabled
[2020-06-11 08:45:58,226] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'administrative': 'unlocked'}
[2020-06-11 08:45:58,226] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:45:58,226] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:45:59,409] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:45:31.757354+00:00                                     |
| uptime                | 310                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:45:59,409] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:45:59,495] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:45:59,496] 3593 INFO  MainThread system_helper.wait_for_host_values:: controller-0 administrative is locked.
[2020-06-11 08:46:02,499] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:46:02,500] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:46:03,444] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:45:31.757354+00:00                                     |
| uptime                | 310                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:46:03,444] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:46:03,533] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:46:06,537] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:46:06,537] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:46:07,502] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:45:31.757354+00:00                                     |
| uptime                | 310                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:46:07,503] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:46:07,578] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:46:10,582] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:46:10,582] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:46:11,508] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:45:31.757354+00:00                                     |
| uptime                | 310                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:46:11,509] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:46:11,587] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:46:14,591] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:46:14,592] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:46:15,511] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:45:31.757354+00:00                                     |
| uptime                | 310                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:46:15,511] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:46:15,595] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:46:18,599] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:46:18,599] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:46:19,488] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:45:31.757354+00:00                                     |
| uptime                | 310                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:46:19,489] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:46:19,580] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:46:22,684] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:46:22,685] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:46:23,590] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:45:31.757354+00:00                                     |
| uptime                | 310                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:46:23,590] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:46:23,697] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:46:26,701] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:46:26,701] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:46:27,596] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | None                                                                 |
| config_target         | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:45:31.757354+00:00                                     |
| uptime                | 310                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:46:27,596] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:46:27,669] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:46:30,673] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:46:30,673] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:46:31,624] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:46:31.406139+00:00                                     |
| uptime                | 310                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:46:31,625] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:46:31,717] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:46:34,720] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:46:34,721] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:46:35,644] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:46:35.257704+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:46:35,644] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:46:35,725] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:46:38,729] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:46:38,729] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:46:39,693] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:46:37.361372+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:46:39,694] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:46:39,769] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:46:42,773] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:46:42,774] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:46:43,761] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:46:37.361372+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:46:43,762] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:46:43,845] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:46:46,849] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:46:46,850] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:46:47,792] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:46:37.361372+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:46:47,792] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:46:47,879] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:46:50,883] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:46:50,883] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:46:51,804] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:46:37.361372+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:46:51,805] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:46:51,910] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:46:54,913] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:46:54,914] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:46:55,846] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:46:37.361372+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:46:55,847] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:46:55,932] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:46:59,193] 54   DEBUG MainThread conftest.update_results:: ***Failure at test call: /root/yuchengde/test/automated-pytest-suite/keywords/system_helper.py:3606: utils.exceptions.TimeoutException: Request(s) timed out
***Details: host_type = 'controller'

    @mark.parametrize('host_type', [
        param('controller', marks=mark.priorities('platform_sanity',
                                                  'sanity', 'cpe_sanity')),
        param('compute', marks=mark.priorities('platform_sanity')),
        param('storage', marks=mark.priorities('platform_sanity')),
    ])
    def test_lock_unlock_host(host_type):
        """
        Verify lock unlock host
    
        Test Steps:
            - Select a host per given type. If type is controller, select
                standby controller.
            - Lock selected host and ensure it is successfully locked
            - Unlock selected host and ensure it is successfully unlocked
    
        """
        LOG.tc_step("Select a {} node from system if any".format(host_type))
        if host_type == 'controller':
            if system_helper.is_aio_simplex():
                host = 'controller-0'
            else:
                host = system_helper.get_standby_controller_name()
                assert host, "No standby controller available"
    
        else:
            if host_type == 'compute' and system_helper.is_aio_system():
                skip("No compute host on AIO system")
            elif host_type == 'storage' and not system_helper.is_storage_system():
                skip("System does not have storage nodes")
    
            hosts = system_helper.get_hosts(personality=host_type,
                                            availability=HostAvailState.AVAILABLE,
                                            operational=HostOperState.ENABLED)
    
            assert hosts, "No good {} host on system".format(host_type)
            host = hosts[0]
    
        LOG.tc_step("Lock {} host - {} and ensure it is successfully "
                    "locked".format(host_type, host))
        HostsToRecover.add(host)
        host_helper.lock_host(host, swact=False)
    
        # wait for services to stabilize before unlocking
        time.sleep(20)
    
        # unlock standby controller node and verify controller node is
        # successfully unlocked
        LOG.tc_step("Unlock {} host - {} and ensure it is successfully "
                    "unlocked".format(host_type, host))
>       host_helper.unlock_host(host)

testcases/functional/mtc/test_lock_unlock_host.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
keywords/host_helper.py:881: in unlock_host
    auth_info=auth_info):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

host = 'controller-0', timeout = 60, check_interval = 3, strict = True, regex = False, fail_ok = False, con_ssh = <utils.clients.ssh.SSHClient object at 0x7feae1f0c6a0>
auth_info = {'domain': 'Default', 'password': '99cloud@SH', 'platform': True, 'tenant': 'admin', ...}, kwargs = {'administrative': 'unlocked'}
end_time = 1591865218.2261875, last_vals = {'administrative': 'locked'}, field = 'administrative', actual_vals = {'administrative': 'locked'}, expt_vals = ['unlocked']
actual_val = 'locked', actual_val_lower = 'locked'

    def wait_for_host_values(host, timeout=HostTimeout.REBOOT, check_interval=3,
                             strict=True, regex=False, fail_ok=True,
                             con_ssh=None, auth_info=Tenant.get('admin_platform'),
                             **kwargs):
        """
        Wait for host values via system host-show
        Args:
            host:
            timeout:
            check_interval:
            strict:
            regex:
            fail_ok:
            con_ssh:
            auth_info
            **kwargs: key/value pair to wait for.
    
        Returns:
    
        """
        if not kwargs:
            raise ValueError(
                "Expected host state(s) has to be specified via "
                "keyword argument states")
    
        LOG.info("Waiting for {} to reach state(s) - {}".format(host, kwargs))
        end_time = time.time() + timeout
        last_vals = {}
        for field in kwargs:
            last_vals[field] = None
    
        while time.time() < end_time:
            actual_vals = get_host_values(host, fields=list(kwargs.keys()),
                                          con_ssh=con_ssh, rtn_dict=True,
                                          auth_info=auth_info, merge_lines=False)
            for field, expt_vals in kwargs.items():
                actual_val = actual_vals[field]
                if isinstance(actual_val, list):
                    actual_val = ' '.join(actual_val)
    
                actual_val_lower = actual_val.lower()
                if isinstance(expt_vals, str):
                    expt_vals = [expt_vals]
    
                for expected_val in expt_vals:
                    expected_val_lower = expected_val.strip().lower()
                    found_match = False
                    if regex:
                        if strict:
                            res_ = re.match(expected_val_lower, actual_val_lower)
                        else:
                            res_ = re.search(expected_val_lower, actual_val_lower)
                        if res_:
                            found_match = True
                    else:
                        if strict:
                            found_match = actual_val_lower == expected_val_lower
                        else:
                            found_match = actual_val_lower in expected_val_lower
    
                    if found_match:
                        LOG.info(
                            "{} {} has reached: {}".format(host, field, actual_val))
                        break
                else:  # no match found. run system host-show again
                    if last_vals[field] != actual_val_lower:
                        LOG.info("{} {} is {}.".format(host, field, actual_val))
                        last_vals[field] = actual_val_lower
                    break
            else:
                LOG.info("{} is in state(s): {}".format(host, kwargs))
                return True
            time.sleep(check_interval)
        else:
            msg = "{} did not reach state(s) within {}s - {}".format(host, timeout,
                                                                     kwargs)
            if fail_ok:
                LOG.warning(msg)
                return False
>           raise exceptions.TimeoutException(msg)
E           utils.exceptions.TimeoutException: Request(s) timed out
E           Details: controller-0 did not reach state(s) within 60s - {'administrative': 'unlocked'}

keywords/system_helper.py:3606: TimeoutException
[2020-06-11 08:47:02,210] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 08:47:02,210] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:47:02,301] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:47:02,301] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 08:47:05,305] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 08:47:05,305] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:47:05,357] 426  DEBUG MainThread ssh.expect  :: Output: 
kroot@forstarlingxtest:~/yuchengde/test/automated-pytest-suite\root@forstarlingxtest$ 
[2020-06-11 08:47:05,357] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 08:47:05,357] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[controller]
[2020-06-11 08:47:05,358] 115  INFO  MainThread recover_hosts._recover_hosts:: 
====================== Teardown Step 1: function Recover simplex host
[2020-06-11 08:47:05,358] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:47:05,358] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:47:05,445] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:47:05,446] 1296 INFO  MainThread host_helper._wait_for_openstack_cli_enable:: Waiting for system cli and subfunctions to be ready and nova cli (if stx-openstack applied) to be enabled on active controller
[2020-06-11 08:47:05,446] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:47:05,446] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 08:47:06,389] 426  DEBUG MainThread ssh.expect  :: Output: 
+----------------------+--------------------------------------+
| Property             | Value                                |
+----------------------+--------------------------------------+
| contact              | None                                 |
| created_at           | 2017-01-01T20:00:35.851917+00:00     |
| description          | None                                 |
| https_enabled        | False                                |
| location             | None                                 |
| name                 | 03a8f8c4-9883-4c0d-a430-63370202a9d0 |
| region_name          | RegionOne                            |
| sdn_enabled          | False                                |
| security_feature     | spectre_meltdown_v1                  |
| service_project_name | services                             |
| software_version     | 19.12                                |
| system_mode          | duplex                               |
| system_type          | All-in-one                           |
| timezone             | UTC                                  |
| updated_at           | 2020-05-12T07:22:44.454374+00:00     |
| uuid                 | 3f620fb2-4ceb-441a-b7d7-8800c6c585f3 |
| vswitch_type         | none                                 |
+----------------------+--------------------------------------+
controller-0:~$ 
[2020-06-11 08:47:06,390] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:47:06,474] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:47:16,485] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:47:16,485] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne servicegroup-list'
[2020-06-11 08:47:17,781] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+-----------------------------+--------------+--------+
| uuid                                 | service_group_name          | hostname     | state  |
+--------------------------------------+-----------------------------+--------------+--------+
| d90e8718-a42a-40e8-b4b3-868f5946be64 | cloud-services              | controller-0 | active |
| f9f2a936-48cc-427c-9486-23f51425df38 | controller-services         | controller-0 | active |
| 4d06986d-84a9-45a3-8f97-5b99663e4fe7 | directory-services          | controller-0 | active |
| 4a92b36e-dfe0-464c-b6ef-ce7786ef23f8 | oam-services                | controller-0 | active |
| c7487169-1218-4a1f-bc6c-8aa049310d1f | patching-services           | controller-0 | active |
| 3c53f730-31ef-49f5-818d-0e0453e6db21 | storage-monitoring-services | controller-0 | active |
| daf9b589-f138-4bda-93b7-fc0139d43bec | storage-services            | controller-0 | active |
| fc7baef7-fb37-44fa-a17b-30418aabbc35 | vim-services                | controller-0 | active |
| f84b88fc-4f6d-437c-bae7-025d143fd541 | web-services                | controller-0 | active |
+--------------------------------------+-----------------------------+--------------+--------+
controller-0:~$ 
[2020-06-11 08:47:17,781] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:47:17,856] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:47:17,856] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): ['controller-0']
[2020-06-11 08:47:17,856] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): []
[2020-06-11 08:47:17,857] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:47:17,857] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:47:17,857] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:47:18,771] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:46:37.361372+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:47:18,772] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:47:18,867] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:47:18,867] 1273 INFO  MainThread host_helper.check_sysinv_cli:: Simplex system in locked state. Wait for task to clear only
[2020-06-11 08:47:18,868] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'task': ''}
[2020-06-11 08:47:18,868] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:47:18,868] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:47:19,821] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 5e0781e6-6f0f-4dc9-af39-d086851cf2b2                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:46:37.361372+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:47:19,822] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:47:19,923] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:47:19,924] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 task has reached: 
[2020-06-11 08:47:19,924] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'task': ''}
[2020-06-11 08:47:19,924] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:47:19,924] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 08:47:20,920] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 08:47:20,921] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:47:21,020] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:47:21,020] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 08:47:21,021] 1284 INFO  MainThread host_helper.check_sysinv_cli:: system cli and subfunction enabled
[2020-06-11 08:47:21,021] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:47:21,021] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:47:22,009] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| config_status         | None                                                                 |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:47:21.319550+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:47:22,010] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:47:22,095] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:47:22,096] 827  INFO  MainThread host_helper.unlock_host:: Unlocking controller-0...
[2020-06-11 08:47:22,096] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:47:22,096] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:47:23,035] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| config_status         | None                                                                 |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:47:21.319550+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:47:23,036] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:47:23,133] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:47:23,134] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:47:23,134] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:47:24,120] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| config_status         | None                                                                 |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:47:21.319550+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:47:24,120] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:47:24,223] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:47:24,224] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:47:24,224] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 08:47:25,209] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 08:47:25,210] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:47:25,297] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:47:25,298] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 08:47:25,298] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:47:25,298] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:47:25,475] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS             RESTARTS   AGE     IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg            0/1     ImagePullBackOff   0          2d6h    172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw            0/1     ImagePullBackOff   0          2d6h    172.16.43.10   worker-0   <none>           <none>
kube-system   calico-kube-controllers-855577b7b5-2jwg2   0/1     Pending            0          9m38s   <none>         <none>     <none>           <none>
kube-system   coredns-6bc668cd76-88snz                   0/1     Pending            0          103m    <none>         <none>     <none>           <none>
kube-system   coredns-6bc668cd76-ls69s                   0/1     Pending            0          9m38s   <none>         <none>     <none>           <none>
kube-system   rbd-provisioner-7484d49cf6-n8s9r           0/1     Pending            0          9m38s   <none>         <none>     <none>           <none>
kube-system   tiller-deploy-d6b59fcb-qfm76               0/1     Pending            0          9m38s   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:47:25,475] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:47:25,577] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:47:25,577] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:47:25,577] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-unlock controller-0'
[2020-06-11 08:47:30,713] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+-------------------------------------------+
| Property              | Value                                     |
+-----------------------+-------------------------------------------+
| action                | none                                      |
| administrative        | locked                                    |
| availability          | online                                    |
| bm_ip                 | None                                      |
| bm_type               | none                                      |
| bm_username           | None                                      |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1 |
| capabilities          | {u'stor_function': u'monitor'}            |
| clock_synchronization | ntp                                       |
| config_applied        | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f      |
| config_status         | None                                      |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f      |
| console               | tty0                                      |
| created_at            | 2017-01-01T20:01:48.619012+00:00          |
| hostname              | controller-0                              |
| id                    | 1                                         |
| install_output        | text                                      |
| install_state         | None                                      |
| install_state_info    | None                                      |
| inv_state             | inventoried                               |
| invprovision          | provisioned                               |
| location              | {}                                        |
| mgmt_ip               | 192.168.104.2                             |
| mgmt_mac              | 54:b2:03:04:b9:13                         |
| operational           | disabled                                  |
| personality           | controller                                |
| reserved              | False                                     |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1 |
| serialid              | None                                      |
| software_load         | 19.12                                     |
| subfunction_avail     | online                                    |
| subfunction_oper      | disabled                                  |
| subfunctions          | controller,worker                         |
| task                  | Unlocking                                 |
| tboot                 | false                                     |
| ttys_dcd              | None                                      |
| updated_at            | 2020-06-11T08:47:21.319550+00:00          |
| uptime                | 373                                       |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650      |
| vim_progress_status   | services-disabled                         |
+-----------------------+-------------------------------------------+
controller-0:~$ 
[2020-06-11 08:47:30,713] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:47:30,840] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:50:00,971] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:50:04,027] 278  INFO  MainThread ssh.wait_for_disconnect:: ssh session to 172.16.130.249 disconnected
[2020-06-11 08:50:34,057] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:50:37,112] 151  INFO  MainThread ssh.connect :: Attempt to connect to host - 172.16.130.249
[2020-06-11 08:51:37,886] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:51:40,941] 206  DEBUG MainThread ssh.connect :: Login failed although no exception caught.
[2020-06-11 08:51:41,342] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:51:41,343] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:52:45,121] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:52:48,172] 206  DEBUG MainThread ssh.connect :: Login failed although no exception caught.
[2020-06-11 08:52:48,374] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
[2020-06-11 08:52:48,374] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 08:52:52,769] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:52:52,854] 426  DEBUG MainThread ssh.expect  :: Output:  
controller-0:~$ 
[2020-06-11 08:52:52,855] 183  INFO  MainThread ssh.connect :: Login successful!
[2020-06-11 08:52:52,855] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:52:52,916] 426  DEBUG MainThread ssh.expect  :: Output: [Kcontroller-0:~$ 
[2020-06-11 08:52:52,971] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:52:55,975] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:52:55,975] 304  DEBUG MainThread ssh.send    :: Send 'unset PROMPT_COMMAND'
[2020-06-11 08:52:56,059] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:52:56,060] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:52:56,060] 304  DEBUG MainThread ssh.send    :: Send 'export TMOUT=0'
[2020-06-11 08:52:56,148] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:52:56,148] 1637 INFO  MainThread ssh.set_active_controller:: Active controller client for no_name is set. Host ip/name: 172.16.130.249
[2020-06-11 08:52:56,148] 1296 INFO  MainThread host_helper._wait_for_openstack_cli_enable:: Waiting for system cli and subfunctions to be ready and nova cli (if stx-openstack applied) to be enabled on active controller
[2020-06-11 08:52:56,149] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:52:56,149] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 08:52:57,004] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 08:52:57,005] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:52:57,090] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 08:52:57,091] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:52:57,211] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:53:07,221] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:53:07,222] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 08:53:07,832] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 08:53:07,832] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:53:07,922] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 08:53:07,922] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:53:08,011] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:53:18,022] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:53:18,022] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 08:53:18,640] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 08:53:18,640] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:53:18,761] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 08:53:18,762] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:53:18,848] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:53:28,858] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:53:28,859] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 08:53:29,471] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 08:53:29,472] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:53:29,591] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 08:53:29,591] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:53:29,679] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:53:39,689] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:53:39,689] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 08:53:40,322] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 08:53:40,322] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:53:40,421] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 08:53:40,421] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 08:53:40,499] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 08:53:50,510] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:53:50,510] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 08:53:51,651] 426  DEBUG MainThread ssh.expect  :: Output: 
+----------------------+--------------------------------------+
| Property             | Value                                |
+----------------------+--------------------------------------+
| contact              | None                                 |
| created_at           | 2017-01-01T20:00:35.851917+00:00     |
| description          | None                                 |
| https_enabled        | False                                |
| location             | None                                 |
| name                 | 03a8f8c4-9883-4c0d-a430-63370202a9d0 |
| region_name          | RegionOne                            |
| sdn_enabled          | False                                |
| security_feature     | spectre_meltdown_v1                  |
| service_project_name | services                             |
| software_version     | 19.12                                |
| system_mode          | duplex                               |
| system_type          | All-in-one                           |
| timezone             | UTC                                  |
| updated_at           | 2020-05-12T07:22:44.454374+00:00     |
| uuid                 | 3f620fb2-4ceb-441a-b7d7-8800c6c585f3 |
| vswitch_type         | none                                 |
+----------------------+--------------------------------------+
controller-0:~$ 
[2020-06-11 08:53:51,651] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:53:51,725] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:54:01,735] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:54:01,736] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne servicegroup-list'
[2020-06-11 08:54:03,667] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+-----------------------------+--------------+--------+
| uuid                                 | service_group_name          | hostname     | state  |
+--------------------------------------+-----------------------------+--------------+--------+
| 082197f6-a781-4d6b-8992-d771f1ba6901 | cloud-services              | controller-0 | active |
| 17878848-a5f1-401f-a6ad-b93c64df572b | controller-services         | controller-0 | active |
| 2647e5fa-c43f-438f-b0bb-86821cb13ec6 | directory-services          | controller-0 | active |
| b1c7a7a9-ff90-429f-bccf-309aa26fef39 | oam-services                | controller-0 | active |
| 1c19d65d-1b57-480a-a560-4c39f9b68a56 | patching-services           | controller-0 | active |
| a46abe06-44f3-495d-8dbd-7271bedefeab | storage-monitoring-services | controller-0 | active |
| 24c3e5ed-0d76-422a-92da-0b2be0acaae3 | storage-services            | controller-0 | active |
| 86497da6-0538-44f2-ad79-1ff04740d5ae | vim-services                | controller-0 | active |
| d3206d06-049d-461a-aa74-8f4b2e8ff0f8 | web-services                | controller-0 | active |
+--------------------------------------+-----------------------------+--------------+--------+
controller-0:~$ 
[2020-06-11 08:54:03,667] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:54:03,769] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:54:03,769] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): ['controller-0']
[2020-06-11 08:54:03,770] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): []
[2020-06-11 08:54:03,770] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 08:54:03,770] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:54:03,770] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:54:04,781] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| config_status         | None                                                                 |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | offline                                                              |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  | Enabling Worker Service                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:53:59.883542+00:00                                     |
| uptime                | 317                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:54:04,781] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:54:04,903] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:54:04,904] 541  INFO  MainThread host_helper.wait_for_task_clear_and_subfunction_ready:: Waiting for task clear and subfunctions enable/available (if applicable) for hosts: ['controller-0']
[2020-06-11 08:54:04,905] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:54:04,905] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:54:05,869] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| config_status         | None                                                                 |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | offline                                                              |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  | Enabling Worker Service                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:53:59.883542+00:00                                     |
| uptime                | 317                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:54:05,869] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:54:05,972] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:54:15,983] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:54:15,983] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:54:16,911] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| config_status         | None                                                                 |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | offline                                                              |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  | Enabling Worker Service                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:53:59.883542+00:00                                     |
| uptime                | 317                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:54:16,911] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:54:17,063] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:54:27,074] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:54:27,075] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:54:27,987] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| config_status         | None                                                                 |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | offline                                                              |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  | Enabling Worker Service                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:53:59.883542+00:00                                     |
| uptime                | 317                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:54:27,987] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:54:28,093] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:54:38,102] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:54:38,102] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:54:38,993] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| config_status         | None                                                                 |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | offline                                                              |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  | Enabling Worker Service                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:53:59.883542+00:00                                     |
| uptime                | 317                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:54:38,993] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:54:39,078] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:54:49,089] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:54:49,090] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:54:50,062] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| config_status         | None                                                                 |
| config_target         | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | offline                                                              |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  | Enabling Worker Service                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:53:59.883542+00:00                                     |
| uptime                | 317                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:54:50,062] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:54:50,148] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:55:00,159] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:55:00,159] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:55:01,385] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 06c0cd47-9a32-4eda-bb34-d55333ba4d82                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:54:59.022013+00:00                                     |
| uptime                | 372                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:55:01,386] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:55:01,493] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:55:01,493] 557  INFO  MainThread host_helper.wait_for_task_clear_and_subfunction_ready:: Hosts task cleared and subfunctions (if applicable) are now in enabled/available states
[2020-06-11 08:55:01,494] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:55:01,494] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 08:55:02,616] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+--------------------------------------------------------------------+
| application         | version | manifest name                 | manifest file    | status   | progress                                                           |
+---------------------+---------+-------------------------------+------------------+----------+--------------------------------------------------------------------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applying | processing chart: stx-ceph-pools-audit, overall completion: 100.0% |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed                                                          |
+---------------------+---------+-------------------------------+------------------+----------+--------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:55:02,616] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:55:02,701] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:55:02,701] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 08:55:02,702] 1284 INFO  MainThread host_helper.check_sysinv_cli:: system cli and subfunction enabled
[2020-06-11 08:55:12,712] 772  INFO  MainThread host_helper._wait_for_simplex_reconnect:: Re-connected via ssh and openstack CLI enabled
[2020-06-11 08:55:12,712] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'administrative': 'unlocked'}
[2020-06-11 08:55:12,712] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:55:12,713] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:55:13,662] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| config_status         | None                                                                 |
| config_target         | 06c0cd47-9a32-4eda-bb34-d55333ba4d82                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:55:01.723815+00:00                                     |
| uptime                | 372                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:55:13,662] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:55:13,781] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:55:13,782] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 administrative has reached: unlocked
[2020-06-11 08:55:13,782] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'administrative': 'unlocked'}
[2020-06-11 08:55:13,782] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'availability': ['available', 'degraded']}
[2020-06-11 08:55:13,783] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:55:13,783] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:55:14,723] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| config_status         | None                                                                 |
| config_target         | 06c0cd47-9a32-4eda-bb34-d55333ba4d82                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:55:01.723815+00:00                                     |
| uptime                | 372                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:55:14,723] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:55:14,819] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:55:14,819] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 availability has reached: available
[2020-06-11 08:55:14,820] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'availability': ['available', 'degraded']}
[2020-06-11 08:55:14,820] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'task': ''}
[2020-06-11 08:55:14,820] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:55:14,820] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:55:15,758] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| config_status         | None                                                                 |
| config_target         | 06c0cd47-9a32-4eda-bb34-d55333ba4d82                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:55:01.723815+00:00                                     |
| uptime                | 372                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:55:15,759] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:55:15,911] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:55:15,912] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 task has reached: 
[2020-06-11 08:55:15,912] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'task': ''}
[2020-06-11 08:55:15,912] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:55:15,913] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:55:16,846] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| config_status         | None                                                                 |
| config_target         | 06c0cd47-9a32-4eda-bb34-d55333ba4d82                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:55:01.723815+00:00                                     |
| uptime                | 372                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:55:16,846] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:55:16,931] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:55:16,931] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:55:16,931] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 08:55:17,841] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 08:55:17,841] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:55:17,926] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:55:17,927] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 08:55:17,927] 1580 INFO  MainThread host_helper.wait_for_webservice_up:: Waiting for ['controller-0'] to be active for web-service in system servicegroup-list...
[2020-06-11 08:55:17,927] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:55:17,927] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne servicegroup-list'
[2020-06-11 08:55:19,176] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+-----------------------------+--------------+--------+
| uuid                                 | service_group_name          | hostname     | state  |
+--------------------------------------+-----------------------------+--------------+--------+
| 082197f6-a781-4d6b-8992-d771f1ba6901 | cloud-services              | controller-0 | active |
| 17878848-a5f1-401f-a6ad-b93c64df572b | controller-services         | controller-0 | active |
| 2647e5fa-c43f-438f-b0bb-86821cb13ec6 | directory-services          | controller-0 | active |
| b1c7a7a9-ff90-429f-bccf-309aa26fef39 | oam-services                | controller-0 | active |
| 1c19d65d-1b57-480a-a560-4c39f9b68a56 | patching-services           | controller-0 | active |
| a46abe06-44f3-495d-8dbd-7271bedefeab | storage-monitoring-services | controller-0 | active |
| 24c3e5ed-0d76-422a-92da-0b2be0acaae3 | storage-services            | controller-0 | active |
| 86497da6-0538-44f2-ad79-1ff04740d5ae | vim-services                | controller-0 | active |
| d3206d06-049d-461a-aa74-8f4b2e8ff0f8 | web-services                | controller-0 | active |
+--------------------------------------+-----------------------------+--------------+--------+
controller-0:~$ 
[2020-06-11 08:55:19,176] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:55:19,292] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:55:19,292] 1600 INFO  MainThread host_helper.wait_for_webservice_up:: Host(s) ['controller-0'] are active for web-service in system servicegroup-list
[2020-06-11 08:55:19,293] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'subfunction_oper': 'enabled', 'subfunction_avail': 'available'}
[2020-06-11 08:55:19,293] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:55:19,293] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 08:55:20,263] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 7e2e7109-df4a-4ad4-b34a-67948fd1d60f                                 |
| config_status         | None                                                                 |
| config_target         | 06c0cd47-9a32-4eda-bb34-d55333ba4d82                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T08:55:01.723815+00:00                                     |
| uptime                | 372                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 08:55:20,263] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:55:20,347] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:55:20,347] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 subfunction_oper has reached: enabled
[2020-06-11 08:55:20,347] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 subfunction_avail has reached: available
[2020-06-11 08:55:20,348] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'subfunction_oper': 'enabled', 'subfunction_avail': 'available'}
[2020-06-11 08:55:20,348] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:55:20,348] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get nodes'
[2020-06-11 08:55:20,611] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME           STATUS   ROLES    AGE   VERSION
controller-0   Ready    master   30d   v1.16.2
worker-0       Ready    <none>   29d   v1.16.2
controller-0:~$ 
[2020-06-11 08:55:20,611] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:55:20,699] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:55:20,699] 684  INFO  MainThread kube_helper.wait_for_nodes_ready:: All nodes are ready: controller-0
[2020-06-11 08:55:20,699] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 08:55:20,699] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:55:20,700] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:55:20,883] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          111m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:55:20,883] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:55:20,969] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:55:30,979] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:55:30,980] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:55:31,160] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          111m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:55:31,160] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:55:31,282] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:55:41,293] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:55:41,293] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:55:41,516] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ErrImagePull       0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          112m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:55:41,516] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:55:41,623] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:55:51,634] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:55:51,635] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:55:51,799] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ErrImagePull       0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          112m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:55:51,800] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:55:51,890] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:56:01,901] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:56:01,901] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:56:02,068] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          112m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:56:02,068] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:56:02,212] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:56:12,223] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:56:12,223] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:56:12,404] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          112m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:56:12,404] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:56:12,504] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:56:22,514] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:56:22,515] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:56:22,678] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          112m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:56:22,679] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:56:22,767] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:56:32,774] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:56:32,774] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:56:32,928] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          112m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:56:32,928] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:56:33,012] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:56:43,023] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:56:43,023] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:56:43,186] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          113m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:56:43,186] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:56:43,283] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:56:53,294] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:56:53,294] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:56:53,459] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          113m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:56:53,460] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:56:53,592] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:57:03,603] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:57:03,603] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:57:03,772] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          113m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:57:03,772] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:57:03,873] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:57:13,884] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:57:13,884] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:57:14,050] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          113m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:57:14,050] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:57:14,136] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:57:24,147] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:57:24,147] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:57:24,322] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          113m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:57:24,322] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:57:24,422] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:57:34,433] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:57:34,433] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:57:34,599] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ErrImagePull       0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          113m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:57:34,599] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:57:34,688] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:57:44,699] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:57:44,700] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:57:44,863] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ErrImagePull       0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          114m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:57:44,863] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:57:44,950] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:57:54,961] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:57:54,961] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:57:55,121] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          114m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:57:55,121] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:57:55,205] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:58:05,216] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:58:05,217] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:58:05,379] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          114m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:58:05,380] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:58:05,482] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:58:15,493] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:58:15,494] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:58:15,661] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          114m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:58:15,661] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:58:15,759] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:58:25,770] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:58:25,770] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:58:25,946] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          114m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:58:25,946] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:58:26,053] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:58:36,063] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:58:36,064] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:58:36,226] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          114m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:58:36,227] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:58:36,313] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:58:46,323] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:58:46,324] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:58:46,492] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          115m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:58:46,492] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:58:46,580] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:58:56,591] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:58:56,591] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:58:56,759] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          115m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:58:56,760] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:58:56,849] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:59:06,860] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:59:06,860] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:59:07,024] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          115m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:59:07,024] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:59:07,114] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:59:17,124] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:59:17,125] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:59:17,289] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          115m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:59:17,289] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:59:17,374] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:59:27,385] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:59:27,385] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:59:27,546] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          115m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:59:27,546] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:59:27,652] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:59:37,663] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:59:37,663] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:59:37,830] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          115m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:59:37,831] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:59:37,928] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:59:47,939] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:59:47,939] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:59:48,113] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          116m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:59:48,114] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:59:48,200] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 08:59:58,201] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 08:59:58,201] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 08:59:58,353] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          116m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 08:59:58,354] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 08:59:58,453] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:08,464] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:08,464] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:00:08,639] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          116m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:00:08,639] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:08,724] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:18,735] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:18,735] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:00:18,898] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          116m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:00:18,898] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:18,988] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:28,999] 792  WARNING MainThread kube_helper.wait_for_pods_healthy:: Some pods are not Running or Completed: {'server-pod-dep-7fc68f8c47-85drg': 'ImagePullBackOff', 'server-pod-dep-7fc68f8c47-957dw': 'ImagePullBackOff', 'coredns-6bc668cd76-88snz': 'Pending'}
[2020-06-11 09:00:29,000] 981  INFO  MainThread kube_helper.dump_pods_info:: ------- Dump pods info --------
[2020-06-11 09:00:29,000] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:29,000] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pods --all-namespaces -o wide | grep -v -e Running -e Completed'
[2020-06-11 09:00:29,162] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS             RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg            0/1     ImagePullBackOff   0          2d6h    172.16.43.9       worker-0       <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw            0/1     ImagePullBackOff   0          2d6h    172.16.43.10      worker-0       <none>           <none>
kube-system   coredns-6bc668cd76-88snz                   0/1     Pending            0          116m    <none>            <none>         <none>           <none>
controller-0:~$ 
[2020-06-11 09:00:29,162] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:29,259] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:29,259] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:29,259] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pods --all-namespaces -o wide | grep -v -e Running -e Completed -e NAMESPACE | awk '{system("kubectl describe pods -n "$1" "$2)}''
[2020-06-11 09:00:29,713] 426  DEBUG MainThread ssh.expect  :: Output: 
Name:         server-pod-dep-7fc68f8c47-85drg
Namespace:    default
Priority:     0
Node:         worker-0/192.168.104.149
Start Time:   Tue, 09 Jun 2020 02:47:21 +0000
Labels:       pod-template-hash=7fc68f8c47
              server=pod-to-pod
Annotations:  cni.projectcalico.org/podIP: 172.16.43.9/32
              k8s.v1.cni.cncf.io/networks-status:
                [{
                    "name": "chain",
                    "ips": [
                        "172.16.43.9"
                    ],
                    "default": true,
                    "dns": {}
                }]
Status:       Pending
IP:           172.16.43.9
IPs:
  IP:           172.16.43.9
Controlled By:  ReplicaSet/server-pod-dep-7fc68f8c47
Containers:
  server-container:
    Container ID:   
    Image:          gcr.io/google-samples/node-hello:1.0
    Image ID:       
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ImagePullBackOff
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bxdw5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-bxdw5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bxdw5
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 30s
                 node.kubernetes.io/unreachable:NoExecute for 30s
Events:
  Type     Reason          Age                       From               Message
  ----     ------          ----                      ----               -------
  Normal   Scheduled       <unknown>                 default-scheduler  Successfully assigned default/server-pod-dep-7fc68f8c47-85drg to worker-0
  Normal   SandboxChanged  2d6h (x5 over 2d6h)       kubelet, worker-0  Pod sandbox changed, it will be killed and re-created.
  Warning  Failed          4h22m (x559 over 2d6h)    kubelet, worker-0  Error: ErrImagePull
  Warning  Failed          57m (x586 over 2d6h)      kubelet, worker-0  Failed to pull image "gcr.io/google-samples/node-hello:1.0": rpc error: code = Unknown desc = Error response from daemon: Get https://gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Normal   Pulling         20m (x606 over 2d6h)      kubelet, worker-0  Pulling image "gcr.io/google-samples/node-hello:1.0"
  Warning  Failed          12m (x13617 over 2d6h)    kubelet, worker-0  Error: ImagePullBackOff
  Normal   BackOff         2m55s (x13658 over 2d6h)  kubelet, worker-0  Back-off pulling image "gcr.io/google-samples/node-hello:1.0"
Name:         server-pod-dep-7fc68f8c47-957dw
Namespace:    default
Priority:     0
Node:         worker-0/192.168.104.149
Start Time:   Tue, 09 Jun 2020 02:47:21 +0000
Labels:       pod-template-hash=7fc68f8c47
              server=pod-to-pod
Annotations:  cni.projectcalico.org/podIP: 172.16.43.10/32
              k8s.v1.cni.cncf.io/networks-status:
                [{
                    "name": "chain",
                    "ips": [
                        "172.16.43.10"
                    ],
                    "default": true,
                    "dns": {}
                }]
Status:       Pending
IP:           172.16.43.10
IPs:
  IP:           172.16.43.10
Controlled By:  ReplicaSet/server-pod-dep-7fc68f8c47
Containers:
  server-container:
    Container ID:   
    Image:          gcr.io/google-samples/node-hello:1.0
    Image ID:       
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ImagePullBackOff
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bxdw5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-bxdw5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bxdw5
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 30s
                 node.kubernetes.io/unreachable:NoExecute for 30s
Events:
  Type     Reason          Age                     From               Message
  ----     ------          ----                    ----               -------
  Normal   Scheduled       <unknown>               default-scheduler  Successfully assigned default/server-pod-dep-7fc68f8c47-957dw to worker-0
  Normal   SandboxChanged  2d6h (x3 over 2d6h)     kubelet, worker-0  Pod sandbox changed, it will be killed and re-created.
  Warning  Failed          2d6h                    kubelet, worker-0  Failed to pull image "gcr.io/google-samples/node-hello:1.0": rpc error: code = Unknown desc = Error response from daemon: Get https://gcr.io/v2/: dial tcp: lookup gcr.io on 192.168.104.1:53: read udp 192.168.104.149:55593->192.168.104.1:53: read: connection refused
  Warning  Failed          2d6h                    kubelet, worker-0  Failed to pull image "gcr.io/google-samples/node-hello:1.0": rpc error: code = Unknown desc = Error response from daemon: Get https://gcr.io/v2/: dial tcp: lookup gcr.io on 192.168.104.1:53: read udp 192.168.104.149:35609->192.168.104.1:53: i/o timeout
  Warning  Failed          38h (x6 over 2d2h)      kubelet, worker-0  (combined from similar events): Failed to pull image "gcr.io/google-samples/node-hello:1.0": rpc error: code = Unknown desc = Error response from daemon: Get https://gcr.io/v2/: dial tcp: lookup gcr.io on 192.168.104.1:53: read udp 192.168.104.149:41533->192.168.104.1:53: i/o timeout
  Warning  Failed          3h52m (x551 over 2d6h)  kubelet, worker-0  Failed to pull image "gcr.io/google-samples/node-hello:1.0": rpc error: code = Unknown desc = Error response from daemon: Get https://gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Normal   Pulling         102m (x590 over 2d6h)   kubelet, worker-0  Pulling image "gcr.io/google-samples/node-hello:1.0"
  Normal   BackOff         58m (x13443 over 2d6h)  kubelet, worker-0  Back-off pulling image "gcr.io/google-samples/node-hello:1.0"
  Warning  Failed          37m (x600 over 2d6h)    kubelet, worker-0  Error: ErrImagePull
  Warning  Failed          3m (x13673 over 2d6h)   kubelet, worker-0  Error: ImagePullBackOff
Name:                 coredns-6bc668cd76-88snz
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               k8s-app=kube-dns
                      pod-template-hash=6bc668cd76
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/coredns-6bc668cd76
Containers:
  coredns:
    Image:       registry.local:9001/k8s.gcr.io/coredns:1.6.2
    Ports:       53/UDP, 53/TCP, 9153/TCP
    Host Ports:  0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from coredns-token-cklq8 (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  coredns-token-cklq8:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  coredns-token-cklq8
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
                 node-role.kubernetes.io/master=
Tolerations:     CriticalAddonsOnly
                 node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 30s
                 node.kubernetes.io/unreachable:NoExecute for 30s
Events:
  Type     Reason            Age        From               Message
  ----     ------            ----       ----               -------
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules.
controller-0:~$ 
[2020-06-11 09:00:29,713] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:29,801] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:29,803] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 2: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 09:00:29,803] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:29,803] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:29,803] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:00:30,983] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| f14f0093-05c8-4f3d-a436-a73a3a57dc8a | 100.114  | NTP address 209.115.181.102 is not a valid or a reachable NTP server.                                                 | host=controller-0.ntp=209.115.181.102                                            | minor    | 2020-06-11T08:54:07.163580 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 0aa412e5-98e5-467d-b906-41a06e441f0f | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                | host=controller-0.ntp                                                            | major    | 2020-06-11T08:45:47.107912 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:00:30,984] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:31,103] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:31,104] 594  INFO  MainThread check_helper.check_alarms:: NTP alarm found, checking ntpq stats
[2020-06-11 09:00:31,104] 3397 INFO  MainThread system_helper.wait_for_ntp_sync:: Waiting for ntp alarm to clear or sudo ntpq -pn indicate unhealthy server for controller-0
[2020-06-11 09:00:31,104] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:31,104] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:31,104] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne network-list --nowrap'
[2020-06-11 09:00:32,013] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
| id | uuid                                 | name            | type            | dynamic | pool_uuid                            |
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
| 20 | 2add22bf-3fbe-4e26-8c5a-4e2851e38c63 | cluster-pod     | cluster-pod     | False   | 98b3a90a-60ee-4ab2-b173-bf439e45bf5a |
| 15 | 2d8e1d09-6ab4-4ccc-9c5c-b4d41260e8ff | mgmt            | mgmt            | True    | 6371a279-0093-4be2-9e70-341ddcc6e207 |
| 16 | 6db43265-f8f1-4720-b2f4-456eb11794ed | pxeboot         | pxeboot         | True    | 7b75fdc7-0eed-4c5a-befd-6a1919dd8d73 |
| 19 | 8241f3e5-70c2-4897-abdd-6b72ddb61243 | cluster-host    | cluster-host    | True    | 423925ae-ddda-4339-b7e1-36a11cfe2b01 |
| 18 | 99418e5a-d003-42f9-b257-631e574d2ffc | multicast       | multicast       | False   | 85d5fa1c-7875-47e2-9c56-95012dfebc71 |
| 17 | e4291b7a-ed43-48db-ab94-d3931a63d0b9 | oam             | oam             | False   | 2d416a87-a545-438c-8d9e-cad3ea022904 |
| 21 | e6e5a4a1-81a0-49c5-b038-6d6b00432646 | cluster-service | cluster-service | False   | b9c532e8-9100-4182-8a3e-9d059d632aa1 |
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
controller-0:~$ 
[2020-06-11 09:00:32,013] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:32,102] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:32,103] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:32,103] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne addrpool-show 6371a279-0093-4be2-9e70-341ddcc6e207'
[2020-06-11 09:00:33,083] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+--------------------------------------+
| Property            | Value                                |
+---------------------+--------------------------------------+
| uuid                | 6371a279-0093-4be2-9e70-341ddcc6e207 |
| name                | management                           |
| network             | 192.168.104.0                        |
| prefix              | 24                                   |
| order               | random                               |
| ranges              | ['192.168.104.1-192.168.104.254']    |
| floating_address    | 192.168.104.1                        |
| controller0_address | 192.168.104.2                        |
| controller1_address | 192.168.104.3                        |
| gateway_address     | None                                 |
+---------------------+--------------------------------------+
controller-0:~$ 
[2020-06-11 09:00:33,083] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:33,203] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:33,204] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:33,204] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:00:34,329] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| f14f0093-05c8-4f3d-a436-a73a3a57dc8a | 100.114  | NTP address 209.115.181.102 is not a valid or a reachable NTP server.                                                 | host=controller-0.ntp=209.115.181.102                                            | minor    | 2020-06-11T08:54:07.163580 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 0aa412e5-98e5-467d-b906-41a06e441f0f | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                | host=controller-0.ntp                                                            | major    | 2020-06-11T08:45:47.107912 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:00:34,330] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:34,414] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:34,415] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:34,415] 304  DEBUG MainThread ssh.send    :: Send 'hostname'
[2020-06-11 09:00:34,503] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0
controller-0:~$ 
[2020-06-11 09:00:34,503] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 09:00:34,503] 304  DEBUG MainThread ssh.send    :: Send 'sudo ntpq -pn'
[2020-06-11 09:00:34,591] 426  DEBUG MainThread ssh.expect  :: Output: 
Password: 
[2020-06-11 09:00:34,591] 304  DEBUG MainThread ssh.send    :: Send '99cloud@SH'
[2020-06-11 09:00:34,720] 426  DEBUG MainThread ssh.expect  :: Output: 
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
 192.168.104.3
                 .INIT.          16 u    -   64    0    0.000    0.000   0.000
*209.115.181.102
                 206.108.0.131    2 u   21   64  377  229.629    1.308   1.704
 94.237.64.20
                 .INIT.          16 u    -   64    0    0.000    0.000   0.000
controller-0:~$ 
[2020-06-11 09:00:34,721] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:34,823] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:34,824] 3414 INFO  MainThread system_helper.wait_for_ntp_sync:: Valid NTP alarm
[2020-06-11 09:00:34,824] 594  INFO  MainThread check_helper.check_alarms:: NTP alarm found, checking ntpq stats
[2020-06-11 09:00:34,824] 3397 INFO  MainThread system_helper.wait_for_ntp_sync:: Waiting for ntp alarm to clear or sudo ntpq -pn indicate unhealthy server for controller-0
[2020-06-11 09:00:34,824] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:34,824] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:34,824] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne network-list --nowrap'
[2020-06-11 09:00:35,741] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
| id | uuid                                 | name            | type            | dynamic | pool_uuid                            |
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
| 20 | 2add22bf-3fbe-4e26-8c5a-4e2851e38c63 | cluster-pod     | cluster-pod     | False   | 98b3a90a-60ee-4ab2-b173-bf439e45bf5a |
| 15 | 2d8e1d09-6ab4-4ccc-9c5c-b4d41260e8ff | mgmt            | mgmt            | True    | 6371a279-0093-4be2-9e70-341ddcc6e207 |
| 16 | 6db43265-f8f1-4720-b2f4-456eb11794ed | pxeboot         | pxeboot         | True    | 7b75fdc7-0eed-4c5a-befd-6a1919dd8d73 |
| 19 | 8241f3e5-70c2-4897-abdd-6b72ddb61243 | cluster-host    | cluster-host    | True    | 423925ae-ddda-4339-b7e1-36a11cfe2b01 |
| 18 | 99418e5a-d003-42f9-b257-631e574d2ffc | multicast       | multicast       | False   | 85d5fa1c-7875-47e2-9c56-95012dfebc71 |
| 17 | e4291b7a-ed43-48db-ab94-d3931a63d0b9 | oam             | oam             | False   | 2d416a87-a545-438c-8d9e-cad3ea022904 |
| 21 | e6e5a4a1-81a0-49c5-b038-6d6b00432646 | cluster-service | cluster-service | False   | b9c532e8-9100-4182-8a3e-9d059d632aa1 |
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
controller-0:~$ 
[2020-06-11 09:00:35,741] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:35,827] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:35,827] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:35,827] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne addrpool-show 6371a279-0093-4be2-9e70-341ddcc6e207'
[2020-06-11 09:00:36,806] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+--------------------------------------+
| Property            | Value                                |
+---------------------+--------------------------------------+
| uuid                | 6371a279-0093-4be2-9e70-341ddcc6e207 |
| name                | management                           |
| network             | 192.168.104.0                        |
| prefix              | 24                                   |
| order               | random                               |
| ranges              | ['192.168.104.1-192.168.104.254']    |
| floating_address    | 192.168.104.1                        |
| controller0_address | 192.168.104.2                        |
| controller1_address | 192.168.104.3                        |
| gateway_address     | None                                 |
+---------------------+--------------------------------------+
controller-0:~$ 
[2020-06-11 09:00:36,806] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:36,893] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:36,893] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:36,894] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:00:37,648] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| f14f0093-05c8-4f3d-a436-a73a3a57dc8a | 100.114  | NTP address 209.115.181.102 is not a valid or a reachable NTP server.                                                 | host=controller-0.ntp=209.115.181.102                                            | minor    | 2020-06-11T08:54:07.163580 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 0aa412e5-98e5-467d-b906-41a06e441f0f | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                | host=controller-0.ntp                                                            | major    | 2020-06-11T08:45:47.107912 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:00:37,648] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:37,739] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:37,739] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:37,739] 304  DEBUG MainThread ssh.send    :: Send 'hostname'
[2020-06-11 09:00:37,843] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0
controller-0:~$ 
[2020-06-11 09:00:37,843] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 09:00:37,843] 304  DEBUG MainThread ssh.send    :: Send 'sudo ntpq -pn'
[2020-06-11 09:00:37,971] 426  DEBUG MainThread ssh.expect  :: Output: 
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
 192.168.104.3
                 .INIT.          16 u    -   64    0    0.000    0.000   0.000
*209.115.181.102
                 206.108.0.131    2 u   24   64  377  229.629    1.308   1.704
 94.237.64.20
                 .INIT.          16 u    -   64    0    0.000    0.000   0.000
controller-0:~$ 
[2020-06-11 09:00:37,971] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:38,073] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:38,074] 3414 INFO  MainThread system_helper.wait_for_ntp_sync:: Valid NTP alarm
[2020-06-11 09:00:38,074] 594  INFO  MainThread check_helper.check_alarms:: NTP alarm found, checking ntpq stats
[2020-06-11 09:00:38,074] 3397 INFO  MainThread system_helper.wait_for_ntp_sync:: Waiting for ntp alarm to clear or sudo ntpq -pn indicate unhealthy server for controller-0
[2020-06-11 09:00:38,074] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:38,074] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:38,074] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne network-list --nowrap'
[2020-06-11 09:00:38,970] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
| id | uuid                                 | name            | type            | dynamic | pool_uuid                            |
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
| 20 | 2add22bf-3fbe-4e26-8c5a-4e2851e38c63 | cluster-pod     | cluster-pod     | False   | 98b3a90a-60ee-4ab2-b173-bf439e45bf5a |
| 15 | 2d8e1d09-6ab4-4ccc-9c5c-b4d41260e8ff | mgmt            | mgmt            | True    | 6371a279-0093-4be2-9e70-341ddcc6e207 |
| 16 | 6db43265-f8f1-4720-b2f4-456eb11794ed | pxeboot         | pxeboot         | True    | 7b75fdc7-0eed-4c5a-befd-6a1919dd8d73 |
| 19 | 8241f3e5-70c2-4897-abdd-6b72ddb61243 | cluster-host    | cluster-host    | True    | 423925ae-ddda-4339-b7e1-36a11cfe2b01 |
| 18 | 99418e5a-d003-42f9-b257-631e574d2ffc | multicast       | multicast       | False   | 85d5fa1c-7875-47e2-9c56-95012dfebc71 |
| 17 | e4291b7a-ed43-48db-ab94-d3931a63d0b9 | oam             | oam             | False   | 2d416a87-a545-438c-8d9e-cad3ea022904 |
| 21 | e6e5a4a1-81a0-49c5-b038-6d6b00432646 | cluster-service | cluster-service | False   | b9c532e8-9100-4182-8a3e-9d059d632aa1 |
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
controller-0:~$ 
[2020-06-11 09:00:38,970] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:39,057] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:39,057] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:39,057] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne addrpool-show 6371a279-0093-4be2-9e70-341ddcc6e207'
[2020-06-11 09:00:39,955] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+--------------------------------------+
| Property            | Value                                |
+---------------------+--------------------------------------+
| uuid                | 6371a279-0093-4be2-9e70-341ddcc6e207 |
| name                | management                           |
| network             | 192.168.104.0                        |
| prefix              | 24                                   |
| order               | random                               |
| ranges              | ['192.168.104.1-192.168.104.254']    |
| floating_address    | 192.168.104.1                        |
| controller0_address | 192.168.104.2                        |
| controller1_address | 192.168.104.3                        |
| gateway_address     | None                                 |
+---------------------+--------------------------------------+
controller-0:~$ 
[2020-06-11 09:00:39,955] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:40,073] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:40,074] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:40,074] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:00:40,873] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| f14f0093-05c8-4f3d-a436-a73a3a57dc8a | 100.114  | NTP address 209.115.181.102 is not a valid or a reachable NTP server.                                                 | host=controller-0.ntp=209.115.181.102                                            | minor    | 2020-06-11T08:54:07.163580 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 0aa412e5-98e5-467d-b906-41a06e441f0f | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                | host=controller-0.ntp                                                            | major    | 2020-06-11T08:45:47.107912 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:00:40,874] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:40,959] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:40,960] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:40,960] 304  DEBUG MainThread ssh.send    :: Send 'hostname'
[2020-06-11 09:00:41,058] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0
controller-0:~$ 
[2020-06-11 09:00:41,059] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 09:00:41,059] 304  DEBUG MainThread ssh.send    :: Send 'sudo ntpq -pn'
[2020-06-11 09:00:41,170] 426  DEBUG MainThread ssh.expect  :: Output: 
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
 192.168.104.3
                 .INIT.          16 u    -   64    0    0.000    0.000   0.000
*209.115.181.102
                 206.108.0.131    2 u   28   64  377  229.629    1.308   1.704
 94.237.64.20
                 .INIT.          16 u    -   64    0    0.000    0.000   0.000
controller-0:~$ 
[2020-06-11 09:00:41,170] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:41,256] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:41,257] 3414 INFO  MainThread system_helper.wait_for_ntp_sync:: Valid NTP alarm
[2020-06-11 09:00:41,257] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:41,257] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:41,257] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:00:42,163] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:00:42,163] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:42,248] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:42,249] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 09:00:42,249] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:42,249] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:42,250] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:00:42,427] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          117m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:00:42,427] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:42,511] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:42,512] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 09:00:42,512] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:42,512] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:42,512] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 09:00:42,677] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 09:00:42,677] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:42,762] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:42,763] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 09:00:43,016] 54   DEBUG MainThread conftest.update_results:: ***Failure at test teardown: /root/yuchengde/test/automated-pytest-suite/keywords/kube_helper.py:796: utils.exceptions.KubeError: Kubernetes error.
***Details: tp = <class 'utils.exceptions.KubeError'>, value = None, tb = None

    def reraise(tp, value, tb=None):
        try:
            if value is None:
                value = tp()
            if value.__traceback__ is not tb:
                raise value.with_traceback(tb)
>           raise value

../../yuchengde_py36/lib/python3.6/site-packages/six.py:703: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../yuchengde_py36/lib/python3.6/site-packages/six.py:703: in reraise
    raise value
../../yuchengde_py36/lib/python3.6/site-packages/six.py:703: in reraise
    raise value
testfixtures/recover_hosts.py:21: in recover_
    HostsToRecover._recover_hosts(hosts, 'function')
testfixtures/recover_hosts.py:116: in _recover_hosts
    host_helper.recover_simplex(fail_ok=False)
keywords/host_helper.py:405: in recover_simplex
    con_ssh=con_ssh, auth_info=auth_info)
keywords/host_helper.py:971: in unlock_host
    all_namespaces=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pod_names = None, namespace = None, all_namespaces = True, labels = None, timeout = 300, check_interval = 10
con_ssh = <utils.clients.ssh.SSHClient object at 0x7feae1f0c6a0>, fail_ok = False, exclude = True, strict = False, kwargs = {'name': [], 'node': 'controller-0'}
bad_pods = {'coredns-6bc668cd76-88snz': 'Pending', 'server-pod-dep-7fc68f8c47-85drg': 'ImagePullBackOff', 'server-pod-dep-7fc68f8c47-957dw': 'ImagePullBackOff'}
end_time = 1591866020.6998146
bad_pods_info = [('server-pod-dep-7fc68f8c47-85drg', 'ImagePullBackOff'), ('server-pod-dep-7fc68f8c47-957dw', 'ImagePullBackOff'), ('coredns-6bc668cd76-88snz', 'Pending')]
msg = "Some pods are not Running or Completed: {'server-pod-dep-7fc68f8c47-85drg': 'ImagePullBackOff', 'server-pod-dep-7fc68f8c47-957dw': 'ImagePullBackOff', 'coredns-6bc668cd76-88snz': 'Pending'}"

    def wait_for_pods_healthy(pod_names=None, namespace=None, all_namespaces=True,
                              labels=None, timeout=300,
                              check_interval=5, con_ssh=None, fail_ok=False,
                              exclude=False, strict=False, **kwargs):
        """
        Wait for pods ready
        Args:
            pod_names (list|tuple|str|None): full name of pod(s)
            namespace (str|None):
            all_namespaces (bool|None)
            labels (str|dict|list|tuple|None):
            timeout:
            check_interval:
            con_ssh:
            fail_ok:
            exclude (bool)
            strict (bool): strict applies to node and name matching if given
            **kwargs
    
        Returns (tuple):
    
        """
        LOG.info("Wait for pods ready..")
        if not pod_names:
            pod_names = None
        elif isinstance(pod_names, str):
            pod_names = [pod_names]
    
        bad_pods = None
        end_time = time.time() + timeout
        while time.time() < end_time:
            bad_pods_info = get_unhealthy_pods(labels=labels,
                                               field=('NAME', 'STATUS'),
                                               namespace=namespace,
                                               all_namespaces=all_namespaces,
                                               con_ssh=con_ssh, exclude=exclude,
                                               strict=strict, **kwargs)
            bad_pods = {pod_info[0]: pod_info[1] for pod_info in bad_pods_info if
                        (not pod_names or pod_info[0] in pod_names)}
            if not bad_pods:
                LOG.info("Pods are Completed or Running.")
                if pod_names:
                    pod_names = [pod for pod in pod_names if
                                 not re.search('audit-|init-', pod)]
                    if not pod_names:
                        return True
    
                is_ready = wait_for_running_pods_ready(
                    pod_names=pod_names,
                    namespace=namespace,
                    all_namespaces=all_namespaces,
                    labels=labels, timeout=int(end_time - time.time()),
                    strict=strict,
                    con_ssh=con_ssh,
                    fail_ok=fail_ok, **kwargs)
                return is_ready
            time.sleep(check_interval)
    
        msg = 'Some pods are not Running or Completed: {}'.format(bad_pods)
        LOG.warning(msg)
        if fail_ok:
            return False
        dump_pods_info(con_ssh=con_ssh)
>       raise exceptions.KubeError(msg)
E       utils.exceptions.KubeError: Kubernetes error.
E       Details: Some pods are not Running or Completed: {'server-pod-dep-7fc68f8c47-85drg': 'ImagePullBackOff', 'server-pod-dep-7fc68f8c47-957dw': 'ImagePullBackOff', 'coredns-6bc668cd76-88snz': 'Pending'}

keywords/kube_helper.py:796: KubeError
[2020-06-11 09:00:43,020] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[controller] - Test Failed at test call, test teardown

[2020-06-11 09:00:43,027] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[compute]
[2020-06-11 09:00:43,029] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 09:00:43,029] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:43,029] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:43,029] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:00:43,806] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| f14f0093-05c8-4f3d-a436-a73a3a57dc8a | 100.114  | NTP address 209.115.181.102 is not a valid or a reachable NTP server.                                                 | host=controller-0.ntp=209.115.181.102                                            | minor    | 2020-06-11T08:54:07.163580 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 0aa412e5-98e5-467d-b906-41a06e441f0f | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                | host=controller-0.ntp                                                            | major    | 2020-06-11T08:45:47.107912 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:00:43,807] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:43,913] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:43,914] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:43,914] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:43,914] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:00:44,079] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          117m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:00:44,079] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:44,169] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:44,169] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:44,169] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:44,169] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:00:45,064] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:00:45,064] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:45,153] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:45,155] 213  DEBUG MainThread conftest.testcase_log:: 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test steps started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[compute]
[2020-06-11 09:00:45,155] 60   INFO  MainThread test_lock_unlock_host.test_lock_unlock_host:: 
====================== Test Step 1: Select a compute node from system if any
[2020-06-11 09:00:48,160] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:00:48,160] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:00:48,245] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 09:00:48,245] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 09:00:51,249] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:00:51,249] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:00:51,300] 426  DEBUG MainThread ssh.expect  :: Output: 
kroot@forstarlingxtest:~/yuchengde/test/automated-pytest-suite\root@forstarlingxtest$ 
[2020-06-11 09:00:51,301] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 09:00:51,301] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[compute]
[2020-06-11 09:00:51,302] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 1: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 09:00:51,302] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:51,302] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:51,302] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:00:52,093] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| f14f0093-05c8-4f3d-a436-a73a3a57dc8a | 100.114  | NTP address 209.115.181.102 is not a valid or a reachable NTP server.                                                 | host=controller-0.ntp=209.115.181.102                                            | minor    | 2020-06-11T08:54:07.163580 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 0aa412e5-98e5-467d-b906-41a06e441f0f | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                | host=controller-0.ntp                                                            | major    | 2020-06-11T08:45:47.107912 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:00:52,093] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:52,204] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:52,204] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:52,204] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:52,204] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:00:53,081] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:00:53,081] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:53,166] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:53,167] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 09:00:53,167] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:53,167] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:53,167] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:00:53,373] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          117m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:00:53,373] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:53,473] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:53,474] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 09:00:53,474] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:53,475] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:53,475] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 09:00:53,643] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE    IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 09:00:53,644] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:53,732] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:53,732] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 09:00:53,733] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[compute] - Test Skipped
Reason: No compute host on AIO system

[2020-06-11 09:00:53,735] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[storage]
[2020-06-11 09:00:53,736] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 09:00:53,736] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:53,736] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:53,736] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:00:54,508] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| f14f0093-05c8-4f3d-a436-a73a3a57dc8a | 100.114  | NTP address 209.115.181.102 is not a valid or a reachable NTP server.                                                 | host=controller-0.ntp=209.115.181.102                                            | minor    | 2020-06-11T08:54:07.163580 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 0aa412e5-98e5-467d-b906-41a06e441f0f | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                | host=controller-0.ntp                                                            | major    | 2020-06-11T08:45:47.107912 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:00:54,509] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:54,643] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:54,644] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:54,644] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:54,644] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:00:54,820] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          117m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:00:54,820] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:54,920] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:54,920] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:00:54,920] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:00:54,921] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:00:55,812] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:00:55,812] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:00:55,897] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:00:55,899] 213  DEBUG MainThread conftest.testcase_log:: 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test steps started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[storage]
[2020-06-11 09:00:55,899] 60   INFO  MainThread test_lock_unlock_host.test_lock_unlock_host:: 
====================== Test Step 1: Select a storage node from system if any
[2020-06-11 09:00:58,904] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:00:58,905] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:00:58,990] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 09:00:58,990] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 09:01:01,994] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:01:01,994] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:01:02,045] 426  DEBUG MainThread ssh.expect  :: Output: 
kroot@forstarlingxtest:~/yuchengde/test/automated-pytest-suite\root@forstarlingxtest$ 
[2020-06-11 09:01:02,046] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 09:01:02,046] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[storage]
[2020-06-11 09:01:02,046] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 1: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 09:01:02,047] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:02,047] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:02,047] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:01:03,156] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| f14f0093-05c8-4f3d-a436-a73a3a57dc8a | 100.114  | NTP address 209.115.181.102 is not a valid or a reachable NTP server.                                                 | host=controller-0.ntp=209.115.181.102                                            | minor    | 2020-06-11T08:54:07.163580 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 0aa412e5-98e5-467d-b906-41a06e441f0f | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                | host=controller-0.ntp                                                            | major    | 2020-06-11T08:45:47.107912 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:01:03,156] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:03,242] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:03,243] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:03,243] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:03,243] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:01:04,214] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:01:04,214] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:04,323] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:04,324] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 09:01:04,324] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:04,324] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:04,324] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:01:04,523] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ErrImagePull       0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          117m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:01:04,524] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:04,610] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:04,610] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 09:01:04,610] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:04,611] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:04,611] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 09:01:04,814] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 09:01:04,814] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:04,899] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:04,899] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 09:01:04,901] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[storage] - Test Skipped
Reason: System does not have storage nodes

[2020-06-11 09:01:04,903] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/mtc/test_swact.py::test_swact_controller_platform
[2020-06-11 09:01:04,904] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:04,904] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:04,904] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 09:01:05,794] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
controller-0:~$ 
[2020-06-11 09:01:05,794] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:05,878] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:05,879] 244  DEBUG MainThread system_helper.get_hosts:: Filtered hosts: ['controller-0']
[2020-06-11 09:01:05,879] 83   INFO  MainThread pre_checks_and_configs.wait_for_con_drbd_sync_complete:: Less than two controllers on system. Do not wait for drbd sync
[2020-06-11 09:01:05,880] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 09:01:05,880] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:05,881] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:05,881] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:01:06,665] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| f14f0093-05c8-4f3d-a436-a73a3a57dc8a | 100.114  | NTP address 209.115.181.102 is not a valid or a reachable NTP server.                                                 | host=controller-0.ntp=209.115.181.102                                            | minor    | 2020-06-11T08:54:07.163580 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 0aa412e5-98e5-467d-b906-41a06e441f0f | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                | host=controller-0.ntp                                                            | major    | 2020-06-11T08:45:47.107912 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:01:06,666] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:06,773] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:06,774] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:06,774] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:06,774] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:01:06,931] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ErrImagePull       0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          117m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:01:06,932] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:07,063] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:07,063] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:07,063] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:07,063] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:01:07,941] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:01:07,942] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:08,027] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:08,029] 213  DEBUG MainThread conftest.testcase_log:: 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test steps started for: testcases/functional/mtc/test_swact.py::test_swact_controller_platform
[2020-06-11 09:01:11,033] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:01:11,034] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:01:11,170] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 09:01:11,170] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 09:01:14,174] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:01:14,174] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:01:14,225] 426  DEBUG MainThread ssh.expect  :: Output: 
kroot@forstarlingxtest:~/yuchengde/test/automated-pytest-suite\root@forstarlingxtest$ 
[2020-06-11 09:01:14,225] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 09:01:14,226] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/mtc/test_swact.py::test_swact_controller_platform
[2020-06-11 09:01:14,226] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 1: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 09:01:14,226] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:14,226] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:14,227] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:01:15,013] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| f14f0093-05c8-4f3d-a436-a73a3a57dc8a | 100.114  | NTP address 209.115.181.102 is not a valid or a reachable NTP server.                                                 | host=controller-0.ntp=209.115.181.102                                            | minor    | 2020-06-11T08:54:07.163580 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 0aa412e5-98e5-467d-b906-41a06e441f0f | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                | host=controller-0.ntp                                                            | major    | 2020-06-11T08:45:47.107912 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:01:15,014] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:15,099] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:15,100] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:15,100] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:15,100] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:01:15,987] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:01:15,987] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:16,072] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:16,073] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 09:01:16,073] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:16,073] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:16,073] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:01:16,253] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          117m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:01:16,254] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:16,343] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:16,343] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 09:01:16,344] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:16,344] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:16,344] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 09:01:16,511] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 09:01:16,511] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:16,597] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:16,597] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 09:01:16,598] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/mtc/test_swact.py::test_swact_controller_platform - Test Skipped
Reason: Simplex system detected

[2020-06-11 09:01:16,600] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_custom_containers.py::test_launch_pod_via_kubectl[active]
[2020-06-11 09:01:19,606] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:01:19,606] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:01:19,713] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 09:01:19,714] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 09:01:22,717] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:01:22,718] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:01:22,769] 426  DEBUG MainThread ssh.expect  :: Output: 
kroot@forstarlingxtest:~/yuchengde/test/automated-pytest-suite\root@forstarlingxtest$ 
[2020-06-11 09:01:22,769] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 09:01:22,769] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_custom_containers.py::test_launch_pod_via_kubectl[active]
[2020-06-11 09:01:22,770] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_custom_containers.py::test_launch_pod_via_kubectl[active] - Test Skipped
Reason: Shared Test File Server is not ready

[2020-06-11 09:01:22,771] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_custom_containers.py::test_launch_pod_via_kubectl[standby]
[2020-06-11 09:01:25,776] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:01:25,776] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:01:25,860] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 09:01:25,861] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 09:01:28,865] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:01:28,865] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:01:28,916] 426  DEBUG MainThread ssh.expect  :: Output: 
kroot@forstarlingxtest:~/yuchengde/test/automated-pytest-suite\root@forstarlingxtest$ 
[2020-06-11 09:01:28,916] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 09:01:28,917] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_custom_containers.py::test_launch_pod_via_kubectl[standby]
[2020-06-11 09:01:28,917] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_custom_containers.py::test_launch_pod_via_kubectl[standby] - Test Skipped
Reason: Shared Test File Server is not ready

[2020-06-11 09:01:28,919] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_custom_containers.py::test_launch_app_via_sysinv
[2020-06-11 09:01:31,925] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:01:31,925] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:01:32,009] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 09:01:32,009] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 09:01:35,013] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:01:35,013] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:01:35,065] 426  DEBUG MainThread ssh.expect  :: Output: 
kroot@forstarlingxtest:~/yuchengde/test/automated-pytest-suite\root@forstarlingxtest$ 
[2020-06-11 09:01:35,065] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 09:01:35,065] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_custom_containers.py::test_launch_app_via_sysinv
[2020-06-11 09:01:35,066] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_custom_containers.py::test_launch_app_via_sysinv - Test Skipped
Reason: Shared Test File Server is not ready

[2020-06-11 09:01:35,067] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_custom_containers.py::test_push_docker_image_to_local_registry[active]
[2020-06-11 09:01:35,068] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 09:01:35,068] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:35,068] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:35,068] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:01:35,838] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| f14f0093-05c8-4f3d-a436-a73a3a57dc8a | 100.114  | NTP address 209.115.181.102 is not a valid or a reachable NTP server.                                                 | host=controller-0.ntp=209.115.181.102                                            | minor    | 2020-06-11T08:54:07.163580 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 0aa412e5-98e5-467d-b906-41a06e441f0f | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                | host=controller-0.ntp                                                            | major    | 2020-06-11T08:45:47.107912 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:01:35,838] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:35,923] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:35,924] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:35,924] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:35,924] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:01:36,098] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          117m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:01:36,098] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:36,199] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:36,200] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:36,200] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:36,200] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:01:37,078] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:01:37,078] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:37,163] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:37,165] 213  DEBUG MainThread conftest.testcase_log:: 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test steps started for: testcases/functional/z_containers/test_custom_containers.py::test_push_docker_image_to_local_registry[active]
[2020-06-11 09:01:37,165] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:37,166] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:37,166] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne servicegroup-list'
[2020-06-11 09:01:38,326] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+-----------------------------+--------------+--------+
| uuid                                 | service_group_name          | hostname     | state  |
+--------------------------------------+-----------------------------+--------------+--------+
| 082197f6-a781-4d6b-8992-d771f1ba6901 | cloud-services              | controller-0 | active |
| 17878848-a5f1-401f-a6ad-b93c64df572b | controller-services         | controller-0 | active |
| 2647e5fa-c43f-438f-b0bb-86821cb13ec6 | directory-services          | controller-0 | active |
| b1c7a7a9-ff90-429f-bccf-309aa26fef39 | oam-services                | controller-0 | active |
| 1c19d65d-1b57-480a-a560-4c39f9b68a56 | patching-services           | controller-0 | active |
| a46abe06-44f3-495d-8dbd-7271bedefeab | storage-monitoring-services | controller-0 | active |
| 24c3e5ed-0d76-422a-92da-0b2be0acaae3 | storage-services            | controller-0 | active |
| 86497da6-0538-44f2-ad79-1ff04740d5ae | vim-services                | controller-0 | active |
| d3206d06-049d-461a-aa74-8f4b2e8ff0f8 | web-services                | controller-0 | active |
+--------------------------------------+-----------------------------+--------------+--------+
controller-0:~$ 
[2020-06-11 09:01:38,326] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:38,412] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:38,412] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): ['controller-0']
[2020-06-11 09:01:38,412] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): []
[2020-06-11 09:01:38,412] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:38,412] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:38,413] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 09:01:39,335] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
controller-0:~$ 
[2020-06-11 09:01:39,335] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:39,423] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:39,424] 244  DEBUG MainThread system_helper.get_hosts:: Filtered hosts: ['controller-0']
[2020-06-11 09:01:39,424] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:39,424] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:39,424] 304  DEBUG MainThread ssh.send    :: Send 'hostname'
[2020-06-11 09:01:39,508] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0
controller-0:~$ 
[2020-06-11 09:01:39,508] 233  INFO  MainThread test_custom_containers.test_push_docker_image_to_local_registry:: 
====================== Test Step 1: Pull busybox image from external on active controller controller-0
[2020-06-11 09:01:39,508] 504  INFO  MainThread container_helper.pull_docker_image:: Pull docker image busybox
[2020-06-11 09:01:39,508] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 09:01:39,508] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image pull busybox'
[2020-06-11 09:01:46,967] 426  DEBUG MainThread ssh.expect  :: Output: 
Using default tag: latest
latest: Pulling from library/busybox

[1A[1K[K76df9210b28c: Pulling fs layer [1B[1A[1K[K76df9210b28c: Downloading [>                                                  ]  8.636kB/760.5kB[1B[1A[1K[K76df9210b28c: Downloading [==>                                                ]  33.17kB/760.5kB[1B[1A[1K[K76df9210b28c: Downloading [====>                                              ]   68.8kB/760.5kB[1B[1A[1K[K76df9210b28c: Downloading [================>                                  ]  244.8kB/760.5kB[1B[1A[1K[K76df9210b28c: Downloading [======================>                            ]  343.1kB/760.5kB[1B[1A[1K[K76df9210b28c: Downloading [===================================>               ]  547.3kB/760.5kB[1B[1A[1K[K76df9210b28c: Verifying Checksum [1B[1A[1K[K76df9210b28c: Download complete [1B[1A[1K[K76df9210b28c: Extracting [==>                                                ]  32.77kB/760.5kB[1B[1A[1K[K76df9210b28c: Extracting [==================================================>]  760.5kB/760.5kB[1B[1A[1K[K76df9210b28c: Pull complete [1BDigest: sha256:95cf004f559831017cdf4628aaf1bb30133677be8702a8c5f2994629f637a209
Status: Downloaded newer image for busybox:latest
controller-0:~$ 
[2020-06-11 09:01:46,967] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:47,053] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:47,053] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 09:01:47,053] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image ls busybox'
[2020-06-11 09:01:47,200] 426  DEBUG MainThread ssh.expect  :: Output: 
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
busybox             latest              1c35c4412082        8 days ago          1.22MB
controller-0:~$ 
[2020-06-11 09:01:47,200] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:47,334] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:47,334] 513  INFO  MainThread container_helper.pull_docker_image:: docker image busybox successfully pulled. ID: 1c35c4412082
[2020-06-11 09:01:47,334] 238  INFO  MainThread test_custom_containers.test_push_docker_image_to_local_registry:: 
====================== Test Step 2: Remove busybox from local registry if exists
[2020-06-11 09:01:47,334] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 09:01:47,334] 304  DEBUG MainThread ssh.send    :: Send 'sudo rm -rf /var/lib/docker-distribution/docker/registry/v2/repositories/busybox'
[2020-06-11 09:01:47,431] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 09:01:47,432] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:47,511] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:47,511] 242  INFO  MainThread test_custom_containers.test_push_docker_image_to_local_registry:: 
====================== Test Step 3: Tag image with local registry
[2020-06-11 09:01:47,511] 615  INFO  MainThread container_helper.tag_docker_image:: Tag docker image 1c35c4412082 as registry.local:9001/busybox
[2020-06-11 09:01:47,511] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 09:01:47,511] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image tag 1c35c4412082 registry.local:9001/busybox'
[2020-06-11 09:01:47,653] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 09:01:47,653] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:47,737] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:47,737] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 09:01:47,737] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image ls registry.local:9001/busybox'
[2020-06-11 09:01:47,894] 426  DEBUG MainThread ssh.expect  :: Output: 
REPOSITORY                    TAG                 IMAGE ID            CREATED             SIZE
registry.local:9001/busybox   latest              1c35c4412082        8 days ago          1.22MB
controller-0:~$ 
[2020-06-11 09:01:47,894] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:48,024] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:48,024] 629  INFO  MainThread container_helper.tag_docker_image:: docker image 1c35c4412082 successfully tagged as registry.local:9001/busybox.
[2020-06-11 09:01:48,024] 249  INFO  MainThread test_custom_containers.test_push_docker_image_to_local_registry:: 
====================== Test Step 4: Login to local docker registry and push test image from active controller controller-0
[2020-06-11 09:01:48,024] 542  INFO  MainThread container_helper.login_to_docker:: Login to docker registry registry.local:9001
[2020-06-11 09:01:48,025] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 09:01:48,025] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker login -u admin -p 99cloud@SH registry.local:9001'
[2020-06-11 09:01:48,576] 426  DEBUG MainThread ssh.expect  :: Output: 
WARNING! Using --password via the CLI is insecure. Use --password-stdin.
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
controller-0:~$ 
[2020-06-11 09:01:48,576] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:48,661] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:48,662] 548  INFO  MainThread container_helper.login_to_docker:: Logged into docker registry successfully: registry.local:9001
[2020-06-11 09:01:48,662] 577  INFO  MainThread container_helper.push_docker_image:: Push docker image: registry.local:9001/busybox
[2020-06-11 09:01:48,662] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 09:01:48,662] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image push registry.local:9001/busybox'
[2020-06-11 09:01:49,615] 426  DEBUG MainThread ssh.expect  :: Output: 
The push refers to repository [registry.local:9001/busybox]

[1A[1K[K1be74353c3d0: Preparing [1B[1A[1K[K1be74353c3d0: Pushing [=>                                                 ]  33.79kB/1.219MB[1B[1A[1K[K1be74353c3d0: Pushing [================================================>  ]  1.183MB/1.219MB[1B[1A[1K[K1be74353c3d0: Pushing [==================================================>]  1.437MB[1B[1A[1K[K1be74353c3d0: Pushed [1Blatest: digest: sha256:fd4a8673d0344c3a7f427fe4440d4b8dfd4fa59cfabbd9098f9eb0cb4ba905d0 size: 527
controller-0:~$ 
[2020-06-11 09:01:49,615] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:49,697] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:49,697] 583  INFO  MainThread container_helper.push_docker_image:: docker image registry.local:9001/busybox successfully pushed.
[2020-06-11 09:01:49,697] 254  INFO  MainThread test_custom_containers.test_push_docker_image_to_local_registry:: 
====================== Test Step 5: Remove cached test images and pull from local registry on controller-0
[2020-06-11 09:01:49,697] 652  INFO  MainThread container_helper.remove_docker_images:: Remove docker images: ('busybox', 'registry.local:9001/busybox')
[2020-06-11 09:01:49,697] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 09:01:49,697] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image rm busybox registry.local:9001/busybox'
[2020-06-11 09:01:49,849] 426  DEBUG MainThread ssh.expect  :: Output: 
Untagged: busybox:latest
Untagged: busybox@sha256:95cf004f559831017cdf4628aaf1bb30133677be8702a8c5f2994629f637a209
Untagged: registry.local:9001/busybox:latest
Untagged: registry.local:9001/busybox@sha256:fd4a8673d0344c3a7f427fe4440d4b8dfd4fa59cfabbd9098f9eb0cb4ba905d0
Deleted: sha256:1c35c441208254cb7c3844ba95a96485388cef9ccc0646d562c7fc026e04c807
Deleted: sha256:1be74353c3d0fd55fb5638a52953e6f1bc441e5b1710921db9ec2aa202725569
controller-0:~$ 
[2020-06-11 09:01:49,850] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:49,949] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:49,949] 504  INFO  MainThread container_helper.pull_docker_image:: Pull docker image registry.local:9001/busybox
[2020-06-11 09:01:49,949] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 09:01:49,950] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image pull registry.local:9001/busybox'
[2020-06-11 09:01:50,505] 426  DEBUG MainThread ssh.expect  :: Output: 
Using default tag: latest
latest: Pulling from busybox

[1A[1K[K76df9210b28c: Pulling fs layer [1B[1A[1K[K76df9210b28c: Downloading [>                                                  ]  8.361kB/760.5kB[1B[1A[1K[K76df9210b28c: Verifying Checksum [1B[1A[1K[K76df9210b28c: Download complete [1B[1A[1K[K76df9210b28c: Extracting [==>                                                ]  32.77kB/760.5kB[1B[1A[1K[K76df9210b28c: Extracting [==================================================>]  760.5kB/760.5kB[1B[1A[1K[K76df9210b28c: Pull complete [1BDigest: sha256:fd4a8673d0344c3a7f427fe4440d4b8dfd4fa59cfabbd9098f9eb0cb4ba905d0
Status: Downloaded newer image for registry.local:9001/busybox:latest
controller-0:~$ 
[2020-06-11 09:01:50,506] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:50,633] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:50,633] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 09:01:50,633] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image ls registry.local:9001/busybox'
[2020-06-11 09:01:50,778] 426  DEBUG MainThread ssh.expect  :: Output: 
REPOSITORY                    TAG                 IMAGE ID            CREATED             SIZE
registry.local:9001/busybox   latest              1c35c4412082        8 days ago          1.22MB
controller-0:~$ 
[2020-06-11 09:01:50,779] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:50,894] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:50,894] 513  INFO  MainThread container_helper.pull_docker_image:: docker image registry.local:9001/busybox successfully pulled. ID: 1c35c4412082
[2020-06-11 09:01:50,894] 652  INFO  MainThread container_helper.remove_docker_images:: Remove docker images: ('registry.local:9001/busybox',)
[2020-06-11 09:01:50,894] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 09:01:50,894] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image rm registry.local:9001/busybox'
[2020-06-11 09:01:51,060] 426  DEBUG MainThread ssh.expect  :: Output: 
Untagged: registry.local:9001/busybox:latest
Untagged: registry.local:9001/busybox@sha256:fd4a8673d0344c3a7f427fe4440d4b8dfd4fa59cfabbd9098f9eb0cb4ba905d0
Deleted: sha256:1c35c441208254cb7c3844ba95a96485388cef9ccc0646d562c7fc026e04c807
Deleted: sha256:1be74353c3d0fd55fb5638a52953e6f1bc441e5b1710921db9ec2aa202725569
controller-0:~$ 
[2020-06-11 09:01:51,061] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:51,146] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:51,146] 275  INFO  MainThread test_custom_containers.test_push_docker_image_to_local_registry:: 
====================== Test Step 6: Cleanup busybox from local docker registry after test
[2020-06-11 09:01:51,147] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 09:01:51,147] 304  DEBUG MainThread ssh.send    :: Send 'sudo rm -rf /var/lib/docker-distribution/docker/registry/v2/repositories/busybox'
[2020-06-11 09:01:51,264] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 09:01:51,264] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:51,348] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:54,353] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:01:54,353] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:01:54,437] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 09:01:54,437] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 09:01:57,441] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:01:57,441] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:01:57,493] 426  DEBUG MainThread ssh.expect  :: Output: 
kroot@forstarlingxtest:~/yuchengde/test/automated-pytest-suite\root@forstarlingxtest$ 
[2020-06-11 09:01:57,493] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 09:01:57,493] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_custom_containers.py::test_push_docker_image_to_local_registry[active]
[2020-06-11 09:01:57,494] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 1: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 09:01:57,494] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:57,495] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:57,495] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:01:58,294] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| f14f0093-05c8-4f3d-a436-a73a3a57dc8a | 100.114  | NTP address 209.115.181.102 is not a valid or a reachable NTP server.                                                 | host=controller-0.ntp=209.115.181.102                                            | minor    | 2020-06-11T08:54:07.163580 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 0aa412e5-98e5-467d-b906-41a06e441f0f | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                | host=controller-0.ntp                                                            | major    | 2020-06-11T08:45:47.107912 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:01:58,294] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:58,379] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:58,380] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:58,380] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:58,380] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:01:59,296] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:01:59,297] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:59,381] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:59,382] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 09:01:59,382] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:59,382] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:59,382] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:01:59,547] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          118m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:01:59,547] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:59,624] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:59,625] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 09:01:59,625] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:59,625] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:59,625] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 09:01:59,799] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE    IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 09:01:59,800] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:01:59,890] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:01:59,891] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 09:01:59,891] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_custom_containers.py::test_push_docker_image_to_local_registry[active] - Test Passed

[2020-06-11 09:01:59,893] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_custom_containers.py::test_push_docker_image_to_local_registry[standby]
[2020-06-11 09:01:59,894] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 09:01:59,894] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:01:59,894] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:01:59,894] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:02:00,648] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| f14f0093-05c8-4f3d-a436-a73a3a57dc8a | 100.114  | NTP address 209.115.181.102 is not a valid or a reachable NTP server.                                                 | host=controller-0.ntp=209.115.181.102                                            | minor    | 2020-06-11T08:54:07.163580 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 0aa412e5-98e5-467d-b906-41a06e441f0f | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                | host=controller-0.ntp                                                            | major    | 2020-06-11T08:45:47.107912 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:02:00,649] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:00,773] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:00,774] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:00,774] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:00,774] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:02:00,938] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          118m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:02:00,938] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:01,051] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:01,052] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:01,052] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:01,052] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:02:01,974] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:02:01,974] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:02,060] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:02,062] 213  DEBUG MainThread conftest.testcase_log:: 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test steps started for: testcases/functional/z_containers/test_custom_containers.py::test_push_docker_image_to_local_registry[standby]
[2020-06-11 09:02:02,062] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:02,062] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:02,062] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne servicegroup-list'
[2020-06-11 09:02:03,243] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+-----------------------------+--------------+--------+
| uuid                                 | service_group_name          | hostname     | state  |
+--------------------------------------+-----------------------------+--------------+--------+
| 082197f6-a781-4d6b-8992-d771f1ba6901 | cloud-services              | controller-0 | active |
| 17878848-a5f1-401f-a6ad-b93c64df572b | controller-services         | controller-0 | active |
| 2647e5fa-c43f-438f-b0bb-86821cb13ec6 | directory-services          | controller-0 | active |
| b1c7a7a9-ff90-429f-bccf-309aa26fef39 | oam-services                | controller-0 | active |
| 1c19d65d-1b57-480a-a560-4c39f9b68a56 | patching-services           | controller-0 | active |
| a46abe06-44f3-495d-8dbd-7271bedefeab | storage-monitoring-services | controller-0 | active |
| 24c3e5ed-0d76-422a-92da-0b2be0acaae3 | storage-services            | controller-0 | active |
| 86497da6-0538-44f2-ad79-1ff04740d5ae | vim-services                | controller-0 | active |
| d3206d06-049d-461a-aa74-8f4b2e8ff0f8 | web-services                | controller-0 | active |
+--------------------------------------+-----------------------------+--------------+--------+
controller-0:~$ 
[2020-06-11 09:02:03,244] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:03,329] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:03,330] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): ['controller-0']
[2020-06-11 09:02:03,330] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): []
[2020-06-11 09:02:03,330] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:03,330] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:03,330] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 09:02:04,283] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
controller-0:~$ 
[2020-06-11 09:02:04,284] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:04,368] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:04,369] 244  DEBUG MainThread system_helper.get_hosts:: Filtered hosts: ['controller-0']
[2020-06-11 09:02:07,374] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:02:07,374] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:02:07,459] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 09:02:07,460] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 09:02:10,463] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:02:10,464] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:02:10,515] 426  DEBUG MainThread ssh.expect  :: Output: 
kroot@forstarlingxtest:~/yuchengde/test/automated-pytest-suite\root@forstarlingxtest$ 
[2020-06-11 09:02:10,515] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 09:02:10,515] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_custom_containers.py::test_push_docker_image_to_local_registry[standby]
[2020-06-11 09:02:10,516] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 1: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 09:02:10,516] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:10,516] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:10,516] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:02:11,296] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| f14f0093-05c8-4f3d-a436-a73a3a57dc8a | 100.114  | NTP address 209.115.181.102 is not a valid or a reachable NTP server.                                                 | host=controller-0.ntp=209.115.181.102                                            | minor    | 2020-06-11T08:54:07.163580 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 0aa412e5-98e5-467d-b906-41a06e441f0f | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                | host=controller-0.ntp                                                            | major    | 2020-06-11T08:45:47.107912 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:02:11,296] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:11,414] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:11,414] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:11,414] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:11,414] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:02:12,364] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:02:12,364] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:12,450] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:12,450] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 09:02:12,450] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:12,450] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:12,450] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:02:12,615] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          118m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:02:12,616] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:12,717] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:12,718] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 09:02:12,718] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:12,718] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:12,718] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 09:02:12,901] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 09:02:12,901] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:13,024] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:13,024] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 09:02:13,025] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_custom_containers.py::test_push_docker_image_to_local_registry[standby] - Test Skipped
Reason: Standby controller does not exist or not in good state

[2020-06-11 09:02:13,027] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_custom_containers.py::test_upload_charts_via_helm_upload
[2020-06-11 09:02:16,032] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:02:16,032] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:02:16,115] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 09:02:16,115] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 09:02:19,119] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:02:19,119] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:02:19,170] 426  DEBUG MainThread ssh.expect  :: Output: 
kroot@forstarlingxtest:~/yuchengde/test/automated-pytest-suite\root@forstarlingxtest$ 
[2020-06-11 09:02:19,171] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 09:02:19,171] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_custom_containers.py::test_upload_charts_via_helm_upload
[2020-06-11 09:02:19,171] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_custom_containers.py::test_upload_charts_via_helm_upload - Test Skipped
Reason: Shared Test File Server is not ready

[2020-06-11 09:02:19,172] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_custom_containers.py::test_host_operations_with_custom_kubectl_app
[2020-06-11 09:02:19,173] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 09:02:19,174] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:19,174] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:19,174] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:02:19,943] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| f14f0093-05c8-4f3d-a436-a73a3a57dc8a | 100.114  | NTP address 209.115.181.102 is not a valid or a reachable NTP server.                                                 | host=controller-0.ntp=209.115.181.102                                            | minor    | 2020-06-11T08:54:07.163580 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 0aa412e5-98e5-467d-b906-41a06e441f0f | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                | host=controller-0.ntp                                                            | major    | 2020-06-11T08:45:47.107912 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:02:19,943] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:20,033] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:20,034] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:20,034] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:20,034] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:02:20,202] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          118m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:02:20,202] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:20,299] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:20,300] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:20,300] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:20,300] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:02:21,224] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:02:21,224] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:21,309] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:21,310] 327  INFO  MainThread test_custom_containers.deploy_delete_kubectl_app:: 
====================== Setup Step 2: Create resource-consumer test app by kubectl run
[2020-06-11 09:02:21,310] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:21,311] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:21,311] 304  DEBUG MainThread ssh.send    :: Send 'kubectl run resource-consumer --image=gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4 --expose --service-overrides='{ "spec": { "type": "LoadBalancer" } }' --port 8080 --requests='cpu=1000m,memory=1024Mi''
[2020-06-11 09:02:21,588] 426  DEBUG MainThread ssh.expect  :: Output: 
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
service/resource-consumer created
deployment.apps/resource-consumer created
controller-0:~$ 
[2020-06-11 09:02:21,588] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:21,681] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:21,682] 331  INFO  MainThread test_custom_containers.deploy_delete_kubectl_app:: 
====================== Setup Step 3: Check resource-consumer test app is created 
[2020-06-11 09:02:21,682] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:21,682] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:21,682] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod -n=default -o=wide'
[2020-06-11 09:02:21,862] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ContainerCreating   0          0s     <none>         worker-0   <none>           <none>
server-pod-dep-7fc68f8c47-85drg     0/1     ImagePullBackOff    0          2d6h   172.16.43.9    worker-0   <none>           <none>
server-pod-dep-7fc68f8c47-957dw     0/1     ImagePullBackOff    0          2d6h   172.16.43.10   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:02:21,863] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:21,962] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:21,963] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:21,963] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:21,963] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:22,141] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ContainerCreating   0          1s    <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:02:22,141] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:22,243] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:25,247] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:25,247] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:25,247] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:25,429] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ContainerCreating   0          4s    <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:02:25,430] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:25,525] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:28,529] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:28,529] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:28,529] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:28,699] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ContainerCreating   0          7s    <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:02:28,699] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:28,800] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:31,803] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:31,804] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:31,804] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:31,973] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ContainerCreating   0          10s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:02:31,973] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:32,067] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:35,070] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:35,071] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:35,071] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:35,255] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ContainerCreating   0          14s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:02:35,256] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:35,340] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:38,344] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:38,345] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:38,345] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:38,512] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ContainerCreating   0          17s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:02:38,513] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:38,597] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:41,601] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:41,601] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:41,602] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:41,770] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ContainerCreating   0          20s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:02:41,771] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:41,894] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:44,897] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:44,898] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:44,898] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:45,064] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ContainerCreating   0          24s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:02:45,064] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:45,153] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:48,157] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:48,157] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:48,157] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:48,352] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ContainerCreating   0          27s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:02:48,352] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:48,445] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:51,449] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:51,449] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:51,449] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:51,633] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ErrImagePull   0          30s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:02:51,634] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:51,717] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:54,721] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:54,721] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:54,722] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:54,883] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ErrImagePull   0          33s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:02:54,884] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:54,983] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:02:57,986] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:02:57,987] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:02:57,987] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:58,153] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ErrImagePull   0          37s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:02:58,153] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:02:58,236] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:01,240] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:01,240] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:01,240] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:01,413] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          40s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:01,413] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:01,512] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:04,516] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:04,516] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:04,516] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:04,703] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          43s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:04,703] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:04,803] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:07,807] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:07,807] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:07,807] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:08,024] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          46s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:08,024] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:08,153] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:11,157] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:11,157] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:11,158] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:11,354] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          50s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:11,354] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:11,474] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:14,478] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:14,478] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:14,478] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:14,639] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          53s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:14,639] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:14,723] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:17,726] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:17,727] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:17,727] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:17,877] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          56s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:17,877] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:17,963] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:20,967] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:20,967] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:20,967] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:21,124] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ErrImagePull   0          60s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:21,124] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:21,210] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:24,214] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:24,214] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:24,215] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:24,382] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ErrImagePull   0          63s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:24,382] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:24,479] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:27,483] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:27,483] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:27,484] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:27,646] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ErrImagePull   0          66s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:27,646] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:27,728] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:30,732] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:30,732] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:30,732] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:30,894] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          69s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:30,894] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:30,985] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:33,989] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:33,989] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:33,989] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:34,161] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          73s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:34,161] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:34,259] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:37,263] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:37,263] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:37,263] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:37,425] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          76s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:37,425] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:37,554] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:40,558] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:40,558] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:40,558] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:40,719] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          79s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:40,719] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:40,818] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:43,822] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:43,822] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:43,822] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:43,986] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          82s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:43,986] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:44,085] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:47,088] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:47,089] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:47,089] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:47,252] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          86s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:47,253] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:47,344] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:50,348] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:50,348] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:50,349] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:50,512] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          89s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:50,512] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:50,634] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:53,638] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:53,638] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:53,638] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:53,810] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          92s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:53,810] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:53,891] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:03:56,895] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:03:56,895] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:03:56,895] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:57,063] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE   IP            NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ErrImagePull   0          96s   172.16.43.1   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:03:57,063] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:03:57,154] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:00,157] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:00,158] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:00,158] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:00,325] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE   IP            NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ErrImagePull   0          99s   172.16.43.1   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:04:00,325] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:00,433] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:03,437] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:03,437] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:03,438] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:03,600] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE    IP            NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ErrImagePull   0          102s   172.16.43.1   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:04:03,600] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:03,703] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:06,707] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:06,707] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:06,707] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:06,881] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE    IP            NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ErrImagePull   0          105s   172.16.43.1   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:04:06,881] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:06,980] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:09,983] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:09,984] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:09,984] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:10,151] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE    IP            NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          109s   172.16.43.1   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:04:10,151] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:10,237] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:13,240] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:13,241] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:13,241] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:13,394] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE    IP            NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          112s   172.16.43.1   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:04:13,394] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:13,504] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:16,508] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:16,508] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:16,508] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:16,724] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE    IP            NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          115s   172.16.43.1   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:04:16,724] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:16,811] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:19,815] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:19,815] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:19,815] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:19,973] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE    IP            NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          118s   172.16.43.1   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:04:19,973] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:20,094] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:23,137] 54   DEBUG MainThread conftest.update_results:: ***Failure at test setup: /root/yuchengde/test/automated-pytest-suite/keywords/kube_helper.py:414: utils.exceptions.KubeError: Kubernetes error.
***Details: request = <SubRequest 'deploy_delete_kubectl_app' for <Function 'test_host_operations_with_custom_kubectl_app'>>

    @fixture()
    def deploy_delete_kubectl_app(request):
        app_name = 'resource-consumer'
        app_params = \
            '--image=gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4' \
            + ' --expose' \
            + ' --service-overrides=' \
            + "'{ " + '"spec": { "type": "LoadBalancer" } }' \
            + "' --port 8080 --requests='cpu=1000m,memory=1024Mi'"
    
        LOG.fixture_step("Create {} test app by kubectl run".format(app_name))
        sub_cmd = "run {}".format(app_name)
        kube_helper.exec_kube_cmd(sub_cmd=sub_cmd, args=app_params, fail_ok=False)
    
        LOG.fixture_step("Check {} test app is created ".format(app_name))
        pod_name = kube_helper.get_pods(field='NAME', namespace='default',
                                        name=app_name, strict=False)[0]
    
        def delete_app():
            LOG.fixture_step("Delete {} pod if exists after test "
                             "run".format(app_name))
            kube_helper.delete_resources(resource_names=app_name,
                                         resource_types=('deployment', 'service'),
                                         namespace='default', post_check=False)
            kube_helper.wait_for_resources_gone(resource_names=pod_name,
                                                namespace='default')
        request.addfinalizer(delete_app)
    
        kube_helper.wait_for_pods_status(pod_names=pod_name, namespace='default',
>                                        fail_ok=False)

testcases/functional/z_containers/test_custom_containers.py:346: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pod_names = ['resource-consumer-5577dbdbf-mp5sp'], partial_names = None, labels = None, namespace = 'default', status = 'Running', timeout = 120, check_interval = 3
con_ssh = None, fail_ok = False, strict = False, kwargs = {}, pods_to_check = ['resource-consumer-5577dbdbf-mp5sp']
actual_status = {'resource-consumer-5577dbdbf-mp5sp': 'ImagePullBackOff'}, end_time = 1591866261.9631872, pod_full_names = ['resource-consumer-5577dbdbf-mp5sp']
pods_values = [('resource-consumer-5577dbdbf-mp5sp', 'ImagePullBackOff')], continue_check = True, pod_info = ('resource-consumer-5577dbdbf-mp5sp', 'ImagePullBackOff')

    def wait_for_pods_status(pod_names=None, partial_names=None, labels=None,
                             namespace=None, status=PodStatus.RUNNING,
                             timeout=120, check_interval=3, con_ssh=None,
                             fail_ok=False, strict=False, **kwargs):
        """
        Wait for pod(s) to reach given status via kubectl get pod
        Args:
            pod_names (str|list|tuple): full name of the pods
            partial_names (str|list|tuple): Used only if pod_names are not provided
            labels (str|list|tuple|dict|None): Used only if pod_names are not
                provided
            namespace (None|str):
            status (str|None|list): None means any state as long as pod exists.
            timeout:
            check_interval:
            con_ssh:
            fail_ok:
            strict (bool):
    
        Returns (tuple):
            (True, <actual_pods_info>)  # actual_pods_info is a dict with
            pod_name as key, and pod_info(dict) as value
            (False, <actual_pods_info>)
    
        """
    
        pods_to_check = []
        if pod_names:
            if isinstance(pod_names, str):
                pod_names = [pod_names]
            else:
                pod_names = list(pod_names)
            labels = partial_names = None
            pods_to_check = list(pod_names)
        elif partial_names:
            if isinstance(partial_names, str):
                partial_names = [partial_names]
            else:
                partial_names = list(partial_names)
            kwargs['NAME'] = partial_names
            pods_to_check = list(partial_names)
    
        actual_status = {}
        end_time = time.time() + timeout
    
        while time.time() < end_time:
            pod_full_names = pods_to_check if pod_names else None
            pods_values = get_pods(pod_names=pod_full_names,
                                   field=('NAME', 'status'), namespace=namespace,
                                   labels=labels,
                                   strict=strict, fail_ok=True, con_ssh=con_ssh,
                                   **kwargs)
            if not pods_values:
                # No pods returned, continue to check.
                time.sleep(check_interval)
                continue
    
            continue_check = False  # This is used when only labels are provided
            for pod_info in pods_values:
                pod_name, pod_status = pod_info
                actual_status[pod_name] = pod_status
                if status and pod_status not in status:
                    # Status not as expected, continue to wait
                    continue_check = True
                    if partial_names:
                        # In this case, there might be multiple pods that matches
                        # 1 partial name, so the partial name that
                        # matches current pod could have been removed if there
                        # was one other pod that also matched the name
                        # had reached the desired state. In this case, we will
                        # add the partial name back to check list
                        for partial_name in partial_names:
                            if partial_name in pod_name and partial_name not in \
                                    pods_to_check:
                                pods_to_check.append(partial_name)
                                break
                else:
                    # Criteria met for current pod, remove it from check_list
                    if pod_names:
                        pods_to_check.remove(pod_name)
                    elif partial_names:
                        for partial_name in partial_names:
                            if partial_name in pod_name and partial_name in \
                                    pods_to_check:
                                pods_to_check.remove(partial_name)
                                break
    
            if not pods_to_check and not continue_check:
                return True, actual_status
    
            time.sleep(check_interval)
    
        name_str = 'Names: {}'.format(pods_to_check) if pods_to_check else ''
        label_str = 'Labels: {}'.format(labels) if labels else ''
        criteria = '{} {}'.format(name_str, label_str).strip()
        msg = "Pods did not reach expected status within {}s. Criteria not met: " \
              "{}. Actual info: {}".format(timeout, criteria, actual_status)
        if fail_ok:
            LOG.info(msg)
            return False, actual_status
    
>       raise exceptions.KubeError(msg)
E       utils.exceptions.KubeError: Kubernetes error.
E       Details: Pods did not reach expected status within 120s. Criteria not met: Names: ['resource-consumer-5577dbdbf-mp5sp']. Actual info: {'resource-consumer-5577dbdbf-mp5sp': 'ImagePullBackOff'}

keywords/kube_helper.py:414: KubeError
[2020-06-11 09:04:26,154] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:04:26,154] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:04:26,239] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 09:04:26,239] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 09:04:29,243] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:04:29,243] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:04:29,294] 426  DEBUG MainThread ssh.expect  :: Output: 
kroot@forstarlingxtest:~/yuchengde/test/automated-pytest-suite\root@forstarlingxtest$ 
[2020-06-11 09:04:29,295] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 09:04:29,295] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_custom_containers.py::test_host_operations_with_custom_kubectl_app
[2020-06-11 09:04:29,295] 337  INFO  MainThread test_custom_containers.delete_app:: 
====================== Teardown Step 1: Delete resource-consumer pod if exists after test run
[2020-06-11 09:04:29,295] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:29,295] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:29,296] 304  DEBUG MainThread ssh.send    :: Send 'kubectl delete deployment,service resource-consumer'
[2020-06-11 09:04:29,494] 426  DEBUG MainThread ssh.expect  :: Output: 
deployment.apps "resource-consumer" deleted
service "resource-consumer" deleted
controller-0:~$ 
[2020-06-11 09:04:29,495] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:29,592] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:29,592] 558  INFO  MainThread kube_helper.delete_resources:: ['resource-consumer'] are successfully removed.
[2020-06-11 09:04:29,592] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:29,593] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:29,593] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:29,782] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE    IP            NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-mp5sp   0/1     ImagePullBackOff   0          2m8s   172.16.43.1   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:04:29,782] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:29,866] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:32,870] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:32,870] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:32,870] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:33,060] 426  DEBUG MainThread ssh.expect  :: Output: 
Error from server (NotFound): pods "resource-consumer-5577dbdbf-mp5sp" not found
controller-0:~$ 
[2020-06-11 09:04:33,061] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:33,146] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 09:04:33,147] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 2: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 09:04:33,147] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:33,147] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:33,147] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:04:33,955] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:04:33,955] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:34,040] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:34,041] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:34,041] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:34,042] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:04:34,928] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:04:34,928] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:35,014] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:35,014] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 09:04:35,015] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:35,015] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:35,015] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:04:35,214] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          120m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:04:35,214] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:35,306] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:35,306] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 09:04:35,307] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:35,307] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:35,307] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 09:04:35,460] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 09:04:35,460] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:35,547] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:35,547] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 09:04:35,549] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_custom_containers.py::test_host_operations_with_custom_kubectl_app - Test Failed at test setup

[2020-06-11 09:04:35,550] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_kube_system_services.py::test_kube_system_services[active]
[2020-06-11 09:04:35,552] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 09:04:35,552] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:35,552] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:35,552] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:04:36,324] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:04:36,324] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:36,408] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:36,408] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:36,408] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:36,408] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:04:36,577] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          120m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:04:36,577] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:36,684] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:36,684] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:36,684] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:36,684] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:04:37,558] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:04:37,558] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:37,694] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:37,696] 213  DEBUG MainThread conftest.testcase_log:: 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test steps started for: testcases/functional/z_containers/test_kube_system_services.py::test_kube_system_services[active]
[2020-06-11 09:04:37,696] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:37,696] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:37,696] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne servicegroup-list'
[2020-06-11 09:04:38,945] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+-----------------------------+--------------+--------+
| uuid                                 | service_group_name          | hostname     | state  |
+--------------------------------------+-----------------------------+--------------+--------+
| 082197f6-a781-4d6b-8992-d771f1ba6901 | cloud-services              | controller-0 | active |
| 17878848-a5f1-401f-a6ad-b93c64df572b | controller-services         | controller-0 | active |
| 2647e5fa-c43f-438f-b0bb-86821cb13ec6 | directory-services          | controller-0 | active |
| b1c7a7a9-ff90-429f-bccf-309aa26fef39 | oam-services                | controller-0 | active |
| 1c19d65d-1b57-480a-a560-4c39f9b68a56 | patching-services           | controller-0 | active |
| a46abe06-44f3-495d-8dbd-7271bedefeab | storage-monitoring-services | controller-0 | active |
| 24c3e5ed-0d76-422a-92da-0b2be0acaae3 | storage-services            | controller-0 | active |
| 86497da6-0538-44f2-ad79-1ff04740d5ae | vim-services                | controller-0 | active |
| d3206d06-049d-461a-aa74-8f4b2e8ff0f8 | web-services                | controller-0 | active |
+--------------------------------------+-----------------------------+--------------+--------+
controller-0:~$ 
[2020-06-11 09:04:38,945] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:39,032] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:39,032] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): ['controller-0']
[2020-06-11 09:04:39,032] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): []
[2020-06-11 09:04:39,032] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:39,033] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:39,033] 304  DEBUG MainThread ssh.send    :: Send 'hostname'
[2020-06-11 09:04:39,129] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0
controller-0:~$ 
[2020-06-11 09:04:39,130] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:39,130] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod -n=kube-system -o=wide'
[2020-06-11 09:04:39,328] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
calico-kube-controllers-855577b7b5-2jwg2   1/1     Running   0          26m     172.16.192.68     controller-0   <none>           <none>
calico-node-6pkbm                          1/1     Running   3          29d     192.168.104.149   worker-0       <none>           <none>
calico-node-fhfpz                          1/1     Running   33         30d     192.168.104.2     controller-0   <none>           <none>
coredns-6bc668cd76-88snz                   0/1     Pending   0          120m    <none>            <none>         <none>           <none>
coredns-6bc668cd76-ls69s                   1/1     Running   0          26m     172.16.192.74     controller-0   <none>           <none>
kube-apiserver-controller-0                1/1     Running   32         30d     192.168.104.2     controller-0   <none>           <none>
kube-controller-manager-controller-0       1/1     Running   29         30d     192.168.104.2     controller-0   <none>           <none>
kube-multus-ds-amd64-4dcjj                 1/1     Running   0          9m48s   192.168.104.2     controller-0   <none>           <none>
kube-multus-ds-amd64-mp8gs                 1/1     Running   0          2d6h    192.168.104.149   worker-0       <none>           <none>
kube-proxy-7bwt2                           1/1     Running   16         30d     192.168.104.2     controller-0   <none>           <none>
kube-proxy-8c4nx                           1/1     Running   1          29d     192.168.104.149   worker-0       <none>           <none>
kube-scheduler-controller-0                1/1     Running   29         30d     192.168.104.2     controller-0   <none>           <none>
kube-sriov-cni-ds-amd64-7zmvm              1/1     Running   0          2d6h    192.168.104.149   worker-0       <none>           <none>
kube-sriov-cni-ds-amd64-xsjzj              1/1     Running   0          9m48s   192.168.104.2     controller-0   <none>           <none>
rbd-provisioner-7484d49cf6-n8s9r           1/1     Running   0          26m     172.16.192.75     controller-0   <none>           <none>
tiller-deploy-d6b59fcb-qfm76               1/1     Running   0          26m     192.168.104.2     controller-0   <none>           <none>
controller-0:~$ 
[2020-06-11 09:04:39,328] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:39,432] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:39,433] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:39,433] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get service -n=kube-system -o=wide'
[2020-06-11 09:04:39,594] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE   SELECTOR
kube-dns        ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP,9153/TCP   30d   k8s-app=kube-dns
tiller-deploy   ClusterIP   10.104.208.44   <none>        44134/TCP                30d   app=helm,name=tiller
controller-0:~$ 
[2020-06-11 09:04:39,595] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:39,693] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:39,694] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:39,694] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get deployment.apps -n=kube-system -o=wide'
[2020-06-11 09:04:39,860] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS                IMAGES                                                                        SELECTOR
calico-kube-controllers   1/1     1            1           30d   calico-kube-controllers   registry.local:9001/quay.io/calico/kube-controllers:v3.6.2                    k8s-app=calico-kube-controllers
coredns                   1/2     2            1           30d   coredns                   registry.local:9001/k8s.gcr.io/coredns:1.6.2                                  k8s-app=kube-dns
rbd-provisioner           1/1     1            1           30d   rbd-provisioner           registry.local:9001/quay.io/external_storage/rbd-provisioner:v2.1.1-k8s1.11   app=rbd-provisioner
tiller-deploy             1/1     1            1           30d   tiller                    registry.local:9001/gcr.io/kubernetes-helm/tiller:v2.13.1                     app=helm,name=tiller
controller-0:~$ 
[2020-06-11 09:04:39,860] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:39,996] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:39,997] 61   INFO  MainThread test_kube_system_services.test_kube_system_services:: 
====================== Test Step 1: Check kube-system pods status on active
[2020-06-11 09:04:39,997] 83   INFO  MainThread test_kube_system_services.test_kube_system_services:: 
====================== Test Step 2: Check kube-system services on active: ('kube-dns', 'tiller-deploy')
[2020-06-11 09:04:39,997] 90   INFO  MainThread test_kube_system_services.test_kube_system_services:: 
====================== Test Step 3: Check kube-system deployments on active: ('calico-kube-controllers', 'coredns', 'tiller-deploy')
[2020-06-11 09:04:43,002] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:04:43,002] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:04:43,114] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 09:04:43,115] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 09:04:46,119] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:04:46,119] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:04:46,170] 426  DEBUG MainThread ssh.expect  :: Output: 
kroot@forstarlingxtest:~/yuchengde/test/automated-pytest-suite\root@forstarlingxtest$ 
[2020-06-11 09:04:46,170] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 09:04:46,170] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_kube_system_services.py::test_kube_system_services[active]
[2020-06-11 09:04:46,171] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 1: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 09:04:46,171] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:46,171] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:46,172] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:04:46,939] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:04:46,940] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:47,025] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:47,026] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:47,026] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:47,026] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:04:47,894] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:04:47,894] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:47,969] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:47,969] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 09:04:47,970] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:47,970] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:47,970] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:04:48,143] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          121m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:04:48,143] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:48,216] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:48,217] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 09:04:48,217] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:48,217] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:48,217] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 09:04:48,384] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 09:04:48,384] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:48,489] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:48,490] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 09:04:48,491] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_kube_system_services.py::test_kube_system_services[active] - Test Passed

[2020-06-11 09:04:48,492] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_kube_system_services.py::test_kube_system_services[standby]
[2020-06-11 09:04:48,493] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 09:04:48,493] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:48,494] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:48,494] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:04:49,287] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:04:49,288] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:49,394] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:49,395] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:49,395] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:49,395] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:04:49,578] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          121m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:04:49,578] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:49,663] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:49,664] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:49,664] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:49,664] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:04:50,595] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:04:50,595] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:50,705] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:50,706] 213  DEBUG MainThread conftest.testcase_log:: 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test steps started for: testcases/functional/z_containers/test_kube_system_services.py::test_kube_system_services[standby]
[2020-06-11 09:04:50,706] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:50,707] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:50,707] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne servicegroup-list'
[2020-06-11 09:04:51,873] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+-----------------------------+--------------+--------+
| uuid                                 | service_group_name          | hostname     | state  |
+--------------------------------------+-----------------------------+--------------+--------+
| 082197f6-a781-4d6b-8992-d771f1ba6901 | cloud-services              | controller-0 | active |
| 17878848-a5f1-401f-a6ad-b93c64df572b | controller-services         | controller-0 | active |
| 2647e5fa-c43f-438f-b0bb-86821cb13ec6 | directory-services          | controller-0 | active |
| b1c7a7a9-ff90-429f-bccf-309aa26fef39 | oam-services                | controller-0 | active |
| 1c19d65d-1b57-480a-a560-4c39f9b68a56 | patching-services           | controller-0 | active |
| a46abe06-44f3-495d-8dbd-7271bedefeab | storage-monitoring-services | controller-0 | active |
| 24c3e5ed-0d76-422a-92da-0b2be0acaae3 | storage-services            | controller-0 | active |
| 86497da6-0538-44f2-ad79-1ff04740d5ae | vim-services                | controller-0 | active |
| d3206d06-049d-461a-aa74-8f4b2e8ff0f8 | web-services                | controller-0 | active |
+--------------------------------------+-----------------------------+--------------+--------+
controller-0:~$ 
[2020-06-11 09:04:51,873] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:52,010] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:52,011] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): ['controller-0']
[2020-06-11 09:04:52,011] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): []
[2020-06-11 09:04:52,011] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:52,011] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:52,011] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 09:04:52,877] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
controller-0:~$ 
[2020-06-11 09:04:52,877] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:04:52,975] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:04:52,976] 244  DEBUG MainThread system_helper.get_hosts:: Filtered hosts: ['controller-0']
[2020-06-11 09:04:55,981] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:04:55,981] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:04:56,083] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 09:04:56,083] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 09:04:59,087] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 09:04:59,087] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 09:04:59,138] 426  DEBUG MainThread ssh.expect  :: Output: 
kroot@forstarlingxtest:~/yuchengde/test/automated-pytest-suite\root@forstarlingxtest$ 
[2020-06-11 09:04:59,139] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 09:04:59,139] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_kube_system_services.py::test_kube_system_services[standby]
[2020-06-11 09:04:59,140] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 1: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 09:04:59,140] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:04:59,140] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:04:59,140] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 09:04:59,918] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 7c1731a7-e61f-4cfd-9446-6b98a27432c5 | 100.114  | NTP address 94.237.64.20 is not a valid or a reachable NTP server.                                                    | host=controller-0.ntp=94.237.64.20                                               | minor    | 2020-06-11T08:54:07.205154 |
| a21be695-c8ee-4015-9328-924f157df302 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T08:53:49.893721 |
| 61331074-03db-475e-aca6-319207a4a51c | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T08:53:49.567767 |
| 6304a73d-a92d-4e49-81c1-6341c68dafe8 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T08:53:49.404733 |
| 48bade79-22b1-4a0d-92c6-01f4bfbab78d | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T08:53:49.239814 |
| 39d4178b-fded-4c26-85a7-93020cbec108 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T08:47:39.363602 |
| a89e04ea-3658-474a-be2a-575c46d116b5 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T08:47:38.634429 |
| f4ab2dac-6001-4a7b-92e6-03da0d1b8856 | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T08:47:38.472482 |
| f1b2098f-a312-4239-91c4-8ba409726159 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T08:47:38.067545 |
| e79d89f2-0cfe-4309-b584-441cd9453068 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T08:47:37.905431 |
| 5f7107ee-3931-4e99-93dc-4a10e2abbdde | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T08:45:28.033392 |
| 2cc2968a-9764-4fad-b18e-8493dbafaf27 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-11T08:45:27.177381 |
| 45cd5638-2959-4cb1-b078-60d532272793 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T08:45:27.009479 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 09:04:59,918] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:05:00,005] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:05:00,005] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:05:00,005] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:05:00,005] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 09:05:00,886] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 09:05:00,886] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:05:00,972] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:05:00,972] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 09:05:00,973] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:05:00,973] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:05:00,973] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 09:05:01,167] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d6h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d6h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-88snz          0/1     Pending            0          121m   <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 09:05:01,167] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:05:01,270] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:05:01,271] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 09:05:01,271] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:05:01,271] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 09:05:01,271] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 09:05:01,468] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE    IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 09:05:01,469] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 09:05:01,569] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 09:05:01,569] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 09:05:01,575] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_kube_system_services.py::test_kube_system_services[standby] - Test Skipped
Reason: Standby controller does not exist or not in good state

[2020-06-11 09:05:01,715] 799  DEBUG MainThread ssh.close   :: connection closed. host: forstarlingxtest, user: root. Object ID: 140646771768848
[2020-06-11 09:05:01,715] 512  INFO  MainThread conftest.pytest_unconfigure:: Test Results saved to: /root/AUTOMATION_LOGS/172.16.130.249/202006111636/test_results.log
[2020-06-11 09:05:01,716] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 09:05:02,141] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140646789727904
