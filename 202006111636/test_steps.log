Setup started for: testcases/functional/horizon/test_hosts.py::test_horizon_host_inventory_display
====================== Setup Step 1: (session) Setting up test session...
[2020-06-11 08:36:20,941] 304  DEBUG MainThread ssh.send    :: Send 'cat /etc/build.info'
[2020-06-11 08:36:21,225] 304  DEBUG MainThread ssh.send    :: Send 'export PS1="\u@\h\$ "'
====================== Setup Step 2: (session) Gathering system health info before test session begins.

[2020-06-11 08:37:28,234] 54   DEBUG MainThread conftest.update_results:: ***Failure at test setup: /root/yuchengde/yuchengde_py36/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py:242: selenium.common.exceptions.WebDriverException: Message: connection refused

Teardown started for: testcases/functional/horizon/test_hosts.py::test_horizon_host_inventory_display






Setup started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[controller]
====================== Setup Step 1: (function) Gathering system health info before test function begins.

Test steps started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[controller]
====================== Test Step 1: Select a controller node from system if any
====================== Test Step 2: Lock controller host - controller-0 and ensure it is successfully locked
[2020-06-11 08:37:45,233] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-lock controller-0'
====================== Test Step 3: Unlock controller host - controller-0 and ensure it is successfully unlocked
[2020-06-11 08:39:05,611] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-unlock controller-0'
[2020-06-11 08:44:15,695] 304  DEBUG MainThread ssh.send    :: Send 'unset PROMPT_COMMAND'
[2020-06-11 08:44:15,789] 304  DEBUG MainThread ssh.send    :: Send 'export TMOUT=0'

[2020-06-11 08:46:59,193] 54   DEBUG MainThread conftest.update_results:: ***Failure at test call: /root/yuchengde/test/automated-pytest-suite/keywords/system_helper.py:3606: utils.exceptions.TimeoutException: Request(s) timed out

Teardown started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[controller]
====================== Teardown Step 1: function Recover simplex host
[2020-06-11 08:47:25,577] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-unlock controller-0'
[2020-06-11 08:52:55,975] 304  DEBUG MainThread ssh.send    :: Send 'unset PROMPT_COMMAND'
[2020-06-11 08:52:56,060] 304  DEBUG MainThread ssh.send    :: Send 'export TMOUT=0'
[2020-06-11 08:55:20,348] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get nodes'
[2020-06-11 09:00:29,000] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pods --all-namespaces -o wide | grep -v -e Running -e Completed'
[2020-06-11 09:00:29,259] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pods --all-namespaces -o wide | grep -v -e Running -e Completed -e NAMESPACE | awk '{system("kubectl describe pods -n "$1" "$2)}''
====================== Teardown Step 2: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 09:00:34,503] 304  DEBUG MainThread ssh.send    :: Send 'sudo ntpq -pn'
[2020-06-11 09:00:34,591] 304  DEBUG MainThread ssh.send    :: Send '99cloud@SH'
[2020-06-11 09:00:37,843] 304  DEBUG MainThread ssh.send    :: Send 'sudo ntpq -pn'
[2020-06-11 09:00:41,059] 304  DEBUG MainThread ssh.send    :: Send 'sudo ntpq -pn'

[2020-06-11 09:00:43,016] 54   DEBUG MainThread conftest.update_results:: ***Failure at test teardown: /root/yuchengde/test/automated-pytest-suite/keywords/kube_helper.py:796: utils.exceptions.KubeError: Kubernetes error.






Setup started for: testcases/functional/z_containers/test_custom_containers.py::test_host_operations_with_custom_kubectl_app
====================== Setup Step 1: (function) Gathering system health info before test function begins.
====================== Setup Step 2: Create resource-consumer test app by kubectl run
[2020-06-11 09:02:21,311] 304  DEBUG MainThread ssh.send    :: Send 'kubectl run resource-consumer --image=gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4 --expose --service-overrides='{ "spec": { "type": "LoadBalancer" } }' --port 8080 --requests='cpu=1000m,memory=1024Mi''
====================== Setup Step 3: Check resource-consumer test app is created 
[2020-06-11 09:02:21,682] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod -n=default -o=wide'
[2020-06-11 09:02:21,963] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:25,247] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:28,529] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:31,804] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:35,071] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:38,345] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:41,602] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:44,898] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:48,157] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:51,449] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:54,722] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:02:57,987] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:01,240] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:04,516] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:07,807] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:11,158] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:14,478] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:17,727] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:20,967] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:24,215] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:27,484] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:30,732] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:33,989] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:37,263] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:40,558] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:43,822] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:47,089] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:50,349] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:53,638] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:03:56,895] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:00,158] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:03,438] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:06,707] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:09,984] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:13,241] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:16,508] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:19,815] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'

[2020-06-11 09:04:23,137] 54   DEBUG MainThread conftest.update_results:: ***Failure at test setup: /root/yuchengde/test/automated-pytest-suite/keywords/kube_helper.py:414: utils.exceptions.KubeError: Kubernetes error.

Teardown started for: testcases/functional/z_containers/test_custom_containers.py::test_host_operations_with_custom_kubectl_app
====================== Teardown Step 1: Delete resource-consumer pod if exists after test run
[2020-06-11 09:04:29,296] 304  DEBUG MainThread ssh.send    :: Send 'kubectl delete deployment,service resource-consumer'
[2020-06-11 09:04:29,593] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
[2020-06-11 09:04:32,870] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-mp5sp -n=default -o=wide'
====================== Teardown Step 2: (function) Verify system alarms, applications and pods status after test function ended...






