[2020-06-11 05:30:26,604] 1544 WARNING MainThread ssh.get_active_controller:: No ssh client found for no_name
[2020-06-11 05:30:26,605] 151  INFO  MainThread ssh.connect :: Attempt to connect to host - 172.16.130.249
[2020-06-11 05:30:28,155] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:30:28,241] 426  DEBUG MainThread ssh.expect  :: Output:  
controller-0:~$ 
[2020-06-11 05:30:28,241] 183  INFO  MainThread ssh.connect :: Login successful!
[2020-06-11 05:30:28,242] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:30:28,307] 426  DEBUG MainThread ssh.expect  :: Output: [Kcontroller-0:~$ 
[2020-06-11 05:30:28,355] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:30:31,358] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:31,359] 304  DEBUG MainThread ssh.send    :: Send 'unset PROMPT_COMMAND'
[2020-06-11 05:30:31,435] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:30:31,435] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:31,435] 304  DEBUG MainThread ssh.send    :: Send 'export TMOUT=0'
[2020-06-11 05:30:31,522] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:30:31,522] 1637 INFO  MainThread ssh.set_active_controller:: Active controller client for no_name is set. Host ip/name: 172.16.130.249
[2020-06-11 05:30:31,522] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:31,522] 304  DEBUG MainThread ssh.send    :: Send 'cat /etc/platform/openrc'
[2020-06-11 05:30:31,607] 426  DEBUG MainThread ssh.expect  :: Output: 
unset OS_SERVICE_TOKEN

export OS_ENDPOINT_TYPE=internalURL
export CINDER_ENDPOINT_TYPE=internalURL

export OS_USERNAME=admin
export OS_PASSWORD=`TERM=linux /opt/platform/.keyring/19.12/.CREDENTIAL 2>/dev/null`
export OS_AUTH_TYPE=password
export OS_AUTH_URL=http://192.168.104.1:5000/v3

export OS_PROJECT_NAME=admin
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_IDENTITY_API_VERSION=3
export OS_REGION_NAME=RegionOne
export OS_INTERFACE=internal

if [ ! -z "${OS_PASSWORD}" ]; then
    export PS1='[\u@\h \W(keystone_$OS_USERNAME)]\$ '
else
    echo 'Openstack Admin credentials can only be loaded from the active controller.'
    export PS1='\h:\w\$ '
fi
controller-0:~$ 
[2020-06-11 05:30:31,607] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:30:31,691] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:30:31,692] 304  DEBUG MainThread ssh.send    :: Send 'source /etc/platform/openrc'
[2020-06-11 05:30:32,127] 426  DEBUG MainThread ssh.expect  :: Output: 
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:32,128] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:30:32,215] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:32,215] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:32,215] 304  DEBUG MainThread ssh.send    :: Send 'openstack --os-interface internal --os-region-name RegionOne endpoint list'
[2020-06-11 05:30:33,572] 426  DEBUG MainThread ssh.expect  :: Output: 
+----------------------------------+-----------+--------------+-----------------+---------+-----------+-------------------------------+
| ID                               | Region    | Service Name | Service Type    | Enabled | Interface | URL                           |
+----------------------------------+-----------+--------------+-----------------+---------+-----------+-------------------------------+
| 1334d559895748059dfa0e84fdc3bfe3 | RegionOne | fm           | faultmanagement | True    | admin     | http://192.168.104.1:18002    |
| 47a8a60bd0a04fa49afe6b5d9c3b40db | RegionOne | fm           | faultmanagement | True    | internal  | http://192.168.104.1:18002    |
| e924ed747c06477e8389884c11f27a7b | RegionOne | fm           | faultmanagement | True    | public    | http://172.16.130.249:18002   |
| 8b0c51548386427f97c32c9db5fb4db6 | RegionOne | patching     | patching        | True    | admin     | http://192.168.104.1:5491     |
| 9ca25e3f2ea046cf9a8c334611d5e017 | RegionOne | patching     | patching        | True    | internal  | http://192.168.104.1:5491     |
| 36c7f73f53544dc6bdf7c640ae9f5c54 | RegionOne | patching     | patching        | True    | public    | http://172.16.130.249:15491   |
| 03d275a9f1f44aa2bd36e51d53061b65 | RegionOne | vim          | nfv             | True    | admin     | http://192.168.104.1:4545     |
| 695d33221f1d4c97bcb7d29f2f40a2c9 | RegionOne | vim          | nfv             | True    | internal  | http://192.168.104.1:4545     |
| c0e36c6346894085b48b9fb3452e2f93 | RegionOne | vim          | nfv             | True    | public    | http://172.16.130.249:4545    |
| 2dc18f72771b4a6db6fe5e724858d19e | RegionOne | smapi        | smapi           | True    | admin     | http://192.168.104.1:7777     |
| 846e05377eed49079080bcad1a9ec83f | RegionOne | smapi        | smapi           | True    | internal  | http://192.168.104.1:7777     |
| fe8028a9ef9c4c71aa3f0a775f640de3 | RegionOne | smapi        | smapi           | True    | public    | http://172.16.130.249:7777    |
| a5bf601b82a84199ab0bf2d828a16b17 | RegionOne | keystone     | identity        | True    | admin     | http://192.168.104.1:5000/v3  |
| 8e98b4a9bc774980a58f00764ca454dd | RegionOne | keystone     | identity        | True    | internal  | http://192.168.104.1:5000/v3  |
| bfc26918df33404cb71b6dcbc7b4d419 | RegionOne | keystone     | identity        | True    | public    | http://172.16.130.249:5000/v3 |
| 3ef8fe1a9009441084fe6a80356e9c57 | RegionOne | barbican     | key-manager     | True    | admin     | http://192.168.104.1:9311     |
| 7d8525c308924e0baafd70209d4330f7 | RegionOne | barbican     | key-manager     | True    | internal  | http://192.168.104.1:9311     |
| 87f32559b7794042818113b52627ef02 | RegionOne | barbican     | key-manager     | True    | public    | http://172.16.130.249:9311    |
| f0c3484970154b9b9a8933c9bf094e21 | RegionOne | sysinv       | platform        | True    | admin     | http://192.168.104.1:6385/v1  |
| 7be457b874574dcca9962744444535cc | RegionOne | sysinv       | platform        | True    | internal  | http://192.168.104.1:6385/v1  |
| b7ae36db3f1643728b45b07cc6f0e01a | RegionOne | sysinv       | platform        | True    | public    | http://172.16.130.249:6385/v1 |
+----------------------------------+-----------+--------------+-----------------+---------+-----------+-------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:33,573] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:30:33,664] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:33,665] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:33,665] 304  DEBUG MainThread ssh.send    :: Send 'unset OS_REGION_NAME'
[2020-06-11 05:30:33,794] 426  DEBUG MainThread ssh.expect  :: Output: 
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:33,794] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:30:33,915] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:33,915] 575  DEBUG MainThread table_parser.get_values:: Returning matching URL value(s): ['http://172.16.130.249:5000/v3']
[2020-06-11 05:30:34,429] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/horizon/test_hosts.py::test_horizon_host_inventory_display
[2020-06-11 05:30:34,430] 19   INFO  MainThread conftest.setup_test_session:: 
====================== Setup Step 1: (session) Setting up test session...
[2020-06-11 05:30:34,430] 67   INFO  MainThread setups.setup_primary_tenant:: Primary Tenant for test session is set to admin
[2020-06-11 05:30:34,430] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:34,430] 304  DEBUG MainThread ssh.send    :: Send 'cat /etc/build.info'
[2020-06-11 05:30:34,515] 426  DEBUG MainThread ssh.expect  :: Output: 
###
### StarlingX
###     Release 19.12
###

OS="centos"
SW_VERSION="19.12"
BUILD_TARGET="Host Installer"
BUILD_TYPE="Formal"
BUILD_ID="r/stx.3.0"

JOB="STX_BUILD_3.0"
BUILD_BY="starlingx.build@cengn.ca"
BUILD_NUMBER="21"
BUILD_HOST="starlingx_mirror"
BUILD_DATE="2019-12-13 02:30:00 +0000"
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:34,515] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:30:34,607] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:34,607] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:30:34,745] 426  DEBUG MainThread ssh.expect  :: Output: 
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:34,745] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 05:30:34,746] 89   DEBUG MainThread local.connect :: Attempt to connect to localhost - forstarlingxtest
[2020-06-11 05:30:34,754] 304  DEBUG MainThread ssh.send    :: Send 'export PS1="\u@\h\$ "'
[2020-06-11 05:30:34,806] 426  DEBUG MainThread ssh.expect  :: Output: 
]0;root@forstarlingxtest:~/yuchengde/test/automated-pytest-suiteroot@forstarlingxtest$ 
[2020-06-11 05:30:34,806] 101  DEBUG MainThread local.connect :: Connected to localhost!
[2020-06-11 05:30:34,806] 1477 INFO  MainThread ssh.set_natbox_client:: NatBox localhost ssh client is set
[2020-06-11 05:30:34,807] 1425 INFO  MainThread ssh.get_natbox_client:: Getting NatBox Client...
[2020-06-11 05:30:34,807] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:34,807] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:30:35,785] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:35,786] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:30:35,867] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:35,867] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 05:30:35,867] 102  INFO  MainThread setups.setup_keypair:: stx-openstack is not applied. Skip nova keypair config.
[2020-06-11 05:30:35,868] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:35,868] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:30:36,854] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:30:25.517252+00:00                                     |
| uptime                | 4323                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:36,855] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:30:36,942] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:36,943] 124  INFO  MainThread system_helper.is_aio_system:: This is small footprint system.
[2020-06-11 05:30:36,944] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:36,944] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 05:30:37,820] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:37,821] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:30:37,909] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:37,910] 244  DEBUG MainThread system_helper.get_hosts:: Filtered hosts: ['controller-0']
[2020-06-11 05:30:37,910] 48   INFO  MainThread system_helper.get_sys_type:: ============= System type: AIO-SX ==============
[2020-06-11 05:30:37,910] 1536 INFO  MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:30:37,911] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:37,911] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 05:30:38,788] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:38,788] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:30:38,876] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:38,876] 244  DEBUG MainThread system_helper.get_hosts:: Filtered hosts: ['controller-0']
[2020-06-11 05:30:38,876] 324  INFO  MainThread setups._rsync_files_to_con1:: Less than two controllers on system. Skip copying file to controller-1.
[2020-06-11 05:30:38,877] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:30:38,877] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:38,877] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:30:39,783] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:39,783] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:30:39,905] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:39,905] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 05:30:39,906] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 2: (session) Gathering system health info before test session begins.
[2020-06-11 05:30:39,906] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:30:39,906] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:39,906] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:30:41,117] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 71285cae-d2da-4890-9dbe-ddae3efbc933 | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T04:20:24.743007 |
| eb0927a3-9800-458f-a589-96b553154441 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T04:20:23.015019 |
| a1992df8-842a-4a9d-ac6a-d33133dcc1c1 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T04:20:22.853014 |
| c82f3111-eb8b-443a-9b34-b3b1321822b1 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T04:20:22.688997 |
| 65f09eab-0da8-49b9-8599-6ae4e1ab6aa1 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T04:14:12.053764 |
| 911e0811-32fa-476d-af3b-164ed006e970 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T04:14:11.565951 |
| add0de33-6b9e-45dc-9532-6c940ff1fd9d | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T04:14:11.484756 |
| 7d9f5f81-647e-4733-aa75-c4147ee61076 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T04:14:11.160747 |
| 8916ca56-af06-4f7e-a56d-d285eb138933 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T04:14:10.998810 |
| 93e919a8-0c04-4f18-bc4e-f77418ffe8e0 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-09T07:10:45.037325 |
| e667bf0b-ed9a-4bf9-aae9-c3e61b386752 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-09T07:10:44.875347 |
| 3c77a4fa-5fc7-4b84-a39a-1fdbd02c2bd0 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-09T07:10:44.642047 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:41,117] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:30:41,206] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:41,206] 634  DEBUG MainThread conftest.config_host_module:: Empty config host module
[2020-06-11 05:30:41,207] 628  DEBUG MainThread conftest.config_host_class:: Empty config host class
[2020-06-11 05:30:41,207] 57   INFO  MainThread helper.get_driver:: Setting Firefox download preferences
[2020-06-11 05:30:41,423] 54   DEBUG MainThread conftest.update_results:: ***Failure at test setup: /root/yuchengde/yuchengde_py36/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py:242: selenium.common.exceptions.SessionNotCreatedException: Message: Unable to find a matching set of capabilities
***Details: request = <SubRequest 'driver' for <Function 'test_horizon_host_inventory_display'>>

    @fixture(scope="session")
    def driver(request):
>       driver_ = HorizonDriver.get_driver()

testfixtures/horizon.py:24: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
utils/horizon/helper.py:76: in get_driver
    driver_ = webdriver.Firefox(firefox_profile=profile)
../../yuchengde_py36/lib/python3.6/site-packages/selenium/webdriver/firefox/webdriver.py:174: in __init__
    keep_alive=True)
../../yuchengde_py36/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py:157: in __init__
    self.start_session(capabilities, browser_profile)
../../yuchengde_py36/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py:252: in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
../../yuchengde_py36/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py:321: in execute
    self.error_handler.check_response(response)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <selenium.webdriver.remote.errorhandler.ErrorHandler object at 0x7f7e8bac0908>
response = {'status': 500, 'value': '{"value":{"error":"session not created","message":"Unable to find a matching set of capabilities","stacktrace":""}}'}

    def check_response(self, response):
        """
        Checks that a JSON response from the WebDriver does not have an error.
    
        :Args:
         - response - The JSON response from the WebDriver server as a dictionary
           object.
    
        :Raises: If the response contains an error message.
        """
        status = response.get('status', None)
        if status is None or status == ErrorCode.SUCCESS:
            return
        value = None
        message = response.get("message", "")
        screen = response.get("screen", "")
        stacktrace = None
        if isinstance(status, int):
            value_json = response.get('value', None)
            if value_json and isinstance(value_json, basestring):
                import json
                try:
                    value = json.loads(value_json)
                    if len(value.keys()) == 1:
                        value = value['value']
                    status = value.get('error', None)
                    if status is None:
                        status = value["status"]
                        message = value["value"]
                        if not isinstance(message, basestring):
                            value = message
                            message = message.get('message')
                    else:
                        message = value.get('message', None)
                except ValueError:
                    pass
    
        exception_class = ErrorInResponseException
        if status in ErrorCode.NO_SUCH_ELEMENT:
            exception_class = NoSuchElementException
        elif status in ErrorCode.NO_SUCH_FRAME:
            exception_class = NoSuchFrameException
        elif status in ErrorCode.NO_SUCH_WINDOW:
            exception_class = NoSuchWindowException
        elif status in ErrorCode.STALE_ELEMENT_REFERENCE:
            exception_class = StaleElementReferenceException
        elif status in ErrorCode.ELEMENT_NOT_VISIBLE:
            exception_class = ElementNotVisibleException
        elif status in ErrorCode.INVALID_ELEMENT_STATE:
            exception_class = InvalidElementStateException
        elif status in ErrorCode.INVALID_SELECTOR \
                or status in ErrorCode.INVALID_XPATH_SELECTOR \
                or status in ErrorCode.INVALID_XPATH_SELECTOR_RETURN_TYPER:
            exception_class = InvalidSelectorException
        elif status in ErrorCode.ELEMENT_IS_NOT_SELECTABLE:
            exception_class = ElementNotSelectableException
        elif status in ErrorCode.ELEMENT_NOT_INTERACTABLE:
            exception_class = ElementNotInteractableException
        elif status in ErrorCode.INVALID_COOKIE_DOMAIN:
            exception_class = InvalidCookieDomainException
        elif status in ErrorCode.UNABLE_TO_SET_COOKIE:
            exception_class = UnableToSetCookieException
        elif status in ErrorCode.TIMEOUT:
            exception_class = TimeoutException
        elif status in ErrorCode.SCRIPT_TIMEOUT:
            exception_class = TimeoutException
        elif status in ErrorCode.UNKNOWN_ERROR:
            exception_class = WebDriverException
        elif status in ErrorCode.UNEXPECTED_ALERT_OPEN:
            exception_class = UnexpectedAlertPresentException
        elif status in ErrorCode.NO_ALERT_OPEN:
            exception_class = NoAlertPresentException
        elif status in ErrorCode.IME_NOT_AVAILABLE:
            exception_class = ImeNotAvailableException
        elif status in ErrorCode.IME_ENGINE_ACTIVATION_FAILED:
            exception_class = ImeActivationFailedException
        elif status in ErrorCode.MOVE_TARGET_OUT_OF_BOUNDS:
            exception_class = MoveTargetOutOfBoundsException
        elif status in ErrorCode.JAVASCRIPT_ERROR:
            exception_class = JavascriptException
        elif status in ErrorCode.SESSION_NOT_CREATED:
            exception_class = SessionNotCreatedException
        elif status in ErrorCode.INVALID_ARGUMENT:
            exception_class = InvalidArgumentException
        elif status in ErrorCode.NO_SUCH_COOKIE:
            exception_class = NoSuchCookieException
        elif status in ErrorCode.UNABLE_TO_CAPTURE_SCREEN:
            exception_class = ScreenshotException
        elif status in ErrorCode.ELEMENT_CLICK_INTERCEPTED:
            exception_class = ElementClickInterceptedException
        elif status in ErrorCode.INSECURE_CERTIFICATE:
            exception_class = InsecureCertificateException
        elif status in ErrorCode.INVALID_COORDINATES:
            exception_class = InvalidCoordinatesException
        elif status in ErrorCode.INVALID_SESSION_ID:
            exception_class = InvalidSessionIdException
        elif status in ErrorCode.UNKNOWN_METHOD:
            exception_class = UnknownMethodException
        else:
            exception_class = WebDriverException
        if value == '' or value is None:
            value = response['value']
        if isinstance(value, basestring):
            if exception_class == ErrorInResponseException:
                raise exception_class(response, value)
            raise exception_class(value)
        if message == "" and 'message' in value:
            message = value['message']
    
        screen = None
        if 'screen' in value:
            screen = value['screen']
    
        stacktrace = None
        if 'stackTrace' in value and value['stackTrace']:
            stacktrace = []
            try:
                for frame in value['stackTrace']:
                    line = self._value_or_default(frame, 'lineNumber', '')
                    file = self._value_or_default(frame, 'fileName', '<anonymous>')
                    if line:
                        file = "%s:%s" % (file, line)
                    meth = self._value_or_default(frame, 'methodName', '<anonymous>')
                    if 'className' in frame:
                        meth = "%s.%s" % (frame['className'], meth)
                    msg = "    at %s (%s)"
                    msg = msg % (meth, file)
                    stacktrace.append(msg)
            except TypeError:
                pass
        if exception_class == ErrorInResponseException:
            raise exception_class(response, message)
        elif exception_class == UnexpectedAlertPresentException:
            alert_text = None
            if 'data' in value:
                alert_text = value['data'].get('text')
            elif 'alert' in value:
                alert_text = value['alert'].get('text')
            raise exception_class(message, screen, stacktrace, alert_text)
>       raise exception_class(message, screen, stacktrace)
E       selenium.common.exceptions.SessionNotCreatedException: Message: Unable to find a matching set of capabilities

../../yuchengde_py36/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py:242: SessionNotCreatedException
[2020-06-11 05:30:44,441] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:30:44,442] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:30:44,527] 426  DEBUG MainThread ssh.expect  :: Output: 
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:44,527] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 05:30:47,530] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:30:47,531] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:30:47,582] 426  DEBUG MainThread ssh.expect  :: Output: 
]0;root@forstarlingxtest:~/yuchengde/test/automated-pytest-suiteroot@forstarlingxtest$ 
[2020-06-11 05:30:47,582] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 05:30:47,582] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/horizon/test_hosts.py::test_horizon_host_inventory_display
[2020-06-11 05:30:47,583] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/horizon/test_hosts.py::test_horizon_host_inventory_display - Test Failed at test setup

[2020-06-11 05:30:47,584] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_active_controller_reject
[2020-06-11 05:30:47,585] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:30:47,585] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:47,585] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:30:48,453] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:48,453] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:30:48,540] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:48,540] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 05:30:48,541] 58   INFO  MainThread pre_checks_and_configs.no_simplex:: 
====================== Setup Step 1: (Session) Skip if Simplex
[2020-06-11 05:30:51,545] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:30:51,546] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:30:51,630] 426  DEBUG MainThread ssh.expect  :: Output: 
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:51,630] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 05:30:54,633] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:30:54,634] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:30:54,685] 426  DEBUG MainThread ssh.expect  :: Output: 
]0;root@forstarlingxtest:~/yuchengde/test/automated-pytest-suiteroot@forstarlingxtest$ 
[2020-06-11 05:30:54,685] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 05:30:54,685] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_active_controller_reject
[2020-06-11 05:30:54,686] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_active_controller_reject - Test Skipped
Reason: Not applicable to Simplex system

[2020-06-11 05:30:54,687] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[controller]
[2020-06-11 05:30:54,689] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 05:30:54,689] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:30:54,689] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:54,689] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:30:55,955] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                           | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 71285cae-d2da-4890-9dbe-ddae3efbc933 | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available           | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T04:20:24.743007 |
| eb0927a3-9800-458f-a589-96b553154441 | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available      | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T04:20:23.015019 |
| a1992df8-842a-4a9d-ac6a-d33133dcc1c1 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T04:20:22.853014 |
| c82f3111-eb8b-443a-9b34-b3b1321822b1 | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available             | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T04:20:22.688997 |
| 65f09eab-0da8-49b9-8599-6ae4e1ab6aa1 | 400.002  | Service group oam-services has no active members available; expected 1 active member                                  | service_domain=controller.service_group=oam-services                             | critical | 2020-06-11T04:14:12.053764 |
| 911e0811-32fa-476d-af3b-164ed006e970 | 400.002  | Service group patching-services has no active members available; expected 1 active member                             | service_domain=controller.service_group=patching-services                        | critical | 2020-06-11T04:14:11.565951 |
| add0de33-6b9e-45dc-9532-6c940ff1fd9d | 400.002  | Service group directory-services has no active members available; expected 2 active members                           | service_domain=controller.service_group=directory-services                       | critical | 2020-06-11T04:14:11.484756 |
| 7d9f5f81-647e-4733-aa75-c4147ee61076 | 400.002  | Service group storage-services has no active members available; expected 2 active members                             | service_domain=controller.service_group=storage-services                         | critical | 2020-06-11T04:14:11.160747 |
| 8916ca56-af06-4f7e-a56d-d285eb138933 | 400.002  | Service group storage-monitoring-services has no active members available; expected 1 active member                   | service_domain=controller.service_group=storage-monitoring-services              | critical | 2020-06-11T04:14:10.998810 |
| 93e919a8-0c04-4f18-bc4e-f77418ffe8e0 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=mgmt                                                   | major    | 2020-06-09T07:10:45.037325 |
| e667bf0b-ed9a-4bf9-aae9-c3e61b386752 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                        | host=controller-0.network=oam                                                    | major    | 2020-06-09T07:10:44.875347 |
| 3c77a4fa-5fc7-4b84-a39a-1fdbd02c2bd0 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                          | host=controller-0.network=cluster-host                                           | major    | 2020-06-09T07:10:44.642047 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details. | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                            | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+-----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:55,955] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:30:56,039] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:56,040] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:30:56,040] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:56,040] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:30:56,283] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          46h    <none>         <none>     <none>           <none>
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:56,283] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:30:56,415] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:56,415] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:30:56,416] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:56,416] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:30:57,305] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:57,306] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:30:57,378] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:57,380] 213  DEBUG MainThread conftest.testcase_log:: 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test steps started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[controller]
[2020-06-11 05:30:57,380] 60   INFO  MainThread test_lock_unlock_host.test_lock_unlock_host:: 
====================== Test Step 1: Select a controller node from system if any
[2020-06-11 05:30:57,380] 82   INFO  MainThread test_lock_unlock_host.test_lock_unlock_host:: 
====================== Test Step 2: Lock controller host - controller-0 and ensure it is successfully locked
[2020-06-11 05:30:57,381] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:30:57,381] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:57,381] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:30:58,305] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:30:25.517252+00:00                                     |
| uptime                | 4323                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:58,305] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:30:58,415] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:30:58,416] 654  INFO  MainThread host_helper.lock_host:: Locking controller-0...
[2020-06-11 05:30:58,416] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:30:58,416] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:30:58,417] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-lock controller-0'
[2020-06-11 05:31:01,035] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+-------------------------------------------+
| Property              | Value                                     |
+-----------------------+-------------------------------------------+
| action                | none                                      |
| administrative        | unlocked                                  |
| availability          | available                                 |
| bm_ip                 | None                                      |
| bm_type               | none                                      |
| bm_username           | None                                      |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1 |
| capabilities          | {u'stor_function': u'monitor'}            |
| clock_synchronization | ntp                                       |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf      |
| config_status         | None                                      |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf      |
| console               | tty0                                      |
| created_at            | 2017-01-01T20:01:48.619012+00:00          |
| hostname              | controller-0                              |
| id                    | 1                                         |
| install_output        | text                                      |
| install_state         | None                                      |
| install_state_info    | None                                      |
| inv_state             | inventoried                               |
| invprovision          | provisioned                               |
| location              | {}                                        |
| mgmt_ip               | 192.168.104.2                             |
| mgmt_mac              | 54:b2:03:04:b9:13                         |
| operational           | enabled                                   |
| personality           | controller                                |
| reserved              | False                                     |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1 |
| serialid              | None                                      |
| software_load         | 19.12                                     |
| subfunction_avail     | available                                 |
| subfunction_oper      | enabled                                   |
| subfunctions          | controller,worker                         |
| task                  | Locking                                   |
| tboot                 | false                                     |
| ttys_dcd              | None                                      |
| updated_at            | 2020-06-11T05:30:25.517252+00:00          |
| uptime                | 4323                                      |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650      |
| vim_progress_status   | services-enabled                          |
+-----------------------+-------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:01,035] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:01,120] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:01,121] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'task': ''}
[2020-06-11 05:31:01,121] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:31:01,121] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:31:01,121] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:31:02,085] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking                                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:00.772203+00:00                                     |
| uptime                | 4323                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:02,086] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:02,204] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:02,205] 3593 INFO  MainThread system_helper.wait_for_host_values:: controller-0 task is Locking.
[2020-06-11 05:31:05,209] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:31:05,209] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:31:05,209] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:31:06,145] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking                                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:00.772203+00:00                                     |
| uptime                | 4323                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:06,145] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:06,232] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:09,235] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:31:09,236] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:31:09,236] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:31:10,119] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking                                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:00.772203+00:00                                     |
| uptime                | 4323                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:10,119] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:10,225] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:13,229] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:31:13,229] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:31:13,229] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:31:14,114] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking                                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:00.772203+00:00                                     |
| uptime                | 4323                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:14,114] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:14,203] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:17,207] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:31:17,207] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:31:17,207] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:31:18,125] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking                                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:00.772203+00:00                                     |
| uptime                | 4323                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:18,125] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:18,266] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:21,270] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:31:21,270] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:31:21,270] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:31:22,174] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking                                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:00.772203+00:00                                     |
| uptime                | 4323                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:22,175] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:22,259] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:25,264] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:31:25,264] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:31:25,264] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:31:26,298] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking                                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:25.658945+00:00                                     |
| uptime                | 4323                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:26,298] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:26,383] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:29,388] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:31:29,388] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:31:29,388] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:31:30,274] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking                                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:25.658945+00:00                                     |
| uptime                | 4323                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:30,275] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:30,364] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:33,368] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:31:33,368] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:31:33,368] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:31:34,295] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking-                                                             |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:33.310342+00:00                                     |
| uptime                | 4323                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:34,295] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:34,381] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:34,381] 3593 INFO  MainThread system_helper.wait_for_host_values:: controller-0 task is Locking-.
[2020-06-11 05:31:37,385] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:31:37,385] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:31:37,385] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:31:38,301] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Locking-                                                             |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:33.310342+00:00                                     |
| uptime                | 4323                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:38,301] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:38,403] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:41,407] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:31:41,407] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:31:41,407] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:31:42,391] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  | Disabling Controller                                                 |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:40.259345+00:00                                     |
| uptime                | 4583                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:42,391] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:42,477] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:42,478] 3593 INFO  MainThread system_helper.wait_for_host_values:: controller-0 task is Disabling Controller.
[2020-06-11 05:31:45,481] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:31:45,482] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:31:45,482] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:31:46,415] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:43.044027+00:00                                     |
| uptime                | 4588                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:46,415] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:46,525] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:46,525] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 task has reached: 
[2020-06-11 05:31:46,526] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'task': ''}
[2020-06-11 05:31:46,526] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'administrative': 'locked'}
[2020-06-11 05:31:46,526] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:31:46,526] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:31:46,526] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:31:47,411] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:43.044027+00:00                                     |
| uptime                | 4588                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:47,412] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:47,492] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:47,492] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 administrative has reached: locked
[2020-06-11 05:31:47,493] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'administrative': 'locked'}
[2020-06-11 05:31:47,493] 714  INFO  MainThread host_helper.lock_host:: controller-0 is locked. Waiting for it to go Online...
[2020-06-11 05:31:47,493] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'availability': 'online'}
[2020-06-11 05:31:47,493] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:31:47,493] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:31:47,493] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:31:48,380] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:43.044027+00:00                                     |
| uptime                | 4588                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:48,380] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:48,505] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:48,506] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 availability has reached: online
[2020-06-11 05:31:48,506] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'availability': 'online'}
[2020-06-11 05:31:53,512] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'availability': 'online'}
[2020-06-11 05:31:53,512] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:31:53,512] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:31:53,512] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:31:54,426] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:43.044027+00:00                                     |
| uptime                | 4588                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:54,426] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:54,513] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:54,514] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 availability has reached: online
[2020-06-11 05:31:54,514] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'availability': 'online'}
[2020-06-11 05:31:54,514] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'task': ''}
[2020-06-11 05:31:54,514] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:31:54,515] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:31:54,515] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:31:55,405] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:43.044027+00:00                                     |
| uptime                | 4588                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:55,405] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:31:55,483] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:31:55,483] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 task has reached: 
[2020-06-11 05:31:55,483] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'task': ''}
[2020-06-11 05:31:55,484] 732  INFO  MainThread host_helper.lock_host:: Host is successfully locked and in online state.
[2020-06-11 05:32:15,504] 92   INFO  MainThread test_lock_unlock_host.test_lock_unlock_host:: 
====================== Test Step 3: Unlock controller host - controller-0 and ensure it is successfully unlocked
[2020-06-11 05:32:15,505] 827  INFO  MainThread host_helper.unlock_host:: Unlocking controller-0...
[2020-06-11 05:32:15,505] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:32:15,505] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:32:15,505] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:32:16,445] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:43.044027+00:00                                     |
| uptime                | 4588                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:32:16,445] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:32:16,535] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:32:16,535] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:32:16,536] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:32:17,428] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | locked                                                               |
| availability          | online                                                               |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | disabled                                                             |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | online                                                               |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:31:43.044027+00:00                                     |
| uptime                | 4588                                                                 |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:32:17,429] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:32:17,512] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:32:17,512] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:32:17,512] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:32:18,425] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:32:18,425] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:32:18,543] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:32:18,543] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 05:32:18,543] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:32:18,544] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:32:18,725] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg            0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw            0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   calico-kube-controllers-855577b7b5-qkqtm   0/1     Pending            0          78s    <none>         <none>     <none>           <none>
kube-system   coredns-6bc668cd76-sxvt7                   0/1     Pending            0          78s    <none>         <none>     <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w                   0/1     Pending            0          46h    <none>         <none>     <none>           <none>
kube-system   rbd-provisioner-7484d49cf6-cq9xp           0/1     Pending            0          78s    <none>         <none>     <none>           <none>
kube-system   tiller-deploy-d6b59fcb-spld8               0/1     Pending            0          78s    <none>         <none>     <none>           <none>
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:32:18,725] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:32:18,811] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:32:18,812] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:32:18,812] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-unlock controller-0'
[2020-06-11 05:32:23,679] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+-------------------------------------------+
| Property              | Value                                     |
+-----------------------+-------------------------------------------+
| action                | none                                      |
| administrative        | locked                                    |
| availability          | online                                    |
| bm_ip                 | None                                      |
| bm_type               | none                                      |
| bm_username           | None                                      |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1 |
| capabilities          | {u'stor_function': u'monitor'}            |
| clock_synchronization | ntp                                       |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf      |
| config_status         | None                                      |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf      |
| console               | tty0                                      |
| created_at            | 2017-01-01T20:01:48.619012+00:00          |
| hostname              | controller-0                              |
| id                    | 1                                         |
| install_output        | text                                      |
| install_state         | None                                      |
| install_state_info    | None                                      |
| inv_state             | inventoried                               |
| invprovision          | provisioned                               |
| location              | {}                                        |
| mgmt_ip               | 192.168.104.2                             |
| mgmt_mac              | 54:b2:03:04:b9:13                         |
| operational           | disabled                                  |
| personality           | controller                                |
| reserved              | False                                     |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1 |
| serialid              | None                                      |
| software_load         | 19.12                                     |
| subfunction_avail     | online                                    |
| subfunction_oper      | disabled                                  |
| subfunctions          | controller,worker                         |
| task                  | Unlocking                                 |
| tboot                 | false                                     |
| ttys_dcd              | None                                      |
| updated_at            | 2020-06-11T05:31:43.044027+00:00          |
| uptime                | 4588                                      |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650      |
| vim_progress_status   | services-disabled                         |
+-----------------------+-------------------------------------------+
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:32:23,679] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:32:23,768] 426  DEBUG MainThread ssh.expect  :: Output: 
0
[sysadmin@controller-0 ~(keystone_admin)]$ 
[2020-06-11 05:34:53,899] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:34:56,954] 278  INFO  MainThread ssh.wait_for_disconnect:: ssh session to 172.16.130.249 disconnected
[2020-06-11 05:35:26,984] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:35:30,038] 151  INFO  MainThread ssh.connect :: Attempt to connect to host - 172.16.130.249
[2020-06-11 05:36:30,813] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:36:33,867] 206  DEBUG MainThread ssh.connect :: Login failed although no exception caught.
[2020-06-11 05:36:34,269] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140181504263896
[2020-06-11 05:36:34,270] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 05:37:38,050] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:37:41,101] 206  DEBUG MainThread ssh.connect :: Login failed although no exception caught.
[2020-06-11 05:37:41,302] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140181504263896
[2020-06-11 05:37:41,303] 251  DEBUG MainThread ssh.connect :: Retry in 3 seconds
[2020-06-11 05:37:45,825] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:37:45,911] 426  DEBUG MainThread ssh.expect  :: Output:  
controller-0:~$ 
[2020-06-11 05:37:45,911] 183  INFO  MainThread ssh.connect :: Login successful!
[2020-06-11 05:37:45,911] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:37:45,968] 426  DEBUG MainThread ssh.expect  :: Output: [Kcontroller-0:~$ 
[2020-06-11 05:37:46,001] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:37:49,005] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:37:49,005] 304  DEBUG MainThread ssh.send    :: Send 'unset PROMPT_COMMAND'
[2020-06-11 05:37:49,092] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:37:49,092] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:37:49,093] 304  DEBUG MainThread ssh.send    :: Send 'export TMOUT=0'
[2020-06-11 05:37:49,217] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:37:49,217] 1637 INFO  MainThread ssh.set_active_controller:: Active controller client for no_name is set. Host ip/name: 172.16.130.249
[2020-06-11 05:37:49,217] 1296 INFO  MainThread host_helper._wait_for_openstack_cli_enable:: Waiting for system cli and subfunctions to be ready and nova cli (if stx-openstack applied) to be enabled on active controller
[2020-06-11 05:37:49,217] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:37:49,217] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 05:37:50,062] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 05:37:50,062] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:37:50,146] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 05:37:50,146] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:37:50,229] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:38:00,240] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:38:00,240] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 05:38:00,857] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 05:38:00,857] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:38:00,942] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 05:38:00,943] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:38:01,029] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:38:11,039] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:38:11,040] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 05:38:11,664] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 05:38:11,665] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:38:11,787] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 05:38:11,787] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:38:11,907] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:38:21,917] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:38:21,918] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 05:38:22,542] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 05:38:22,543] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:38:22,628] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 05:38:22,628] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:38:22,756] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:38:32,767] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:38:32,767] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 05:38:33,428] 426  DEBUG MainThread ssh.expect  :: Output: 
Authorization failed: Unable to establish connection to http://192.168.104.1:5000/v3/auth/tokens
controller-0:~$ 
[2020-06-11 05:38:33,429] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:38:33,547] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 05:38:33,548] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:38:33,645] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:38:43,656] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:38:43,656] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 05:38:44,716] 426  DEBUG MainThread ssh.expect  :: Output: 
+----------------------+--------------------------------------+
| Property             | Value                                |
+----------------------+--------------------------------------+
| contact              | None                                 |
| created_at           | 2017-01-01T20:00:35.851917+00:00     |
| description          | None                                 |
| https_enabled        | False                                |
| location             | None                                 |
| name                 | 03a8f8c4-9883-4c0d-a430-63370202a9d0 |
| region_name          | RegionOne                            |
| sdn_enabled          | False                                |
| security_feature     | spectre_meltdown_v1                  |
| service_project_name | services                             |
| software_version     | 19.12                                |
| system_mode          | duplex                               |
| system_type          | All-in-one                           |
| timezone             | UTC                                  |
| updated_at           | 2020-05-12T07:22:44.454374+00:00     |
| uuid                 | 3f620fb2-4ceb-441a-b7d7-8800c6c585f3 |
| vswitch_type         | none                                 |
+----------------------+--------------------------------------+
controller-0:~$ 
[2020-06-11 05:38:44,716] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:38:44,815] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:38:54,820] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:38:54,820] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne servicegroup-list'
[2020-06-11 05:38:56,642] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+-----------------------------+--------------+--------+
| uuid                                 | service_group_name          | hostname     | state  |
+--------------------------------------+-----------------------------+--------------+--------+
| 6d74599f-5aa5-4667-92b8-bcf065b97566 | cloud-services              | controller-0 | active |
| e6f0d8e5-8313-4c07-8597-06f87ff72758 | controller-services         | controller-0 | active |
| 0bd309c1-616b-4b3c-b0fa-588b5d59a6ed | directory-services          | controller-0 | active |
| db0096b5-0946-4518-ba8c-07676312736c | oam-services                | controller-0 | active |
| f2a7b29a-9e00-4c1a-ac9d-646d28b22e88 | patching-services           | controller-0 | active |
| 6ba173e0-cb68-44f8-817f-bc330ca41322 | storage-monitoring-services | controller-0 | active |
| ff7a236c-d5cb-4b2c-af44-4990e774fc76 | storage-services            | controller-0 | active |
| 39603c17-ae56-4106-bb4d-4e89c8bd14a6 | vim-services                | controller-0 | active |
| 1a32d85a-3abf-44c8-a5e5-9a6da9d88068 | web-services                | controller-0 | active |
+--------------------------------------+-----------------------------+--------------+--------+
controller-0:~$ 
[2020-06-11 05:38:56,642] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:38:56,727] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:38:56,728] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): ['controller-0']
[2020-06-11 05:38:56,728] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): []
[2020-06-11 05:38:56,728] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:38:56,728] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:38:56,729] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:38:57,809] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | offline                                                              |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  | Enabling Worker Service                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:38:52.181784+00:00                                     |
| uptime                | 318                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 05:38:57,810] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:38:57,927] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:38:57,928] 541  INFO  MainThread host_helper.wait_for_task_clear_and_subfunction_ready:: Waiting for task clear and subfunctions enable/available (if applicable) for hosts: ['controller-0']
[2020-06-11 05:38:57,928] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:38:57,928] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:38:59,024] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | offline                                                              |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  | Enabling Worker Service                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:38:52.181784+00:00                                     |
| uptime                | 318                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 05:38:59,024] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:38:59,114] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:39:09,125] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:39:09,125] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:39:10,046] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | offline                                                              |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  | Enabling Worker Service                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:38:52.181784+00:00                                     |
| uptime                | 318                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 05:39:10,046] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:39:10,167] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:39:20,178] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:39:20,178] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:39:21,131] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | offline                                                              |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  | Enabling Worker Service                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:38:52.181784+00:00                                     |
| uptime                | 318                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 05:39:21,131] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:39:21,235] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:39:31,246] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:39:31,247] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:39:32,154] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | None                                                                 |
| config_target         | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | offline                                                              |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  | Enabling Worker Service                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:38:52.181784+00:00                                     |
| uptime                | 318                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 05:39:32,155] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:39:32,287] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:39:42,298] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:39:42,298] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:39:43,260] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 3dfced72-40c6-43fc-b224-fec81b9df26e                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | offline                                                              |
| subfunction_oper      | disabled                                                             |
| subfunctions          | controller,worker                                                    |
| task                  | Enabling Worker Service                                              |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:39:42.700007+00:00                                     |
| uptime                | 318                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-disabled                                                    |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 05:39:43,260] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:39:43,397] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:39:53,408] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:39:53,409] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:39:54,417] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 3dfced72-40c6-43fc-b224-fec81b9df26e                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:39:48.160766+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 05:39:54,418] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:39:54,502] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:39:54,503] 557  INFO  MainThread host_helper.wait_for_task_clear_and_subfunction_ready:: Hosts task cleared and subfunctions (if applicable) are now in enabled/available states
[2020-06-11 05:39:54,503] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:39:54,504] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:39:55,498] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:39:55,498] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:39:55,650] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:39:55,651] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 05:39:55,651] 1284 INFO  MainThread host_helper.check_sysinv_cli:: system cli and subfunction enabled
[2020-06-11 05:40:05,661] 772  INFO  MainThread host_helper._wait_for_simplex_reconnect:: Re-connected via ssh and openstack CLI enabled
[2020-06-11 05:40:05,661] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'administrative': 'unlocked'}
[2020-06-11 05:40:05,662] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:40:05,662] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:40:06,607] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 3dfced72-40c6-43fc-b224-fec81b9df26e                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:39:48.160766+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 05:40:06,607] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:40:06,693] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:40:06,693] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 administrative has reached: unlocked
[2020-06-11 05:40:06,693] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'administrative': 'unlocked'}
[2020-06-11 05:40:06,693] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'availability': ['available', 'degraded']}
[2020-06-11 05:40:06,694] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:40:06,694] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:40:07,640] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 3dfced72-40c6-43fc-b224-fec81b9df26e                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:39:48.160766+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 05:40:07,640] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:40:07,741] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:40:07,742] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 availability has reached: available
[2020-06-11 05:40:07,742] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'availability': ['available', 'degraded']}
[2020-06-11 05:40:07,742] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'task': ''}
[2020-06-11 05:40:07,742] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:40:07,742] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:40:08,684] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 3dfced72-40c6-43fc-b224-fec81b9df26e                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:39:48.160766+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 05:40:08,685] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:40:08,802] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:40:08,803] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 task has reached: 
[2020-06-11 05:40:08,803] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'task': ''}
[2020-06-11 05:40:08,803] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:40:08,803] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:40:09,786] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 3dfced72-40c6-43fc-b224-fec81b9df26e                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:39:48.160766+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 05:40:09,786] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:40:09,870] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:40:09,871] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:40:09,871] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:40:10,805] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:40:10,805] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:40:10,917] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:40:10,918] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 05:40:10,918] 1580 INFO  MainThread host_helper.wait_for_webservice_up:: Waiting for ['controller-0'] to be active for web-service in system servicegroup-list...
[2020-06-11 05:40:10,918] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:40:10,918] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne servicegroup-list'
[2020-06-11 05:40:12,126] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+-----------------------------+--------------+--------+
| uuid                                 | service_group_name          | hostname     | state  |
+--------------------------------------+-----------------------------+--------------+--------+
| 6d74599f-5aa5-4667-92b8-bcf065b97566 | cloud-services              | controller-0 | active |
| e6f0d8e5-8313-4c07-8597-06f87ff72758 | controller-services         | controller-0 | active |
| 0bd309c1-616b-4b3c-b0fa-588b5d59a6ed | directory-services          | controller-0 | active |
| db0096b5-0946-4518-ba8c-07676312736c | oam-services                | controller-0 | active |
| f2a7b29a-9e00-4c1a-ac9d-646d28b22e88 | patching-services           | controller-0 | active |
| 6ba173e0-cb68-44f8-817f-bc330ca41322 | storage-monitoring-services | controller-0 | active |
| ff7a236c-d5cb-4b2c-af44-4990e774fc76 | storage-services            | controller-0 | active |
| 39603c17-ae56-4106-bb4d-4e89c8bd14a6 | vim-services                | controller-0 | active |
| 1a32d85a-3abf-44c8-a5e5-9a6da9d88068 | web-services                | controller-0 | active |
+--------------------------------------+-----------------------------+--------------+--------+
controller-0:~$ 
[2020-06-11 05:40:12,126] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:40:12,213] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:40:12,213] 1600 INFO  MainThread host_helper.wait_for_webservice_up:: Host(s) ['controller-0'] are active for web-service in system servicegroup-list
[2020-06-11 05:40:12,213] 3552 INFO  MainThread system_helper.wait_for_host_values:: Waiting for controller-0 to reach state(s) - {'subfunction_oper': 'enabled', 'subfunction_avail': 'available'}
[2020-06-11 05:40:12,213] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:40:12,213] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:40:13,198] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 1dea51f3-ddc6-43dd-9236-d73d6ba60ebf                                 |
| config_status         | Config out-of-date                                                   |
| config_target         | 3dfced72-40c6-43fc-b224-fec81b9df26e                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:39:48.160766+00:00                                     |
| uptime                | 373                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 05:40:13,198] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:40:13,286] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:40:13,286] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 subfunction_oper has reached: enabled
[2020-06-11 05:40:13,287] 3589 INFO  MainThread system_helper.wait_for_host_values:: controller-0 subfunction_avail has reached: available
[2020-06-11 05:40:13,287] 3597 INFO  MainThread system_helper.wait_for_host_values:: controller-0 is in state(s): {'subfunction_oper': 'enabled', 'subfunction_avail': 'available'}
[2020-06-11 05:40:13,287] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:40:13,287] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get nodes'
[2020-06-11 05:40:13,484] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME           STATUS   ROLES    AGE   VERSION
controller-0   Ready    master   29d   v1.16.2
worker-0       Ready    <none>   29d   v1.16.2
controller-0:~$ 
[2020-06-11 05:40:13,484] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:40:13,570] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:40:13,570] 684  INFO  MainThread kube_helper.wait_for_nodes_ready:: All nodes are ready: controller-0
[2020-06-11 05:40:13,571] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 05:40:13,571] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:40:13,571] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:40:13,761] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          46h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:40:13,761] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:40:13,876] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:40:23,886] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:40:23,887] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:40:24,058] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ErrImagePull       0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          46h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:40:24,058] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:40:24,145] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:40:34,155] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:40:34,156] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:40:34,327] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          46h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:40:34,328] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:40:34,416] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:40:44,427] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:40:44,428] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:40:44,595] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:40:44,596] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:40:44,679] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:40:54,690] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:40:54,691] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:40:54,857] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:40:54,857] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:40:54,987] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:41:04,998] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:41:04,998] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:41:05,286] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:41:05,287] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:41:05,375] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:41:15,386] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:41:15,386] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:41:15,550] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:41:15,550] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:41:15,636] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:41:25,647] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:41:25,648] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:41:25,815] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:41:25,815] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:41:25,904] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:41:35,915] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:41:35,916] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:41:36,079] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:41:36,079] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:41:36,169] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:41:46,180] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:41:46,180] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:41:46,345] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:41:46,345] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:41:46,443] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:41:56,454] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:41:56,454] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:41:56,654] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:41:56,654] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:41:56,753] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:42:06,764] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:42:06,764] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:42:06,988] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:42:06,988] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:42:07,075] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:42:17,086] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:42:17,086] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:42:17,251] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:42:17,251] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:42:17,337] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:42:27,347] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:42:27,348] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:42:27,516] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:42:27,517] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:42:27,604] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:42:37,615] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:42:37,615] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:42:37,784] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:42:37,784] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:42:37,874] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:42:47,885] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:42:47,885] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:42:48,054] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:42:48,054] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:42:48,178] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:42:58,189] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:42:58,189] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:42:58,394] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ErrImagePull       0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:42:58,395] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:42:58,492] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:43:08,503] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:43:08,504] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:43:08,669] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:43:08,669] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:43:08,756] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:43:18,767] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:43:18,767] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:43:18,941] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:43:18,941] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:43:19,013] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:43:29,024] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:43:29,024] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:43:29,187] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:43:29,187] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:43:29,273] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:43:39,284] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:43:39,284] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:43:39,448] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:43:39,448] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:43:39,536] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:43:49,547] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:43:49,547] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:43:49,712] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:43:49,712] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:43:49,799] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:43:59,810] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:43:59,811] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:43:59,978] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:43:59,978] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:44:00,085] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:44:10,097] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:44:10,097] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:44:10,262] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:44:10,262] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:44:10,351] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:44:20,362] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:44:20,362] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:44:20,526] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:44:20,526] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:44:20,621] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:44:30,632] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:44:30,632] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:44:30,798] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:44:30,798] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:44:30,886] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:44:40,897] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:44:40,897] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:44:41,058] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:44:41,058] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:44:41,156] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:44:51,166] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:44:51,167] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:44:51,339] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:44:51,339] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:44:51,447] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:01,458] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:01,458] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:45:01,636] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:45:01,636] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:01,724] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:11,735] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:11,735] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:45:11,903] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:45:11,903] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:12,003] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:22,014] 792  WARNING MainThread kube_helper.wait_for_pods_healthy:: Some pods are not Running or Completed: {'server-pod-dep-7fc68f8c47-85drg': 'ImagePullBackOff', 'server-pod-dep-7fc68f8c47-957dw': 'ImagePullBackOff', 'coredns-6bc668cd76-vlc6w': 'Pending'}
[2020-06-11 05:45:22,014] 981  INFO  MainThread kube_helper.dump_pods_info:: ------- Dump pods info --------
[2020-06-11 05:45:22,014] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:22,014] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pods --all-namespaces -o wide | grep -v -e Running -e Completed'
[2020-06-11 05:45:22,179] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS             RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg            0/1     ImagePullBackOff   0          2d2h    172.16.43.9       worker-0       <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw            0/1     ImagePullBackOff   0          2d2h    172.16.43.10      worker-0       <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w                   0/1     Pending            0          47h     <none>            <none>         <none>           <none>
controller-0:~$ 
[2020-06-11 05:45:22,179] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:22,267] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:22,267] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:22,267] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pods --all-namespaces -o wide | grep -v -e Running -e Completed -e NAMESPACE | awk '{system("kubectl describe pods -n "$1" "$2)}''
[2020-06-11 05:45:22,711] 426  DEBUG MainThread ssh.expect  :: Output: 
Name:         server-pod-dep-7fc68f8c47-85drg
Namespace:    default
Priority:     0
Node:         worker-0/192.168.104.149
Start Time:   Tue, 09 Jun 2020 02:47:21 +0000
Labels:       pod-template-hash=7fc68f8c47
              server=pod-to-pod
Annotations:  cni.projectcalico.org/podIP: 172.16.43.9/32
              k8s.v1.cni.cncf.io/networks-status:
                [{
                    "name": "chain",
                    "ips": [
                        "172.16.43.9"
                    ],
                    "default": true,
                    "dns": {}
                }]
Status:       Pending
IP:           172.16.43.9
IPs:
  IP:           172.16.43.9
Controlled By:  ReplicaSet/server-pod-dep-7fc68f8c47
Containers:
  server-container:
    Container ID:   
    Image:          gcr.io/google-samples/node-hello:1.0
    Image ID:       
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ImagePullBackOff
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bxdw5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-bxdw5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bxdw5
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 30s
                 node.kubernetes.io/unreachable:NoExecute for 30s
Events:
  Type     Reason          Age                     From               Message
  ----     ------          ----                    ----               -------
  Normal   Scheduled       <unknown>               default-scheduler  Successfully assigned default/server-pod-dep-7fc68f8c47-85drg to worker-0
  Normal   SandboxChanged  2d2h (x5 over 2d2h)     kubelet, worker-0  Pod sandbox changed, it will be killed and re-created.
  Normal   Pulling         4h37m (x522 over 2d2h)  kubelet, worker-0  Pulling image "gcr.io/google-samples/node-hello:1.0"
  Warning  Failed          67m (x559 over 2d2h)    kubelet, worker-0  Error: ErrImagePull
  Normal   BackOff         17m (x12785 over 2d2h)  kubelet, worker-0  Back-off pulling image "gcr.io/google-samples/node-hello:1.0"
  Warning  Failed          12m (x12807 over 2d2h)  kubelet, worker-0  Error: ImagePullBackOff
  Warning  Failed          2m42s (x562 over 2d2h)  kubelet, worker-0  Failed to pull image "gcr.io/google-samples/node-hello:1.0": rpc error: code = Unknown desc = Error response from daemon: Get https://gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
Name:         server-pod-dep-7fc68f8c47-957dw
Namespace:    default
Priority:     0
Node:         worker-0/192.168.104.149
Start Time:   Tue, 09 Jun 2020 02:47:21 +0000
Labels:       pod-template-hash=7fc68f8c47
              server=pod-to-pod
Annotations:  cni.projectcalico.org/podIP: 172.16.43.10/32
              k8s.v1.cni.cncf.io/networks-status:
                [{
                    "name": "chain",
                    "ips": [
                        "172.16.43.10"
                    ],
                    "default": true,
                    "dns": {}
                }]
Status:       Pending
IP:           172.16.43.10
IPs:
  IP:           172.16.43.10
Controlled By:  ReplicaSet/server-pod-dep-7fc68f8c47
Containers:
  server-container:
    Container ID:   
    Image:          gcr.io/google-samples/node-hello:1.0
    Image ID:       
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ImagePullBackOff
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-bxdw5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-bxdw5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-bxdw5
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 30s
                 node.kubernetes.io/unreachable:NoExecute for 30s
Events:
  Type     Reason          Age                       From               Message
  ----     ------          ----                      ----               -------
  Normal   Scheduled       <unknown>                 default-scheduler  Successfully assigned default/server-pod-dep-7fc68f8c47-957dw to worker-0
  Normal   SandboxChanged  2d2h (x3 over 2d2h)       kubelet, worker-0  Pod sandbox changed, it will be killed and re-created.
  Warning  Failed          2d2h                      kubelet, worker-0  Failed to pull image "gcr.io/google-samples/node-hello:1.0": rpc error: code = Unknown desc = Error response from daemon: Get https://gcr.io/v2/: dial tcp: lookup gcr.io on 192.168.104.1:53: read udp 192.168.104.149:55593->192.168.104.1:53: read: connection refused
  Warning  Failed          2d2h                      kubelet, worker-0  Failed to pull image "gcr.io/google-samples/node-hello:1.0": rpc error: code = Unknown desc = Error response from daemon: Get https://gcr.io/v2/: dial tcp: lookup gcr.io on 192.168.104.1:53: read udp 192.168.104.149:35609->192.168.104.1:53: i/o timeout
  Warning  Failed          35h (x6 over 47h)         kubelet, worker-0  (combined from similar events): Failed to pull image "gcr.io/google-samples/node-hello:1.0": rpc error: code = Unknown desc = Error response from daemon: Get https://gcr.io/v2/: dial tcp: lookup gcr.io on 192.168.104.1:53: read udp 192.168.104.149:41533->192.168.104.1:53: i/o timeout
  Warning  Failed          3h7m (x536 over 2d2h)     kubelet, worker-0  Error: ErrImagePull
  Normal   Pulling         107m (x553 over 2d2h)     kubelet, worker-0  Pulling image "gcr.io/google-samples/node-hello:1.0"
  Normal   BackOff         52m (x12641 over 2d2h)    kubelet, worker-0  Back-off pulling image "gcr.io/google-samples/node-hello:1.0"
  Warning  Failed          37m (x551 over 2d2h)      kubelet, worker-0  Failed to pull image "gcr.io/google-samples/node-hello:1.0": rpc error: code = Unknown desc = Error response from daemon: Get https://gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
  Warning  Failed          2m52s (x12855 over 2d2h)  kubelet, worker-0  Error: ImagePullBackOff
Name:                 coredns-6bc668cd76-vlc6w
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               k8s-app=kube-dns
                      pod-template-hash=6bc668cd76
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/coredns-6bc668cd76
Containers:
  coredns:
    Image:       registry.local:9001/k8s.gcr.io/coredns:1.6.2
    Ports:       53/UDP, 53/TCP, 9153/TCP
    Host Ports:  0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from coredns-token-cklq8 (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  coredns-token-cklq8:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  coredns-token-cklq8
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
                 node-role.kubernetes.io/master=
Tolerations:     CriticalAddonsOnly
                 node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 30s
                 node.kubernetes.io/unreachable:NoExecute for 30s
Events:
  Type     Reason            Age        From               Message
  ----     ------            ----       ----               -------
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules.
controller-0:~$ 
[2020-06-11 05:45:22,711] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:22,829] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:23,003] 54   DEBUG MainThread conftest.update_results:: ***Failure at test call: /root/yuchengde/test/automated-pytest-suite/keywords/kube_helper.py:796: utils.exceptions.KubeError: Kubernetes error.
***Details: host_type = 'controller'

    @mark.parametrize('host_type', [
        param('controller', marks=mark.priorities('platform_sanity',
                                                  'sanity', 'cpe_sanity')),
        param('compute', marks=mark.priorities('platform_sanity')),
        param('storage', marks=mark.priorities('platform_sanity')),
    ])
    def test_lock_unlock_host(host_type):
        """
        Verify lock unlock host
    
        Test Steps:
            - Select a host per given type. If type is controller, select
                standby controller.
            - Lock selected host and ensure it is successfully locked
            - Unlock selected host and ensure it is successfully unlocked
    
        """
        LOG.tc_step("Select a {} node from system if any".format(host_type))
        if host_type == 'controller':
            if system_helper.is_aio_simplex():
                host = 'controller-0'
            else:
                host = system_helper.get_standby_controller_name()
                assert host, "No standby controller available"
    
        else:
            if host_type == 'compute' and system_helper.is_aio_system():
                skip("No compute host on AIO system")
            elif host_type == 'storage' and not system_helper.is_storage_system():
                skip("System does not have storage nodes")
    
            hosts = system_helper.get_hosts(personality=host_type,
                                            availability=HostAvailState.AVAILABLE,
                                            operational=HostOperState.ENABLED)
    
            assert hosts, "No good {} host on system".format(host_type)
            host = hosts[0]
    
        LOG.tc_step("Lock {} host - {} and ensure it is successfully "
                    "locked".format(host_type, host))
        HostsToRecover.add(host)
        host_helper.lock_host(host, swact=False)
    
        # wait for services to stabilize before unlocking
        time.sleep(20)
    
        # unlock standby controller node and verify controller node is
        # successfully unlocked
        LOG.tc_step("Unlock {} host - {} and ensure it is successfully "
                    "unlocked".format(host_type, host))
>       host_helper.unlock_host(host)

testcases/functional/mtc/test_lock_unlock_host.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
keywords/host_helper.py:971: in unlock_host
    all_namespaces=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pod_names = None, namespace = None, all_namespaces = True, labels = None, timeout = 300, check_interval = 10
con_ssh = <utils.clients.ssh.SSHClient object at 0x7f7e8cc406d8>, fail_ok = False, exclude = True, strict = False, kwargs = {'name': [], 'node': 'controller-0'}
bad_pods = {'coredns-6bc668cd76-vlc6w': 'Pending', 'server-pod-dep-7fc68f8c47-85drg': 'ImagePullBackOff', 'server-pod-dep-7fc68f8c47-957dw': 'ImagePullBackOff'}
end_time = 1591854313.5711815
bad_pods_info = [('server-pod-dep-7fc68f8c47-85drg', 'ImagePullBackOff'), ('server-pod-dep-7fc68f8c47-957dw', 'ImagePullBackOff'), ('coredns-6bc668cd76-vlc6w', 'Pending')]
msg = "Some pods are not Running or Completed: {'server-pod-dep-7fc68f8c47-85drg': 'ImagePullBackOff', 'server-pod-dep-7fc68f8c47-957dw': 'ImagePullBackOff', 'coredns-6bc668cd76-vlc6w': 'Pending'}"

    def wait_for_pods_healthy(pod_names=None, namespace=None, all_namespaces=True,
                              labels=None, timeout=300,
                              check_interval=5, con_ssh=None, fail_ok=False,
                              exclude=False, strict=False, **kwargs):
        """
        Wait for pods ready
        Args:
            pod_names (list|tuple|str|None): full name of pod(s)
            namespace (str|None):
            all_namespaces (bool|None)
            labels (str|dict|list|tuple|None):
            timeout:
            check_interval:
            con_ssh:
            fail_ok:
            exclude (bool)
            strict (bool): strict applies to node and name matching if given
            **kwargs
    
        Returns (tuple):
    
        """
        LOG.info("Wait for pods ready..")
        if not pod_names:
            pod_names = None
        elif isinstance(pod_names, str):
            pod_names = [pod_names]
    
        bad_pods = None
        end_time = time.time() + timeout
        while time.time() < end_time:
            bad_pods_info = get_unhealthy_pods(labels=labels,
                                               field=('NAME', 'STATUS'),
                                               namespace=namespace,
                                               all_namespaces=all_namespaces,
                                               con_ssh=con_ssh, exclude=exclude,
                                               strict=strict, **kwargs)
            bad_pods = {pod_info[0]: pod_info[1] for pod_info in bad_pods_info if
                        (not pod_names or pod_info[0] in pod_names)}
            if not bad_pods:
                LOG.info("Pods are Completed or Running.")
                if pod_names:
                    pod_names = [pod for pod in pod_names if
                                 not re.search('audit-|init-', pod)]
                    if not pod_names:
                        return True
    
                is_ready = wait_for_running_pods_ready(
                    pod_names=pod_names,
                    namespace=namespace,
                    all_namespaces=all_namespaces,
                    labels=labels, timeout=int(end_time - time.time()),
                    strict=strict,
                    con_ssh=con_ssh,
                    fail_ok=fail_ok, **kwargs)
                return is_ready
            time.sleep(check_interval)
    
        msg = 'Some pods are not Running or Completed: {}'.format(bad_pods)
        LOG.warning(msg)
        if fail_ok:
            return False
        dump_pods_info(con_ssh=con_ssh)
>       raise exceptions.KubeError(msg)
E       utils.exceptions.KubeError: Kubernetes error.
E       Details: Some pods are not Running or Completed: {'server-pod-dep-7fc68f8c47-85drg': 'ImagePullBackOff', 'server-pod-dep-7fc68f8c47-957dw': 'ImagePullBackOff', 'coredns-6bc668cd76-vlc6w': 'Pending'}

keywords/kube_helper.py:796: KubeError
[2020-06-11 05:45:26,018] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:45:26,019] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:45:26,103] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:45:26,103] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 05:45:29,107] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:45:29,107] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:45:29,158] 426  DEBUG MainThread ssh.expect  :: Output: 
]0;root@forstarlingxtest:~/yuchengde/test/automated-pytest-suiteroot@forstarlingxtest$ 
[2020-06-11 05:45:29,159] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 05:45:29,159] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[controller]
[2020-06-11 05:45:29,159] 115  INFO  MainThread recover_hosts._recover_hosts:: 
====================== Teardown Step 1: function Recover simplex host
[2020-06-11 05:45:29,160] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:45:29,160] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:45:29,262] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:45:29,263] 1296 INFO  MainThread host_helper._wait_for_openstack_cli_enable:: Waiting for system cli and subfunctions to be ready and nova cli (if stx-openstack applied) to be enabled on active controller
[2020-06-11 05:45:29,263] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:29,263] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne show'
[2020-06-11 05:45:30,143] 426  DEBUG MainThread ssh.expect  :: Output: 
+----------------------+--------------------------------------+
| Property             | Value                                |
+----------------------+--------------------------------------+
| contact              | None                                 |
| created_at           | 2017-01-01T20:00:35.851917+00:00     |
| description          | None                                 |
| https_enabled        | False                                |
| location             | None                                 |
| name                 | 03a8f8c4-9883-4c0d-a430-63370202a9d0 |
| region_name          | RegionOne                            |
| sdn_enabled          | False                                |
| security_feature     | spectre_meltdown_v1                  |
| service_project_name | services                             |
| software_version     | 19.12                                |
| system_mode          | duplex                               |
| system_type          | All-in-one                           |
| timezone             | UTC                                  |
| updated_at           | 2020-05-12T07:22:44.454374+00:00     |
| uuid                 | 3f620fb2-4ceb-441a-b7d7-8800c6c585f3 |
| vswitch_type         | none                                 |
+----------------------+--------------------------------------+
controller-0:~$ 
[2020-06-11 05:45:30,143] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:30,228] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:40,239] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:40,239] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne servicegroup-list'
[2020-06-11 05:45:41,399] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+-----------------------------+--------------+--------+
| uuid                                 | service_group_name          | hostname     | state  |
+--------------------------------------+-----------------------------+--------------+--------+
| 6d74599f-5aa5-4667-92b8-bcf065b97566 | cloud-services              | controller-0 | active |
| e6f0d8e5-8313-4c07-8597-06f87ff72758 | controller-services         | controller-0 | active |
| 0bd309c1-616b-4b3c-b0fa-588b5d59a6ed | directory-services          | controller-0 | active |
| db0096b5-0946-4518-ba8c-07676312736c | oam-services                | controller-0 | active |
| f2a7b29a-9e00-4c1a-ac9d-646d28b22e88 | patching-services           | controller-0 | active |
| 6ba173e0-cb68-44f8-817f-bc330ca41322 | storage-monitoring-services | controller-0 | active |
| ff7a236c-d5cb-4b2c-af44-4990e774fc76 | storage-services            | controller-0 | active |
| 39603c17-ae56-4106-bb4d-4e89c8bd14a6 | vim-services                | controller-0 | active |
| 1a32d85a-3abf-44c8-a5e5-9a6da9d88068 | web-services                | controller-0 | active |
+--------------------------------------+-----------------------------+--------------+--------+
controller-0:~$ 
[2020-06-11 05:45:41,399] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:41,485] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:41,486] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): ['controller-0']
[2020-06-11 05:45:41,486] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): []
[2020-06-11 05:45:41,486] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:45:41,486] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:41,486] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:45:42,396] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 3dfced72-40c6-43fc-b224-fec81b9df26e                                 |
| config_status         | None                                                                 |
| config_target         | 3dfced72-40c6-43fc-b224-fec81b9df26e                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:45:17.142695+00:00                                     |
| uptime                | 703                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 05:45:42,397] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:42,485] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:42,486] 541  INFO  MainThread host_helper.wait_for_task_clear_and_subfunction_ready:: Waiting for task clear and subfunctions enable/available (if applicable) for hosts: ['controller-0']
[2020-06-11 05:45:42,486] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:42,486] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:45:43,448] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 3dfced72-40c6-43fc-b224-fec81b9df26e                                 |
| config_status         | None                                                                 |
| config_target         | 3dfced72-40c6-43fc-b224-fec81b9df26e                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:45:42.554116+00:00                                     |
| uptime                | 703                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 05:45:43,448] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:43,533] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:43,534] 557  INFO  MainThread host_helper.wait_for_task_clear_and_subfunction_ready:: Hosts task cleared and subfunctions (if applicable) are now in enabled/available states
[2020-06-11 05:45:43,534] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:43,534] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:45:44,407] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:45:44,408] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:44,494] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:44,495] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 05:45:44,495] 1284 INFO  MainThread host_helper.check_sysinv_cli:: system cli and subfunction enabled
[2020-06-11 05:45:44,495] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:44,495] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:45:45,418] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 3dfced72-40c6-43fc-b224-fec81b9df26e                                 |
| config_status         | None                                                                 |
| config_target         | 3dfced72-40c6-43fc-b224-fec81b9df26e                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:45:42.554116+00:00                                     |
| uptime                | 703                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 05:45:45,418] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:45,505] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:45,505] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:45,505] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 05:45:46,429] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
controller-0:~$ 
[2020-06-11 05:45:46,429] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:46,518] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:46,518] 244  DEBUG MainThread system_helper.get_hosts:: Filtered hosts: []
[2020-06-11 05:45:46,519] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:46,519] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 05:45:47,438] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
controller-0:~$ 
[2020-06-11 05:45:47,439] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:47,524] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:47,525] 244  DEBUG MainThread system_helper.get_hosts:: Filtered hosts: ['controller-0']
[2020-06-11 05:45:47,525] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:47,525] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 05:45:48,415] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
controller-0:~$ 
[2020-06-11 05:45:48,415] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:48,505] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:48,505] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): ['controller-0']
[2020-06-11 05:45:48,506] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): ['worker-0']
[2020-06-11 05:45:48,506] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): []
[2020-06-11 05:45:48,506] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:48,506] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:45:49,448] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:45:49,448] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:49,559] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:49,559] 845  INFO  MainThread container_helper.is_stx_openstack_deployed:: []
[2020-06-11 05:45:49,559] 480  INFO  MainThread host_helper.wait_for_hosts_ready:: Wait for hosts to be available: ['controller-0']
[2020-06-11 05:45:49,559] 3492 INFO  MainThread system_helper.wait_for_hosts_states:: Waiting for ['controller-0'] to reach state(s): {'availability': ['available']}...
[2020-06-11 05:45:49,560] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:49,560] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 05:45:50,428] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
controller-0:~$ 
[2020-06-11 05:45:50,429] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:50,525] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:50,526] 244  DEBUG MainThread system_helper.get_hosts:: Filtered hosts: {'availability': ['available']}
[2020-06-11 05:45:51,527] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:51,527] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 05:45:52,398] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
controller-0:~$ 
[2020-06-11 05:45:52,399] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:52,484] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:52,484] 244  DEBUG MainThread system_helper.get_hosts:: Filtered hosts: {'availability': ['available']}
[2020-06-11 05:45:53,486] 3498 INFO  MainThread system_helper.wait_for_hosts_states:: ['controller-0'] have reached state(s): {'availability': ['available']}
[2020-06-11 05:45:53,486] 541  INFO  MainThread host_helper.wait_for_task_clear_and_subfunction_ready:: Waiting for task clear and subfunctions enable/available (if applicable) for hosts: ['controller-0']
[2020-06-11 05:45:53,486] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:53,486] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-show controller-0'
[2020-06-11 05:45:54,457] 426  DEBUG MainThread ssh.expect  :: Output: 
+-----------------------+----------------------------------------------------------------------+
| Property              | Value                                                                |
+-----------------------+----------------------------------------------------------------------+
| action                | none                                                                 |
| administrative        | unlocked                                                             |
| availability          | available                                                            |
| bm_ip                 | None                                                                 |
| bm_type               | none                                                                 |
| bm_username           | None                                                                 |
| boot_device           | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| capabilities          | {u'stor_function': u'monitor', u'Personality': u'Controller-Active'} |
| clock_synchronization | ntp                                                                  |
| config_applied        | 3dfced72-40c6-43fc-b224-fec81b9df26e                                 |
| config_status         | None                                                                 |
| config_target         | 3dfced72-40c6-43fc-b224-fec81b9df26e                                 |
| console               | tty0                                                                 |
| created_at            | 2017-01-01T20:01:48.619012+00:00                                     |
| hostname              | controller-0                                                         |
| id                    | 1                                                                    |
| install_output        | text                                                                 |
| install_state         | None                                                                 |
| install_state_info    | None                                                                 |
| inv_state             | inventoried                                                          |
| invprovision          | provisioned                                                          |
| location              | {}                                                                   |
| mgmt_ip               | 192.168.104.2                                                        |
| mgmt_mac              | 54:b2:03:04:b9:13                                                    |
| operational           | enabled                                                              |
| personality           | controller                                                           |
| reserved              | False                                                                |
| rootfs_device         | /dev/disk/by-path/pci-0000:72:00.0-nvme-1                            |
| serialid              | None                                                                 |
| software_load         | 19.12                                                                |
| subfunction_avail     | available                                                            |
| subfunction_oper      | enabled                                                              |
| subfunctions          | controller,worker                                                    |
| task                  |                                                                      |
| tboot                 | false                                                                |
| ttys_dcd              | None                                                                 |
| updated_at            | 2020-06-11T05:45:42.554116+00:00                                     |
| uptime                | 703                                                                  |
| uuid                  | 66cea463-2bf4-490c-be31-89c9c3c56650                                 |
| vim_progress_status   | services-enabled                                                     |
+-----------------------+----------------------------------------------------------------------+
controller-0:~$ 
[2020-06-11 05:45:54,458] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:54,547] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:54,547] 557  INFO  MainThread host_helper.wait_for_task_clear_and_subfunction_ready:: Hosts task cleared and subfunctions (if applicable) are now in enabled/available states
[2020-06-11 05:45:54,548] 499  INFO  MainThread host_helper.wait_for_hosts_ready:: Wait for webservices up for hosts: ['controller-0']
[2020-06-11 05:45:54,548] 1580 INFO  MainThread host_helper.wait_for_webservice_up:: Waiting for ['controller-0'] to be active for web-service in system servicegroup-list...
[2020-06-11 05:45:54,548] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:54,548] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne servicegroup-list'
[2020-06-11 05:45:55,738] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+-----------------------------+--------------+--------+
| uuid                                 | service_group_name          | hostname     | state  |
+--------------------------------------+-----------------------------+--------------+--------+
| 6d74599f-5aa5-4667-92b8-bcf065b97566 | cloud-services              | controller-0 | active |
| e6f0d8e5-8313-4c07-8597-06f87ff72758 | controller-services         | controller-0 | active |
| 0bd309c1-616b-4b3c-b0fa-588b5d59a6ed | directory-services          | controller-0 | active |
| db0096b5-0946-4518-ba8c-07676312736c | oam-services                | controller-0 | active |
| f2a7b29a-9e00-4c1a-ac9d-646d28b22e88 | patching-services           | controller-0 | active |
| 6ba173e0-cb68-44f8-817f-bc330ca41322 | storage-monitoring-services | controller-0 | active |
| ff7a236c-d5cb-4b2c-af44-4990e774fc76 | storage-services            | controller-0 | active |
| 39603c17-ae56-4106-bb4d-4e89c8bd14a6 | vim-services                | controller-0 | active |
| 1a32d85a-3abf-44c8-a5e5-9a6da9d88068 | web-services                | controller-0 | active |
+--------------------------------------+-----------------------------+--------------+--------+
controller-0:~$ 
[2020-06-11 05:45:55,739] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:55,827] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:55,827] 1600 INFO  MainThread host_helper.wait_for_webservice_up:: Host(s) ['controller-0'] are active for web-service in system servicegroup-list
[2020-06-11 05:45:55,827] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:55,827] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get nodes'
[2020-06-11 05:45:56,005] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME           STATUS   ROLES    AGE   VERSION
controller-0   Ready    master   29d   v1.16.2
worker-0       Ready    <none>   29d   v1.16.2
controller-0:~$ 
[2020-06-11 05:45:56,005] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:56,097] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:56,098] 684  INFO  MainThread kube_helper.wait_for_nodes_ready:: All nodes are ready: ['controller-0']
[2020-06-11 05:45:56,098] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 2: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 05:45:56,098] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:45:56,098] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:56,099] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:45:57,247] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 24a96174-80b7-484e-9127-7cd32ebed66b | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                   | host=controller-0.ntp                                                            | major    | 2020-06-11T05:38:59.708120 |
| d5a2b085-7f1a-423d-b36a-cd7a50bff267 | 100.114  | NTP address 5.79.108.34 is not a valid or a reachable NTP server.                                                        | host=controller-0.ntp=5.79.108.34                                                | minor    | 2020-06-11T05:38:59.626433 |
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:45:57,247] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:57,332] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:57,333] 594  INFO  MainThread check_helper.check_alarms:: NTP alarm found, checking ntpq stats
[2020-06-11 05:45:57,333] 3397 INFO  MainThread system_helper.wait_for_ntp_sync:: Waiting for ntp alarm to clear or sudo ntpq -pn indicate unhealthy server for controller-0
[2020-06-11 05:45:57,333] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:45:57,333] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:57,333] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne network-list --nowrap'
[2020-06-11 05:45:58,249] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
| id | uuid                                 | name            | type            | dynamic | pool_uuid                            |
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
| 20 | 2add22bf-3fbe-4e26-8c5a-4e2851e38c63 | cluster-pod     | cluster-pod     | False   | 98b3a90a-60ee-4ab2-b173-bf439e45bf5a |
| 15 | 2d8e1d09-6ab4-4ccc-9c5c-b4d41260e8ff | mgmt            | mgmt            | True    | 6371a279-0093-4be2-9e70-341ddcc6e207 |
| 16 | 6db43265-f8f1-4720-b2f4-456eb11794ed | pxeboot         | pxeboot         | True    | 7b75fdc7-0eed-4c5a-befd-6a1919dd8d73 |
| 19 | 8241f3e5-70c2-4897-abdd-6b72ddb61243 | cluster-host    | cluster-host    | True    | 423925ae-ddda-4339-b7e1-36a11cfe2b01 |
| 18 | 99418e5a-d003-42f9-b257-631e574d2ffc | multicast       | multicast       | False   | 85d5fa1c-7875-47e2-9c56-95012dfebc71 |
| 17 | e4291b7a-ed43-48db-ab94-d3931a63d0b9 | oam             | oam             | False   | 2d416a87-a545-438c-8d9e-cad3ea022904 |
| 21 | e6e5a4a1-81a0-49c5-b038-6d6b00432646 | cluster-service | cluster-service | False   | b9c532e8-9100-4182-8a3e-9d059d632aa1 |
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
controller-0:~$ 
[2020-06-11 05:45:58,249] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:58,345] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:58,346] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:58,346] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne addrpool-show 6371a279-0093-4be2-9e70-341ddcc6e207'
[2020-06-11 05:45:59,292] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+--------------------------------------+
| Property            | Value                                |
+---------------------+--------------------------------------+
| uuid                | 6371a279-0093-4be2-9e70-341ddcc6e207 |
| name                | management                           |
| network             | 192.168.104.0                        |
| prefix              | 24                                   |
| order               | random                               |
| ranges              | ['192.168.104.1-192.168.104.254']    |
| floating_address    | 192.168.104.1                        |
| controller0_address | 192.168.104.2                        |
| controller1_address | 192.168.104.3                        |
| gateway_address     | None                                 |
+---------------------+--------------------------------------+
controller-0:~$ 
[2020-06-11 05:45:59,292] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:45:59,377] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:45:59,377] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:45:59,377] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:46:00,189] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 24a96174-80b7-484e-9127-7cd32ebed66b | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                   | host=controller-0.ntp                                                            | major    | 2020-06-11T05:38:59.708120 |
| d5a2b085-7f1a-423d-b36a-cd7a50bff267 | 100.114  | NTP address 5.79.108.34 is not a valid or a reachable NTP server.                                                        | host=controller-0.ntp=5.79.108.34                                                | minor    | 2020-06-11T05:38:59.626433 |
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:46:00,189] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:00,277] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:00,277] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:00,277] 304  DEBUG MainThread ssh.send    :: Send 'hostname'
[2020-06-11 05:46:00,367] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0
controller-0:~$ 
[2020-06-11 05:46:00,367] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 05:46:00,367] 304  DEBUG MainThread ssh.send    :: Send 'sudo ntpq -pn'
[2020-06-11 05:46:00,455] 426  DEBUG MainThread ssh.expect  :: Output: 
Password: 
[2020-06-11 05:46:00,455] 304  DEBUG MainThread ssh.send    :: Send '99cloud@SH'
[2020-06-11 05:46:00,588] 426  DEBUG MainThread ssh.expect  :: Output: 
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
 192.168.104.3
                 .INIT.          16 u    -   64    0    0.000    0.000   0.000
 139.99.236.185
                 150.101.186.50   3 u   52   64  377  326.643   83.538  43.352
*5.79.108.34
                 130.133.1.10     2 u   51   64  377  266.130  -94.766  33.422
controller-0:~$ 
[2020-06-11 05:46:00,589] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:00,677] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:00,677] 3414 INFO  MainThread system_helper.wait_for_ntp_sync:: Valid NTP alarm
[2020-06-11 05:46:00,677] 594  INFO  MainThread check_helper.check_alarms:: NTP alarm found, checking ntpq stats
[2020-06-11 05:46:00,678] 3397 INFO  MainThread system_helper.wait_for_ntp_sync:: Waiting for ntp alarm to clear or sudo ntpq -pn indicate unhealthy server for controller-0
[2020-06-11 05:46:00,678] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:00,678] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:00,678] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne network-list --nowrap'
[2020-06-11 05:46:01,568] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
| id | uuid                                 | name            | type            | dynamic | pool_uuid                            |
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
| 20 | 2add22bf-3fbe-4e26-8c5a-4e2851e38c63 | cluster-pod     | cluster-pod     | False   | 98b3a90a-60ee-4ab2-b173-bf439e45bf5a |
| 15 | 2d8e1d09-6ab4-4ccc-9c5c-b4d41260e8ff | mgmt            | mgmt            | True    | 6371a279-0093-4be2-9e70-341ddcc6e207 |
| 16 | 6db43265-f8f1-4720-b2f4-456eb11794ed | pxeboot         | pxeboot         | True    | 7b75fdc7-0eed-4c5a-befd-6a1919dd8d73 |
| 19 | 8241f3e5-70c2-4897-abdd-6b72ddb61243 | cluster-host    | cluster-host    | True    | 423925ae-ddda-4339-b7e1-36a11cfe2b01 |
| 18 | 99418e5a-d003-42f9-b257-631e574d2ffc | multicast       | multicast       | False   | 85d5fa1c-7875-47e2-9c56-95012dfebc71 |
| 17 | e4291b7a-ed43-48db-ab94-d3931a63d0b9 | oam             | oam             | False   | 2d416a87-a545-438c-8d9e-cad3ea022904 |
| 21 | e6e5a4a1-81a0-49c5-b038-6d6b00432646 | cluster-service | cluster-service | False   | b9c532e8-9100-4182-8a3e-9d059d632aa1 |
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
controller-0:~$ 
[2020-06-11 05:46:01,569] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:01,664] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:01,665] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:01,665] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne addrpool-show 6371a279-0093-4be2-9e70-341ddcc6e207'
[2020-06-11 05:46:02,578] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+--------------------------------------+
| Property            | Value                                |
+---------------------+--------------------------------------+
| uuid                | 6371a279-0093-4be2-9e70-341ddcc6e207 |
| name                | management                           |
| network             | 192.168.104.0                        |
| prefix              | 24                                   |
| order               | random                               |
| ranges              | ['192.168.104.1-192.168.104.254']    |
| floating_address    | 192.168.104.1                        |
| controller0_address | 192.168.104.2                        |
| controller1_address | 192.168.104.3                        |
| gateway_address     | None                                 |
+---------------------+--------------------------------------+
controller-0:~$ 
[2020-06-11 05:46:02,579] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:02,729] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:02,729] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:02,729] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:46:03,865] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 24a96174-80b7-484e-9127-7cd32ebed66b | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                   | host=controller-0.ntp                                                            | major    | 2020-06-11T05:38:59.708120 |
| d5a2b085-7f1a-423d-b36a-cd7a50bff267 | 100.114  | NTP address 5.79.108.34 is not a valid or a reachable NTP server.                                                        | host=controller-0.ntp=5.79.108.34                                                | minor    | 2020-06-11T05:38:59.626433 |
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:46:03,865] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:03,956] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:03,957] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:03,957] 304  DEBUG MainThread ssh.send    :: Send 'hostname'
[2020-06-11 05:46:04,046] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0
controller-0:~$ 
[2020-06-11 05:46:04,046] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 05:46:04,046] 304  DEBUG MainThread ssh.send    :: Send 'sudo ntpq -pn'
[2020-06-11 05:46:04,170] 426  DEBUG MainThread ssh.expect  :: Output: 
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
 192.168.104.3
                 .INIT.          16 u    -   64    0    0.000    0.000   0.000
 139.99.236.185
                 150.101.186.50   3 u   56   64  377  326.643   83.538  43.352
*5.79.108.34
                 130.133.1.10     2 u   55   64  377  266.130  -94.766  33.422
controller-0:~$ 
[2020-06-11 05:46:04,170] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:04,299] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:04,299] 3414 INFO  MainThread system_helper.wait_for_ntp_sync:: Valid NTP alarm
[2020-06-11 05:46:04,299] 594  INFO  MainThread check_helper.check_alarms:: NTP alarm found, checking ntpq stats
[2020-06-11 05:46:04,300] 3397 INFO  MainThread system_helper.wait_for_ntp_sync:: Waiting for ntp alarm to clear or sudo ntpq -pn indicate unhealthy server for controller-0
[2020-06-11 05:46:04,300] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:04,300] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:04,300] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne network-list --nowrap'
[2020-06-11 05:46:05,196] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
| id | uuid                                 | name            | type            | dynamic | pool_uuid                            |
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
| 20 | 2add22bf-3fbe-4e26-8c5a-4e2851e38c63 | cluster-pod     | cluster-pod     | False   | 98b3a90a-60ee-4ab2-b173-bf439e45bf5a |
| 15 | 2d8e1d09-6ab4-4ccc-9c5c-b4d41260e8ff | mgmt            | mgmt            | True    | 6371a279-0093-4be2-9e70-341ddcc6e207 |
| 16 | 6db43265-f8f1-4720-b2f4-456eb11794ed | pxeboot         | pxeboot         | True    | 7b75fdc7-0eed-4c5a-befd-6a1919dd8d73 |
| 19 | 8241f3e5-70c2-4897-abdd-6b72ddb61243 | cluster-host    | cluster-host    | True    | 423925ae-ddda-4339-b7e1-36a11cfe2b01 |
| 18 | 99418e5a-d003-42f9-b257-631e574d2ffc | multicast       | multicast       | False   | 85d5fa1c-7875-47e2-9c56-95012dfebc71 |
| 17 | e4291b7a-ed43-48db-ab94-d3931a63d0b9 | oam             | oam             | False   | 2d416a87-a545-438c-8d9e-cad3ea022904 |
| 21 | e6e5a4a1-81a0-49c5-b038-6d6b00432646 | cluster-service | cluster-service | False   | b9c532e8-9100-4182-8a3e-9d059d632aa1 |
+----+--------------------------------------+-----------------+-----------------+---------+--------------------------------------+
controller-0:~$ 
[2020-06-11 05:46:05,197] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:05,279] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:05,279] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:05,280] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne addrpool-show 6371a279-0093-4be2-9e70-341ddcc6e207'
[2020-06-11 05:46:06,192] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+--------------------------------------+
| Property            | Value                                |
+---------------------+--------------------------------------+
| uuid                | 6371a279-0093-4be2-9e70-341ddcc6e207 |
| name                | management                           |
| network             | 192.168.104.0                        |
| prefix              | 24                                   |
| order               | random                               |
| ranges              | ['192.168.104.1-192.168.104.254']    |
| floating_address    | 192.168.104.1                        |
| controller0_address | 192.168.104.2                        |
| controller1_address | 192.168.104.3                        |
| gateway_address     | None                                 |
+---------------------+--------------------------------------+
controller-0:~$ 
[2020-06-11 05:46:06,192] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:06,277] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:06,277] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:06,277] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:46:07,459] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 24a96174-80b7-484e-9127-7cd32ebed66b | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                   | host=controller-0.ntp                                                            | major    | 2020-06-11T05:38:59.708120 |
| d5a2b085-7f1a-423d-b36a-cd7a50bff267 | 100.114  | NTP address 5.79.108.34 is not a valid or a reachable NTP server.                                                        | host=controller-0.ntp=5.79.108.34                                                | minor    | 2020-06-11T05:38:59.626433 |
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:46:07,459] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:07,545] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:07,545] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:07,545] 304  DEBUG MainThread ssh.send    :: Send 'hostname'
[2020-06-11 05:46:07,632] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0
controller-0:~$ 
[2020-06-11 05:46:07,632] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 05:46:07,632] 304  DEBUG MainThread ssh.send    :: Send 'sudo ntpq -pn'
[2020-06-11 05:46:07,738] 426  DEBUG MainThread ssh.expect  :: Output: 
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
 192.168.104.3
                 .INIT.          16 u    -   64    0    0.000    0.000   0.000
 139.99.236.185
                 150.101.186.50   3 u   59   64  377  326.643   83.538  43.352
*5.79.108.34
                 130.133.1.10     2 u   58   64  377  266.130  -94.766  33.422
controller-0:~$ 
[2020-06-11 05:46:07,738] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:07,827] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:07,828] 3414 INFO  MainThread system_helper.wait_for_ntp_sync:: Valid NTP alarm
[2020-06-11 05:46:07,828] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:07,828] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:07,828] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:46:08,758] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:46:08,759] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:08,846] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:08,846] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 05:46:08,847] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:08,847] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:08,847] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:46:09,022] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:46:09,022] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:09,111] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:09,112] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 05:46:09,112] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:09,112] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:09,112] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 05:46:09,282] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 05:46:09,283] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:09,354] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:09,355] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 05:46:09,356] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[controller] - Test Failed at test call

[2020-06-11 05:46:09,358] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[compute]
[2020-06-11 05:46:09,359] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 05:46:09,359] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:09,359] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:09,359] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:46:10,121] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 24a96174-80b7-484e-9127-7cd32ebed66b | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                   | host=controller-0.ntp                                                            | major    | 2020-06-11T05:38:59.708120 |
| d5a2b085-7f1a-423d-b36a-cd7a50bff267 | 100.114  | NTP address 5.79.108.34 is not a valid or a reachable NTP server.                                                        | host=controller-0.ntp=5.79.108.34                                                | minor    | 2020-06-11T05:38:59.626433 |
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:46:10,122] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:10,206] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:10,207] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:10,207] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:10,207] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:46:10,372] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:46:10,372] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:10,499] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:10,499] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:10,499] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:10,499] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:46:11,400] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:46:11,400] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:11,529] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:11,530] 213  DEBUG MainThread conftest.testcase_log:: 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test steps started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[compute]
[2020-06-11 05:46:11,530] 60   INFO  MainThread test_lock_unlock_host.test_lock_unlock_host:: 
====================== Test Step 1: Select a compute node from system if any
[2020-06-11 05:46:14,535] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:46:14,536] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:46:14,620] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:46:14,621] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 05:46:17,624] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:46:17,625] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:46:17,676] 426  DEBUG MainThread ssh.expect  :: Output: 
]0;root@forstarlingxtest:~/yuchengde/test/automated-pytest-suiteroot@forstarlingxtest$ 
[2020-06-11 05:46:17,676] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 05:46:17,676] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[compute]
[2020-06-11 05:46:17,677] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 1: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 05:46:17,677] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:17,677] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:17,677] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:46:18,488] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 24a96174-80b7-484e-9127-7cd32ebed66b | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                   | host=controller-0.ntp                                                            | major    | 2020-06-11T05:38:59.708120 |
| d5a2b085-7f1a-423d-b36a-cd7a50bff267 | 100.114  | NTP address 5.79.108.34 is not a valid or a reachable NTP server.                                                        | host=controller-0.ntp=5.79.108.34                                                | minor    | 2020-06-11T05:38:59.626433 |
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:46:18,489] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:18,584] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:18,585] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:18,585] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:18,585] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:46:19,483] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:46:19,483] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:19,569] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:19,570] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 05:46:19,570] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:19,570] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:19,571] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:46:19,736] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:46:19,737] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:19,849] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:19,850] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 05:46:19,850] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:19,850] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:19,850] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 05:46:20,039] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 05:46:20,040] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:20,126] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:20,126] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 05:46:20,128] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[compute] - Test Skipped
Reason: No compute host on AIO system

[2020-06-11 05:46:20,129] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[storage]
[2020-06-11 05:46:20,130] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 05:46:20,130] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:20,131] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:20,131] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:46:20,899] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 24a96174-80b7-484e-9127-7cd32ebed66b | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                   | host=controller-0.ntp                                                            | major    | 2020-06-11T05:38:59.708120 |
| d5a2b085-7f1a-423d-b36a-cd7a50bff267 | 100.114  | NTP address 5.79.108.34 is not a valid or a reachable NTP server.                                                        | host=controller-0.ntp=5.79.108.34                                                | minor    | 2020-06-11T05:38:59.626433 |
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:46:20,899] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:20,985] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:20,986] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:20,986] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:20,986] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:46:21,161] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:46:21,161] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:21,254] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:21,254] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:21,254] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:21,254] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:46:22,129] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:46:22,130] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:22,216] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:22,218] 213  DEBUG MainThread conftest.testcase_log:: 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test steps started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[storage]
[2020-06-11 05:46:22,218] 60   INFO  MainThread test_lock_unlock_host.test_lock_unlock_host:: 
====================== Test Step 1: Select a storage node from system if any
[2020-06-11 05:46:25,223] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:46:25,223] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:46:25,316] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:46:25,317] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 05:46:28,320] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:46:28,320] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:46:28,372] 426  DEBUG MainThread ssh.expect  :: Output: 
]0;root@forstarlingxtest:~/yuchengde/test/automated-pytest-suiteroot@forstarlingxtest$ 
[2020-06-11 05:46:28,372] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 05:46:28,372] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[storage]
[2020-06-11 05:46:28,373] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 1: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 05:46:28,373] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:28,373] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:28,373] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:46:29,199] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 24a96174-80b7-484e-9127-7cd32ebed66b | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                   | host=controller-0.ntp                                                            | major    | 2020-06-11T05:38:59.708120 |
| d5a2b085-7f1a-423d-b36a-cd7a50bff267 | 100.114  | NTP address 5.79.108.34 is not a valid or a reachable NTP server.                                                        | host=controller-0.ntp=5.79.108.34                                                | minor    | 2020-06-11T05:38:59.626433 |
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:46:29,200] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:29,289] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:29,290] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:29,290] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:29,290] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:46:30,164] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:46:30,165] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:30,279] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:30,279] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 05:46:30,279] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:30,279] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:30,280] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:46:30,444] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:46:30,445] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:30,531] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:30,531] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 05:46:30,531] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:30,531] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:30,532] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 05:46:30,704] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 05:46:30,705] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:30,807] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:30,807] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 05:46:30,809] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/mtc/test_lock_unlock_host.py::test_lock_unlock_host[storage] - Test Skipped
Reason: System does not have storage nodes

[2020-06-11 05:46:30,810] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/mtc/test_swact.py::test_swact_controller_platform
[2020-06-11 05:46:30,811] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:30,811] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:30,811] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 05:46:31,681] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
controller-0:~$ 
[2020-06-11 05:46:31,681] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:31,767] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:31,767] 244  DEBUG MainThread system_helper.get_hosts:: Filtered hosts: ['controller-0']
[2020-06-11 05:46:31,767] 83   INFO  MainThread pre_checks_and_configs.wait_for_con_drbd_sync_complete:: Less than two controllers on system. Do not wait for drbd sync
[2020-06-11 05:46:31,769] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 05:46:31,769] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:31,769] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:31,769] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:46:32,528] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 24a96174-80b7-484e-9127-7cd32ebed66b | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                   | host=controller-0.ntp                                                            | major    | 2020-06-11T05:38:59.708120 |
| d5a2b085-7f1a-423d-b36a-cd7a50bff267 | 100.114  | NTP address 5.79.108.34 is not a valid or a reachable NTP server.                                                        | host=controller-0.ntp=5.79.108.34                                                | minor    | 2020-06-11T05:38:59.626433 |
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:46:32,529] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:32,615] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:32,615] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:32,615] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:32,615] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:46:32,791] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:46:32,792] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:32,909] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:32,909] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:32,909] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:32,910] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:46:33,859] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:46:33,860] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:33,979] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:33,980] 213  DEBUG MainThread conftest.testcase_log:: 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test steps started for: testcases/functional/mtc/test_swact.py::test_swact_controller_platform
[2020-06-11 05:46:36,985] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:46:36,986] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:46:37,080] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:46:37,080] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 05:46:40,084] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:46:40,084] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:46:40,135] 426  DEBUG MainThread ssh.expect  :: Output: 
]0;root@forstarlingxtest:~/yuchengde/test/automated-pytest-suiteroot@forstarlingxtest$ 
[2020-06-11 05:46:40,136] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 05:46:40,136] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/mtc/test_swact.py::test_swact_controller_platform
[2020-06-11 05:46:40,136] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 1: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 05:46:40,137] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:40,137] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:40,137] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:46:40,898] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 24a96174-80b7-484e-9127-7cd32ebed66b | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                   | host=controller-0.ntp                                                            | major    | 2020-06-11T05:38:59.708120 |
| d5a2b085-7f1a-423d-b36a-cd7a50bff267 | 100.114  | NTP address 5.79.108.34 is not a valid or a reachable NTP server.                                                        | host=controller-0.ntp=5.79.108.34                                                | minor    | 2020-06-11T05:38:59.626433 |
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:46:40,898] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:40,986] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:40,987] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:40,987] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:40,987] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:46:41,939] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:46:41,939] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:42,024] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:42,025] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 05:46:42,025] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:42,025] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:42,025] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:46:42,210] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:46:42,210] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:42,311] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:42,311] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 05:46:42,311] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:46:42,312] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:46:42,312] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 05:46:42,506] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 05:46:42,506] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:46:42,629] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:46:42,629] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 05:46:42,630] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/mtc/test_swact.py::test_swact_controller_platform - Test Skipped
Reason: Simplex system detected

[2020-06-11 05:46:42,632] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_custom_containers.py::test_launch_pod_via_kubectl[active]
[2020-06-11 05:46:45,637] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:46:45,638] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:46:45,722] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:46:45,722] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 05:46:48,726] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:46:48,726] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:46:48,778] 426  DEBUG MainThread ssh.expect  :: Output: 
]0;root@forstarlingxtest:~/yuchengde/test/automated-pytest-suiteroot@forstarlingxtest$ 
[2020-06-11 05:46:48,778] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 05:46:48,778] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_custom_containers.py::test_launch_pod_via_kubectl[active]
[2020-06-11 05:46:48,778] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_custom_containers.py::test_launch_pod_via_kubectl[active] - Test Skipped
Reason: Shared Test File Server is not ready

[2020-06-11 05:46:48,780] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_custom_containers.py::test_launch_pod_via_kubectl[standby]
[2020-06-11 05:46:51,785] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:46:51,785] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:46:51,870] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:46:51,870] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 05:46:54,873] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:46:54,874] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:46:54,925] 426  DEBUG MainThread ssh.expect  :: Output: 
]0;root@forstarlingxtest:~/yuchengde/test/automated-pytest-suiteroot@forstarlingxtest$ 
[2020-06-11 05:46:54,925] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 05:46:54,925] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_custom_containers.py::test_launch_pod_via_kubectl[standby]
[2020-06-11 05:46:54,926] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_custom_containers.py::test_launch_pod_via_kubectl[standby] - Test Skipped
Reason: Shared Test File Server is not ready

[2020-06-11 05:46:54,927] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_custom_containers.py::test_launch_app_via_sysinv
[2020-06-11 05:46:57,932] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:46:57,932] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:46:58,017] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:46:58,017] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 05:47:01,021] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:47:01,021] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:47:01,073] 426  DEBUG MainThread ssh.expect  :: Output: 
]0;root@forstarlingxtest:~/yuchengde/test/automated-pytest-suiteroot@forstarlingxtest$ 
[2020-06-11 05:47:01,073] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 05:47:01,073] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_custom_containers.py::test_launch_app_via_sysinv
[2020-06-11 05:47:01,074] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_custom_containers.py::test_launch_app_via_sysinv - Test Skipped
Reason: Shared Test File Server is not ready

[2020-06-11 05:47:01,075] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_custom_containers.py::test_push_docker_image_to_local_registry[active]
[2020-06-11 05:47:01,076] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 05:47:01,076] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:01,076] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:01,076] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:47:01,840] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 24a96174-80b7-484e-9127-7cd32ebed66b | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                   | host=controller-0.ntp                                                            | major    | 2020-06-11T05:38:59.708120 |
| d5a2b085-7f1a-423d-b36a-cd7a50bff267 | 100.114  | NTP address 5.79.108.34 is not a valid or a reachable NTP server.                                                        | host=controller-0.ntp=5.79.108.34                                                | minor    | 2020-06-11T05:38:59.626433 |
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:47:01,840] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:01,979] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:01,979] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:01,979] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:01,980] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:47:02,143] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d2h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d2h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:47:02,143] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:02,231] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:02,232] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:02,232] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:02,232] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:47:03,147] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:47:03,148] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:03,269] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:03,271] 213  DEBUG MainThread conftest.testcase_log:: 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test steps started for: testcases/functional/z_containers/test_custom_containers.py::test_push_docker_image_to_local_registry[active]
[2020-06-11 05:47:03,271] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:03,271] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:03,271] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne servicegroup-list'
[2020-06-11 05:47:04,434] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+-----------------------------+--------------+--------+
| uuid                                 | service_group_name          | hostname     | state  |
+--------------------------------------+-----------------------------+--------------+--------+
| 6d74599f-5aa5-4667-92b8-bcf065b97566 | cloud-services              | controller-0 | active |
| e6f0d8e5-8313-4c07-8597-06f87ff72758 | controller-services         | controller-0 | active |
| 0bd309c1-616b-4b3c-b0fa-588b5d59a6ed | directory-services          | controller-0 | active |
| db0096b5-0946-4518-ba8c-07676312736c | oam-services                | controller-0 | active |
| f2a7b29a-9e00-4c1a-ac9d-646d28b22e88 | patching-services           | controller-0 | active |
| 6ba173e0-cb68-44f8-817f-bc330ca41322 | storage-monitoring-services | controller-0 | active |
| ff7a236c-d5cb-4b2c-af44-4990e774fc76 | storage-services            | controller-0 | active |
| 39603c17-ae56-4106-bb4d-4e89c8bd14a6 | vim-services                | controller-0 | active |
| 1a32d85a-3abf-44c8-a5e5-9a6da9d88068 | web-services                | controller-0 | active |
+--------------------------------------+-----------------------------+--------------+--------+
controller-0:~$ 
[2020-06-11 05:47:04,435] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:04,521] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:04,521] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): ['controller-0']
[2020-06-11 05:47:04,522] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): []
[2020-06-11 05:47:04,522] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:04,522] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:04,522] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 05:47:05,423] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
controller-0:~$ 
[2020-06-11 05:47:05,423] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:05,503] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:05,504] 244  DEBUG MainThread system_helper.get_hosts:: Filtered hosts: ['controller-0']
[2020-06-11 05:47:05,504] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:05,504] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:05,505] 304  DEBUG MainThread ssh.send    :: Send 'hostname'
[2020-06-11 05:47:05,629] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0
controller-0:~$ 
[2020-06-11 05:47:05,629] 233  INFO  MainThread test_custom_containers.test_push_docker_image_to_local_registry:: 
====================== Test Step 1: Pull busybox image from external on active controller controller-0
[2020-06-11 05:47:05,630] 504  INFO  MainThread container_helper.pull_docker_image:: Pull docker image busybox
[2020-06-11 05:47:05,630] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 05:47:05,630] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image pull busybox'
[2020-06-11 05:47:12,970] 426  DEBUG MainThread ssh.expect  :: Output: 
Using default tag: latest
latest: Pulling from library/busybox

[1A[1K[K76df9210b28c: Pulling fs layer [1B[1A[1K[K76df9210b28c: Downloading [>                                                  ]  8.789kB/760.5kB[1B[1A[1K[K76df9210b28c: Downloading [==>                                                ]  33.43kB/760.5kB[1B[1A[1K[K76df9210b28c: Downloading [=========>                                         ]  142.8kB/760.5kB[1B[1A[1K[K76df9210b28c: Downloading [=======================>                           ]  358.9kB/760.5kB[1B[1A[1K[K76df9210b28c: Downloading [==================================================>]  760.5kB/760.5kB[1B[1A[1K[K76df9210b28c: Verifying Checksum [1B[1A[1K[K76df9210b28c: Download complete [1B[1A[1K[K76df9210b28c: Extracting [==>                                                ]  32.77kB/760.5kB[1B[1A[1K[K76df9210b28c: Extracting [==================================================>]  760.5kB/760.5kB[1B[1A[1K[K76df9210b28c: Pull complete [1BDigest: sha256:95cf004f559831017cdf4628aaf1bb30133677be8702a8c5f2994629f637a209
Status: Downloaded newer image for busybox:latest
controller-0:~$ 
[2020-06-11 05:47:12,970] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:13,089] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:13,089] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 05:47:13,089] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image ls busybox'
[2020-06-11 05:47:13,243] 426  DEBUG MainThread ssh.expect  :: Output: 
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
busybox             latest              1c35c4412082        8 days ago          1.22MB
controller-0:~$ 
[2020-06-11 05:47:13,244] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:13,340] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:13,341] 513  INFO  MainThread container_helper.pull_docker_image:: docker image busybox successfully pulled. ID: 1c35c4412082
[2020-06-11 05:47:13,341] 238  INFO  MainThread test_custom_containers.test_push_docker_image_to_local_registry:: 
====================== Test Step 2: Remove busybox from local registry if exists
[2020-06-11 05:47:13,341] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 05:47:13,341] 304  DEBUG MainThread ssh.send    :: Send 'sudo rm -rf /var/lib/docker-distribution/docker/registry/v2/repositories/busybox'
[2020-06-11 05:47:13,442] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:47:13,442] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:13,527] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:13,527] 242  INFO  MainThread test_custom_containers.test_push_docker_image_to_local_registry:: 
====================== Test Step 3: Tag image with local registry
[2020-06-11 05:47:13,527] 615  INFO  MainThread container_helper.tag_docker_image:: Tag docker image 1c35c4412082 as registry.local:9001/busybox
[2020-06-11 05:47:13,527] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 05:47:13,527] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image tag 1c35c4412082 registry.local:9001/busybox'
[2020-06-11 05:47:13,682] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:47:13,682] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:13,786] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:13,787] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 05:47:13,787] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image ls registry.local:9001/busybox'
[2020-06-11 05:47:13,941] 426  DEBUG MainThread ssh.expect  :: Output: 
REPOSITORY                    TAG                 IMAGE ID            CREATED             SIZE
registry.local:9001/busybox   latest              1c35c4412082        8 days ago          1.22MB
controller-0:~$ 
[2020-06-11 05:47:13,941] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:14,047] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:14,047] 629  INFO  MainThread container_helper.tag_docker_image:: docker image 1c35c4412082 successfully tagged as registry.local:9001/busybox.
[2020-06-11 05:47:14,047] 249  INFO  MainThread test_custom_containers.test_push_docker_image_to_local_registry:: 
====================== Test Step 4: Login to local docker registry and push test image from active controller controller-0
[2020-06-11 05:47:14,047] 542  INFO  MainThread container_helper.login_to_docker:: Login to docker registry registry.local:9001
[2020-06-11 05:47:14,048] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 05:47:14,048] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker login -u admin -p 99cloud@SH registry.local:9001'
[2020-06-11 05:47:14,636] 426  DEBUG MainThread ssh.expect  :: Output: 
WARNING! Using --password via the CLI is insecure. Use --password-stdin.
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
controller-0:~$ 
[2020-06-11 05:47:14,636] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:14,759] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:14,759] 548  INFO  MainThread container_helper.login_to_docker:: Logged into docker registry successfully: registry.local:9001
[2020-06-11 05:47:14,759] 577  INFO  MainThread container_helper.push_docker_image:: Push docker image: registry.local:9001/busybox
[2020-06-11 05:47:14,759] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 05:47:14,760] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image push registry.local:9001/busybox'
[2020-06-11 05:47:15,768] 426  DEBUG MainThread ssh.expect  :: Output: 
The push refers to repository [registry.local:9001/busybox]

[1A[1K[K1be74353c3d0: Preparing [1B[1A[1K[K1be74353c3d0: Pushing [=>                                                 ]  33.79kB/1.219MB[1B[1A[1K[K1be74353c3d0: Pushing [=============================================>     ]  1.115MB/1.219MB[1B[1A[1K[K1be74353c3d0: Pushing [==================================================>]  1.437MB[1B[1A[1K[K1be74353c3d0: Pushed [1Blatest: digest: sha256:fd4a8673d0344c3a7f427fe4440d4b8dfd4fa59cfabbd9098f9eb0cb4ba905d0 size: 527
controller-0:~$ 
[2020-06-11 05:47:15,768] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:15,855] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:15,855] 583  INFO  MainThread container_helper.push_docker_image:: docker image registry.local:9001/busybox successfully pushed.
[2020-06-11 05:47:15,855] 254  INFO  MainThread test_custom_containers.test_push_docker_image_to_local_registry:: 
====================== Test Step 5: Remove cached test images and pull from local registry on controller-0
[2020-06-11 05:47:15,855] 652  INFO  MainThread container_helper.remove_docker_images:: Remove docker images: ('busybox', 'registry.local:9001/busybox')
[2020-06-11 05:47:15,855] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 05:47:15,855] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image rm busybox registry.local:9001/busybox'
[2020-06-11 05:47:15,995] 426  DEBUG MainThread ssh.expect  :: Output: 
Untagged: busybox:latest
Untagged: busybox@sha256:95cf004f559831017cdf4628aaf1bb30133677be8702a8c5f2994629f637a209
Untagged: registry.local:9001/busybox:latest
Untagged: registry.local:9001/busybox@sha256:fd4a8673d0344c3a7f427fe4440d4b8dfd4fa59cfabbd9098f9eb0cb4ba905d0
Deleted: sha256:1c35c441208254cb7c3844ba95a96485388cef9ccc0646d562c7fc026e04c807
Deleted: sha256:1be74353c3d0fd55fb5638a52953e6f1bc441e5b1710921db9ec2aa202725569
controller-0:~$ 
[2020-06-11 05:47:15,995] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:16,092] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:16,092] 504  INFO  MainThread container_helper.pull_docker_image:: Pull docker image registry.local:9001/busybox
[2020-06-11 05:47:16,093] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 05:47:16,093] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image pull registry.local:9001/busybox'
[2020-06-11 05:47:16,641] 426  DEBUG MainThread ssh.expect  :: Output: 
Using default tag: latest
latest: Pulling from busybox

[1A[1K[K76df9210b28c: Pulling fs layer [1B[1A[1K[K76df9210b28c: Downloading [>                                                  ]  8.361kB/760.5kB[1B[1A[1K[K76df9210b28c: Verifying Checksum [1B[1A[1K[K76df9210b28c: Download complete [1B[1A[1K[K76df9210b28c: Extracting [==>                                                ]  32.77kB/760.5kB[1B[1A[1K[K76df9210b28c: Extracting [==================================================>]  760.5kB/760.5kB[1B[1A[1K[K76df9210b28c: Pull complete [1BDigest: sha256:fd4a8673d0344c3a7f427fe4440d4b8dfd4fa59cfabbd9098f9eb0cb4ba905d0
Status: Downloaded newer image for registry.local:9001/busybox:latest
controller-0:~$ 
[2020-06-11 05:47:16,642] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:16,779] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:16,779] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 05:47:16,780] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image ls registry.local:9001/busybox'
[2020-06-11 05:47:16,959] 426  DEBUG MainThread ssh.expect  :: Output: 
REPOSITORY                    TAG                 IMAGE ID            CREATED             SIZE
registry.local:9001/busybox   latest              1c35c4412082        8 days ago          1.22MB
controller-0:~$ 
[2020-06-11 05:47:16,959] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:17,045] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:17,046] 513  INFO  MainThread container_helper.pull_docker_image:: docker image registry.local:9001/busybox successfully pulled. ID: 1c35c4412082
[2020-06-11 05:47:17,046] 652  INFO  MainThread container_helper.remove_docker_images:: Remove docker images: ('registry.local:9001/busybox',)
[2020-06-11 05:47:17,046] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 05:47:17,046] 304  DEBUG MainThread ssh.send    :: Send 'sudo docker image rm registry.local:9001/busybox'
[2020-06-11 05:47:17,200] 426  DEBUG MainThread ssh.expect  :: Output: 
Untagged: registry.local:9001/busybox:latest
Untagged: registry.local:9001/busybox@sha256:fd4a8673d0344c3a7f427fe4440d4b8dfd4fa59cfabbd9098f9eb0cb4ba905d0
Deleted: sha256:1c35c441208254cb7c3844ba95a96485388cef9ccc0646d562c7fc026e04c807
Deleted: sha256:1be74353c3d0fd55fb5638a52953e6f1bc441e5b1710921db9ec2aa202725569
controller-0:~$ 
[2020-06-11 05:47:17,201] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:17,302] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:17,302] 275  INFO  MainThread test_custom_containers.test_push_docker_image_to_local_registry:: 
====================== Test Step 6: Cleanup busybox from local docker registry after test
[2020-06-11 05:47:17,303] 758  DEBUG MainThread ssh.exec_sudo_cmd:: Executing sudo command...
[2020-06-11 05:47:17,303] 304  DEBUG MainThread ssh.send    :: Send 'sudo rm -rf /var/lib/docker-distribution/docker/registry/v2/repositories/busybox'
[2020-06-11 05:47:17,414] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:47:17,415] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:17,509] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:20,514] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:47:20,514] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:47:20,629] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:47:20,629] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 05:47:23,633] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:47:23,633] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:47:23,684] 426  DEBUG MainThread ssh.expect  :: Output: 
]0;root@forstarlingxtest:~/yuchengde/test/automated-pytest-suiteroot@forstarlingxtest$ 
[2020-06-11 05:47:23,685] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 05:47:23,685] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_custom_containers.py::test_push_docker_image_to_local_registry[active]
[2020-06-11 05:47:23,685] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 1: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 05:47:23,685] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:23,686] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:23,686] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:47:24,459] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 24a96174-80b7-484e-9127-7cd32ebed66b | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                   | host=controller-0.ntp                                                            | major    | 2020-06-11T05:38:59.708120 |
| d5a2b085-7f1a-423d-b36a-cd7a50bff267 | 100.114  | NTP address 5.79.108.34 is not a valid or a reachable NTP server.                                                        | host=controller-0.ntp=5.79.108.34                                                | minor    | 2020-06-11T05:38:59.626433 |
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:47:24,459] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:24,545] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:24,546] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:24,546] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:24,546] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:47:25,434] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:47:25,434] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:25,530] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:25,531] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 05:47:25,531] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:25,531] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:25,531] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:47:25,686] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d3h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d3h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:47:25,686] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:25,784] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:25,785] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 05:47:25,785] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:25,785] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:25,785] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 05:47:25,958] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 05:47:25,958] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:26,043] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:26,044] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 05:47:26,045] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_custom_containers.py::test_push_docker_image_to_local_registry[active] - Test Passed

[2020-06-11 05:47:26,046] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_custom_containers.py::test_push_docker_image_to_local_registry[standby]
[2020-06-11 05:47:26,047] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 05:47:26,047] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:26,047] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:26,048] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:47:26,818] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 24a96174-80b7-484e-9127-7cd32ebed66b | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                   | host=controller-0.ntp                                                            | major    | 2020-06-11T05:38:59.708120 |
| d5a2b085-7f1a-423d-b36a-cd7a50bff267 | 100.114  | NTP address 5.79.108.34 is not a valid or a reachable NTP server.                                                        | host=controller-0.ntp=5.79.108.34                                                | minor    | 2020-06-11T05:38:59.626433 |
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:47:26,818] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:26,929] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:26,930] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:26,930] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:26,930] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:47:27,110] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d3h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d3h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:47:27,110] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:27,216] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:27,216] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:27,216] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:27,217] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:47:28,112] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:47:28,112] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:28,199] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:28,201] 213  DEBUG MainThread conftest.testcase_log:: 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test steps started for: testcases/functional/z_containers/test_custom_containers.py::test_push_docker_image_to_local_registry[standby]
[2020-06-11 05:47:28,201] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:28,201] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:28,201] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne servicegroup-list'
[2020-06-11 05:47:29,395] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+-----------------------------+--------------+--------+
| uuid                                 | service_group_name          | hostname     | state  |
+--------------------------------------+-----------------------------+--------------+--------+
| 6d74599f-5aa5-4667-92b8-bcf065b97566 | cloud-services              | controller-0 | active |
| e6f0d8e5-8313-4c07-8597-06f87ff72758 | controller-services         | controller-0 | active |
| 0bd309c1-616b-4b3c-b0fa-588b5d59a6ed | directory-services          | controller-0 | active |
| db0096b5-0946-4518-ba8c-07676312736c | oam-services                | controller-0 | active |
| f2a7b29a-9e00-4c1a-ac9d-646d28b22e88 | patching-services           | controller-0 | active |
| 6ba173e0-cb68-44f8-817f-bc330ca41322 | storage-monitoring-services | controller-0 | active |
| ff7a236c-d5cb-4b2c-af44-4990e774fc76 | storage-services            | controller-0 | active |
| 39603c17-ae56-4106-bb4d-4e89c8bd14a6 | vim-services                | controller-0 | active |
| 1a32d85a-3abf-44c8-a5e5-9a6da9d88068 | web-services                | controller-0 | active |
+--------------------------------------+-----------------------------+--------------+--------+
controller-0:~$ 
[2020-06-11 05:47:29,395] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:29,494] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:29,494] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): ['controller-0']
[2020-06-11 05:47:29,495] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): []
[2020-06-11 05:47:29,495] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:29,495] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:29,495] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 05:47:30,398] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
controller-0:~$ 
[2020-06-11 05:47:30,398] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:30,489] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:30,490] 244  DEBUG MainThread system_helper.get_hosts:: Filtered hosts: ['controller-0']
[2020-06-11 05:47:33,494] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:47:33,495] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:47:33,610] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:47:33,610] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 05:47:36,614] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:47:36,614] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:47:36,665] 426  DEBUG MainThread ssh.expect  :: Output: 
]0;root@forstarlingxtest:~/yuchengde/test/automated-pytest-suiteroot@forstarlingxtest$ 
[2020-06-11 05:47:36,666] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 05:47:36,666] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_custom_containers.py::test_push_docker_image_to_local_registry[standby]
[2020-06-11 05:47:36,666] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 1: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 05:47:36,666] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:36,667] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:36,667] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:47:37,461] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 24a96174-80b7-484e-9127-7cd32ebed66b | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                   | host=controller-0.ntp                                                            | major    | 2020-06-11T05:38:59.708120 |
| d5a2b085-7f1a-423d-b36a-cd7a50bff267 | 100.114  | NTP address 5.79.108.34 is not a valid or a reachable NTP server.                                                        | host=controller-0.ntp=5.79.108.34                                                | minor    | 2020-06-11T05:38:59.626433 |
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:47:37,461] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:37,544] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:37,544] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:37,545] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:37,545] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:47:38,440] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:47:38,441] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:38,527] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:38,527] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 05:47:38,527] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:38,528] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:38,528] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:47:38,699] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d3h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d3h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:47:38,699] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:38,784] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:38,785] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 05:47:38,785] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:38,785] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:38,785] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 05:47:38,969] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 05:47:38,969] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:39,088] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:39,088] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 05:47:39,089] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_custom_containers.py::test_push_docker_image_to_local_registry[standby] - Test Skipped
Reason: Standby controller does not exist or not in good state

[2020-06-11 05:47:39,091] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_custom_containers.py::test_upload_charts_via_helm_upload
[2020-06-11 05:47:42,095] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:47:42,096] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:47:42,181] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:47:42,181] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 05:47:45,185] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:47:45,185] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:47:45,236] 426  DEBUG MainThread ssh.expect  :: Output: 
]0;root@forstarlingxtest:~/yuchengde/test/automated-pytest-suiteroot@forstarlingxtest$ 
[2020-06-11 05:47:45,237] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 05:47:45,237] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_custom_containers.py::test_upload_charts_via_helm_upload
[2020-06-11 05:47:45,237] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_custom_containers.py::test_upload_charts_via_helm_upload - Test Skipped
Reason: Shared Test File Server is not ready

[2020-06-11 05:47:45,239] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_custom_containers.py::test_host_operations_with_custom_kubectl_app
[2020-06-11 05:47:45,240] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 05:47:45,240] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:45,240] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:45,240] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:47:46,049] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| 24a96174-80b7-484e-9127-7cd32ebed66b | 100.114  | NTP configuration does not contain any valid or reachable NTP servers.                                                   | host=controller-0.ntp                                                            | major    | 2020-06-11T05:38:59.708120 |
| d5a2b085-7f1a-423d-b36a-cd7a50bff267 | 100.114  | NTP address 5.79.108.34 is not a valid or a reachable NTP server.                                                        | host=controller-0.ntp=5.79.108.34                                                | minor    | 2020-06-11T05:38:59.626433 |
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:47:46,049] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:46,135] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:46,135] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:46,136] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:46,136] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:47:46,314] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d3h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d3h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:47:46,315] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:46,402] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:46,402] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:46,402] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:46,402] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:47:47,319] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:47:47,319] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:47,404] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:47,405] 327  INFO  MainThread test_custom_containers.deploy_delete_kubectl_app:: 
====================== Setup Step 2: Create resource-consumer test app by kubectl run
[2020-06-11 05:47:47,406] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:47,406] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:47,406] 304  DEBUG MainThread ssh.send    :: Send 'kubectl run resource-consumer --image=gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4 --expose --service-overrides='{ "spec": { "type": "LoadBalancer" } }' --port 8080 --requests='cpu=1000m,memory=1024Mi''
[2020-06-11 05:47:47,584] 426  DEBUG MainThread ssh.expect  :: Output: 
kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
service/resource-consumer created
deployment.apps/resource-consumer created
controller-0:~$ 
[2020-06-11 05:47:47,585] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:47,682] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:47,682] 331  INFO  MainThread test_custom_containers.deploy_delete_kubectl_app:: 
====================== Setup Step 3: Check resource-consumer test app is created 
[2020-06-11 05:47:47,683] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:47,683] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:47,683] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod -n=default -o=wide'
[2020-06-11 05:47:47,848] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ContainerCreating   0          0s     <none>         worker-0   <none>           <none>
server-pod-dep-7fc68f8c47-85drg     0/1     ImagePullBackOff    0          2d3h   172.16.43.9    worker-0   <none>           <none>
server-pod-dep-7fc68f8c47-957dw     0/1     ImagePullBackOff    0          2d3h   172.16.43.10   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:47:47,848] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:47,950] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:47,950] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:47,951] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:47,951] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:47:48,159] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ContainerCreating   0          1s    <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:47:48,159] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:48,261] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:51,264] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:51,265] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:51,265] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:47:51,435] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ContainerCreating   0          4s    <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:47:51,436] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:51,526] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:54,530] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:54,530] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:54,530] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:47:54,694] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ContainerCreating   0          7s    <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:47:54,694] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:54,799] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:47:57,803] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:47:57,804] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:47:57,804] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:47:57,978] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ContainerCreating   0          10s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:47:57,978] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:47:58,062] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:01,066] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:01,066] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:01,066] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:01,228] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ContainerCreating   0          14s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:01,228] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:01,356] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:04,359] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:04,359] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:04,360] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:04,523] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ContainerCreating   0          17s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:04,523] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:04,649] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:07,653] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:07,653] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:07,653] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:07,818] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ContainerCreating   0          20s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:07,818] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:07,906] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:10,909] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:10,909] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:10,910] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:11,081] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ContainerCreating   0          24s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:11,082] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:11,181] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:14,184] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:14,185] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:14,185] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:14,352] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ContainerCreating   0          27s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:14,352] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:14,441] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:17,445] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:17,445] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:17,445] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:17,596] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ContainerCreating   0          30s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:17,596] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:17,672] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:20,676] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:20,676] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:20,676] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:20,855] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS              RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ContainerCreating   0          33s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:20,855] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:20,941] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:23,944] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:23,945] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:23,945] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:24,107] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ErrImagePull   0          37s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:24,107] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:24,196] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:27,200] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:27,200] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:27,200] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:27,368] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ErrImagePull   0          40s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:27,368] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:27,469] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:30,473] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:30,473] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:30,473] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:30,632] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          43s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:30,633] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:30,749] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:33,753] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:33,753] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:33,753] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:33,997] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          46s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:33,997] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:34,084] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:37,088] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:37,088] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:37,088] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:37,276] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          50s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:37,276] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:37,365] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:40,368] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:40,369] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:40,369] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:40,540] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          53s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:40,540] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:40,659] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:43,663] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:43,663] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:43,664] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:43,850] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          56s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:43,850] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:43,937] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:46,941] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:46,941] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:46,941] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:47,105] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          60s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:47,105] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:47,205] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:50,208] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:50,209] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:50,209] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:50,371] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP       NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          63s   <none>   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:50,372] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:50,471] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:53,475] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:53,475] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:53,475] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:53,630] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ErrImagePull   0          66s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:53,630] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:53,717] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:48:56,721] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:48:56,721] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:48:56,721] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:48:56,895] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ErrImagePull   0          69s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:48:56,895] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:48:57,020] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:00,023] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:00,024] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:00,024] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:00,187] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ErrImagePull   0          73s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:49:00,187] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:00,283] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:03,287] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:03,287] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:03,287] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:03,457] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ErrImagePull   0          76s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:49:03,457] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:03,567] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:06,571] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:06,571] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:06,571] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:06,740] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          79s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:49:06,740] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:06,826] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:09,830] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:09,830] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:09,830] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:10,019] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          82s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:49:10,020] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:10,106] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:13,109] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:13,110] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:13,110] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:13,271] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          86s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:49:13,271] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:13,372] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:16,376] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:16,376] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:16,376] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:16,525] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          89s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:49:16,525] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:16,625] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:19,629] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:19,630] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:19,630] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:19,781] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          92s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:49:19,781] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:19,868] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:22,871] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:22,871] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:22,872] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:23,037] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          96s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:49:23,037] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:23,128] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:26,131] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:26,131] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:26,132] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:26,295] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE   IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          99s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:49:26,295] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:26,410] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:29,414] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:29,414] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:29,414] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:29,576] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          102s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:49:29,576] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:29,656] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:32,660] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:32,660] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:32,660] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:32,823] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          105s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:49:32,823] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:32,921] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:35,925] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:35,925] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:35,925] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:36,090] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          109s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:49:36,090] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:36,175] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:39,178] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:39,179] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:39,179] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:39,342] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          112s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:49:39,342] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:39,454] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:42,457] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:42,458] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:42,458] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:42,709] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ImagePullBackOff   0          115s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:49:42,709] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:42,795] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:45,799] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:45,799] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:45,799] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:45,990] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ErrImagePull   0          118s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:49:45,990] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:46,076] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:49,121] 54   DEBUG MainThread conftest.update_results:: ***Failure at test setup: /root/yuchengde/test/automated-pytest-suite/keywords/kube_helper.py:414: utils.exceptions.KubeError: Kubernetes error.
***Details: request = <SubRequest 'deploy_delete_kubectl_app' for <Function 'test_host_operations_with_custom_kubectl_app'>>

    @fixture()
    def deploy_delete_kubectl_app(request):
        app_name = 'resource-consumer'
        app_params = \
            '--image=gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4' \
            + ' --expose' \
            + ' --service-overrides=' \
            + "'{ " + '"spec": { "type": "LoadBalancer" } }' \
            + "' --port 8080 --requests='cpu=1000m,memory=1024Mi'"
    
        LOG.fixture_step("Create {} test app by kubectl run".format(app_name))
        sub_cmd = "run {}".format(app_name)
        kube_helper.exec_kube_cmd(sub_cmd=sub_cmd, args=app_params, fail_ok=False)
    
        LOG.fixture_step("Check {} test app is created ".format(app_name))
        pod_name = kube_helper.get_pods(field='NAME', namespace='default',
                                        name=app_name, strict=False)[0]
    
        def delete_app():
            LOG.fixture_step("Delete {} pod if exists after test "
                             "run".format(app_name))
            kube_helper.delete_resources(resource_names=app_name,
                                         resource_types=('deployment', 'service'),
                                         namespace='default', post_check=False)
            kube_helper.wait_for_resources_gone(resource_names=pod_name,
                                                namespace='default')
        request.addfinalizer(delete_app)
    
        kube_helper.wait_for_pods_status(pod_names=pod_name, namespace='default',
>                                        fail_ok=False)

testcases/functional/z_containers/test_custom_containers.py:346: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pod_names = ['resource-consumer-5577dbdbf-4rqbc'], partial_names = None, labels = None, namespace = 'default', status = 'Running', timeout = 120, check_interval = 3
con_ssh = None, fail_ok = False, strict = False, kwargs = {}, pods_to_check = ['resource-consumer-5577dbdbf-4rqbc']
actual_status = {'resource-consumer-5577dbdbf-4rqbc': 'ErrImagePull'}, end_time = 1591854587.9508226, pod_full_names = ['resource-consumer-5577dbdbf-4rqbc']
pods_values = [('resource-consumer-5577dbdbf-4rqbc', 'ErrImagePull')], continue_check = True, pod_info = ('resource-consumer-5577dbdbf-4rqbc', 'ErrImagePull')

    def wait_for_pods_status(pod_names=None, partial_names=None, labels=None,
                             namespace=None, status=PodStatus.RUNNING,
                             timeout=120, check_interval=3, con_ssh=None,
                             fail_ok=False, strict=False, **kwargs):
        """
        Wait for pod(s) to reach given status via kubectl get pod
        Args:
            pod_names (str|list|tuple): full name of the pods
            partial_names (str|list|tuple): Used only if pod_names are not provided
            labels (str|list|tuple|dict|None): Used only if pod_names are not
                provided
            namespace (None|str):
            status (str|None|list): None means any state as long as pod exists.
            timeout:
            check_interval:
            con_ssh:
            fail_ok:
            strict (bool):
    
        Returns (tuple):
            (True, <actual_pods_info>)  # actual_pods_info is a dict with
            pod_name as key, and pod_info(dict) as value
            (False, <actual_pods_info>)
    
        """
    
        pods_to_check = []
        if pod_names:
            if isinstance(pod_names, str):
                pod_names = [pod_names]
            else:
                pod_names = list(pod_names)
            labels = partial_names = None
            pods_to_check = list(pod_names)
        elif partial_names:
            if isinstance(partial_names, str):
                partial_names = [partial_names]
            else:
                partial_names = list(partial_names)
            kwargs['NAME'] = partial_names
            pods_to_check = list(partial_names)
    
        actual_status = {}
        end_time = time.time() + timeout
    
        while time.time() < end_time:
            pod_full_names = pods_to_check if pod_names else None
            pods_values = get_pods(pod_names=pod_full_names,
                                   field=('NAME', 'status'), namespace=namespace,
                                   labels=labels,
                                   strict=strict, fail_ok=True, con_ssh=con_ssh,
                                   **kwargs)
            if not pods_values:
                # No pods returned, continue to check.
                time.sleep(check_interval)
                continue
    
            continue_check = False  # This is used when only labels are provided
            for pod_info in pods_values:
                pod_name, pod_status = pod_info
                actual_status[pod_name] = pod_status
                if status and pod_status not in status:
                    # Status not as expected, continue to wait
                    continue_check = True
                    if partial_names:
                        # In this case, there might be multiple pods that matches
                        # 1 partial name, so the partial name that
                        # matches current pod could have been removed if there
                        # was one other pod that also matched the name
                        # had reached the desired state. In this case, we will
                        # add the partial name back to check list
                        for partial_name in partial_names:
                            if partial_name in pod_name and partial_name not in \
                                    pods_to_check:
                                pods_to_check.append(partial_name)
                                break
                else:
                    # Criteria met for current pod, remove it from check_list
                    if pod_names:
                        pods_to_check.remove(pod_name)
                    elif partial_names:
                        for partial_name in partial_names:
                            if partial_name in pod_name and partial_name in \
                                    pods_to_check:
                                pods_to_check.remove(partial_name)
                                break
    
            if not pods_to_check and not continue_check:
                return True, actual_status
    
            time.sleep(check_interval)
    
        name_str = 'Names: {}'.format(pods_to_check) if pods_to_check else ''
        label_str = 'Labels: {}'.format(labels) if labels else ''
        criteria = '{} {}'.format(name_str, label_str).strip()
        msg = "Pods did not reach expected status within {}s. Criteria not met: " \
              "{}. Actual info: {}".format(timeout, criteria, actual_status)
        if fail_ok:
            LOG.info(msg)
            return False, actual_status
    
>       raise exceptions.KubeError(msg)
E       utils.exceptions.KubeError: Kubernetes error.
E       Details: Pods did not reach expected status within 120s. Criteria not met: Names: ['resource-consumer-5577dbdbf-4rqbc']. Actual info: {'resource-consumer-5577dbdbf-4rqbc': 'ErrImagePull'}

keywords/kube_helper.py:414: KubeError
[2020-06-11 05:49:52,138] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:49:52,138] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:49:52,222] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:49:52,222] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 05:49:55,226] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:49:55,227] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:49:55,278] 426  DEBUG MainThread ssh.expect  :: Output: 
]0;root@forstarlingxtest:~/yuchengde/test/automated-pytest-suiteroot@forstarlingxtest$ 
[2020-06-11 05:49:55,278] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 05:49:55,279] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_custom_containers.py::test_host_operations_with_custom_kubectl_app
[2020-06-11 05:49:55,279] 337  INFO  MainThread test_custom_containers.delete_app:: 
====================== Teardown Step 1: Delete resource-consumer pod if exists after test run
[2020-06-11 05:49:55,279] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:55,279] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:55,279] 304  DEBUG MainThread ssh.send    :: Send 'kubectl delete deployment,service resource-consumer'
[2020-06-11 05:49:55,447] 426  DEBUG MainThread ssh.expect  :: Output: 
deployment.apps "resource-consumer" deleted
service "resource-consumer" deleted
controller-0:~$ 
[2020-06-11 05:49:55,447] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:55,546] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:55,546] 558  INFO  MainThread kube_helper.delete_resources:: ['resource-consumer'] are successfully removed.
[2020-06-11 05:49:55,546] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:55,546] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:55,546] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:55,704] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                READY   STATUS         RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
resource-consumer-5577dbdbf-4rqbc   0/1     ErrImagePull   0          2m8s   172.16.43.58   worker-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:49:55,704] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:55,782] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:58,786] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:58,786] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:58,786] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod resource-consumer-5577dbdbf-4rqbc -n=default -o=wide'
[2020-06-11 05:49:58,957] 426  DEBUG MainThread ssh.expect  :: Output: 
Error from server (NotFound): pods "resource-consumer-5577dbdbf-4rqbc" not found
controller-0:~$ 
[2020-06-11 05:49:58,958] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:59,059] 426  DEBUG MainThread ssh.expect  :: Output: 
1
controller-0:~$ 
[2020-06-11 05:49:59,060] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 2: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 05:49:59,060] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:59,060] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:59,061] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:49:59,871] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:49:59,871] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:49:59,970] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:49:59,970] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:49:59,970] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:49:59,971] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:50:00,847] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:50:00,847] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:00,979] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:00,980] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 05:50:00,980] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:00,980] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:00,980] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:50:01,148] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d3h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d3h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:50:01,148] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:01,241] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:01,242] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 05:50:01,242] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:01,242] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:01,242] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 05:50:01,445] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE    IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 05:50:01,446] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:01,537] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:01,538] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 05:50:01,540] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_custom_containers.py::test_host_operations_with_custom_kubectl_app - Test Failed at test setup

[2020-06-11 05:50:01,541] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_kube_system_services.py::test_kube_system_services[active]
[2020-06-11 05:50:01,542] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 05:50:01,543] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:01,543] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:01,543] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:50:02,380] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:50:02,380] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:02,467] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:02,468] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:02,468] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:02,468] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:50:02,647] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d3h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d3h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:50:02,647] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:02,747] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:02,748] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:02,748] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:02,748] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:50:03,656] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:50:03,656] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:03,745] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:03,747] 213  DEBUG MainThread conftest.testcase_log:: 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test steps started for: testcases/functional/z_containers/test_kube_system_services.py::test_kube_system_services[active]
[2020-06-11 05:50:03,747] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:03,747] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:03,748] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne servicegroup-list'
[2020-06-11 05:50:04,973] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+-----------------------------+--------------+--------+
| uuid                                 | service_group_name          | hostname     | state  |
+--------------------------------------+-----------------------------+--------------+--------+
| 6d74599f-5aa5-4667-92b8-bcf065b97566 | cloud-services              | controller-0 | active |
| e6f0d8e5-8313-4c07-8597-06f87ff72758 | controller-services         | controller-0 | active |
| 0bd309c1-616b-4b3c-b0fa-588b5d59a6ed | directory-services          | controller-0 | active |
| db0096b5-0946-4518-ba8c-07676312736c | oam-services                | controller-0 | active |
| f2a7b29a-9e00-4c1a-ac9d-646d28b22e88 | patching-services           | controller-0 | active |
| 6ba173e0-cb68-44f8-817f-bc330ca41322 | storage-monitoring-services | controller-0 | active |
| ff7a236c-d5cb-4b2c-af44-4990e774fc76 | storage-services            | controller-0 | active |
| 39603c17-ae56-4106-bb4d-4e89c8bd14a6 | vim-services                | controller-0 | active |
| 1a32d85a-3abf-44c8-a5e5-9a6da9d88068 | web-services                | controller-0 | active |
+--------------------------------------+-----------------------------+--------------+--------+
controller-0:~$ 
[2020-06-11 05:50:04,973] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:05,059] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:05,060] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): ['controller-0']
[2020-06-11 05:50:05,060] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): []
[2020-06-11 05:50:05,060] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:05,060] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:05,060] 304  DEBUG MainThread ssh.send    :: Send 'hostname'
[2020-06-11 05:50:05,146] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0
controller-0:~$ 
[2020-06-11 05:50:05,146] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:05,146] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod -n=kube-system -o=wide'
[2020-06-11 05:50:05,381] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                                       READY   STATUS    RESTARTS   AGE    IP                NODE           NOMINATED NODE   READINESS GATES
calico-kube-controllers-855577b7b5-qkqtm   1/1     Running   0          19m    172.16.192.126    controller-0   <none>           <none>
calico-node-6pkbm                          1/1     Running   3          29d    192.168.104.149   worker-0       <none>           <none>
calico-node-fhfpz                          1/1     Running   25         29d    192.168.104.2     controller-0   <none>           <none>
coredns-6bc668cd76-sxvt7                   1/1     Running   0          19m    172.16.192.127    controller-0   <none>           <none>
coredns-6bc668cd76-vlc6w                   0/1     Pending   0          47h    <none>            <none>         <none>           <none>
kube-apiserver-controller-0                1/1     Running   24         29d    192.168.104.2     controller-0   <none>           <none>
kube-controller-manager-controller-0       1/1     Running   22         29d    192.168.104.2     controller-0   <none>           <none>
kube-multus-ds-amd64-966nx                 1/1     Running   0          10m    192.168.104.2     controller-0   <none>           <none>
kube-multus-ds-amd64-mp8gs                 1/1     Running   0          2d3h   192.168.104.149   worker-0       <none>           <none>
kube-proxy-7bwt2                           1/1     Running   12         29d    192.168.104.2     controller-0   <none>           <none>
kube-proxy-8c4nx                           1/1     Running   1          29d    192.168.104.149   worker-0       <none>           <none>
kube-scheduler-controller-0                1/1     Running   22         29d    192.168.104.2     controller-0   <none>           <none>
kube-sriov-cni-ds-amd64-7zmvm              1/1     Running   0          2d3h   192.168.104.149   worker-0       <none>           <none>
kube-sriov-cni-ds-amd64-jxwvc              1/1     Running   0          10m    192.168.104.2     controller-0   <none>           <none>
rbd-provisioner-7484d49cf6-cq9xp           1/1     Running   0          19m    172.16.192.125    controller-0   <none>           <none>
tiller-deploy-d6b59fcb-spld8               1/1     Running   0          19m    192.168.104.2     controller-0   <none>           <none>
controller-0:~$ 
[2020-06-11 05:50:05,382] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:05,469] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:05,470] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:05,470] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get service -n=kube-system -o=wide'
[2020-06-11 05:50:05,635] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE   SELECTOR
kube-dns        ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP,9153/TCP   29d   k8s-app=kube-dns
tiller-deploy   ClusterIP   10.104.208.44   <none>        44134/TCP                29d   app=helm,name=tiller
controller-0:~$ 
[2020-06-11 05:50:05,636] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:05,718] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:05,719] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:05,719] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get deployment.apps -n=kube-system -o=wide'
[2020-06-11 05:50:05,875] 426  DEBUG MainThread ssh.expect  :: Output: 
NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS                IMAGES                                                                        SELECTOR
calico-kube-controllers   1/1     1            1           29d   calico-kube-controllers   registry.local:9001/quay.io/calico/kube-controllers:v3.6.2                    k8s-app=calico-kube-controllers
coredns                   1/2     2            1           29d   coredns                   registry.local:9001/k8s.gcr.io/coredns:1.6.2                                  k8s-app=kube-dns
rbd-provisioner           1/1     1            1           29d   rbd-provisioner           registry.local:9001/quay.io/external_storage/rbd-provisioner:v2.1.1-k8s1.11   app=rbd-provisioner
tiller-deploy             1/1     1            1           29d   tiller                    registry.local:9001/gcr.io/kubernetes-helm/tiller:v2.13.1                     app=helm,name=tiller
controller-0:~$ 
[2020-06-11 05:50:05,876] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:05,989] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:05,990] 61   INFO  MainThread test_kube_system_services.test_kube_system_services:: 
====================== Test Step 1: Check kube-system pods status on active
[2020-06-11 05:50:05,990] 83   INFO  MainThread test_kube_system_services.test_kube_system_services:: 
====================== Test Step 2: Check kube-system services on active: ('kube-dns', 'tiller-deploy')
[2020-06-11 05:50:05,990] 90   INFO  MainThread test_kube_system_services.test_kube_system_services:: 
====================== Test Step 3: Check kube-system deployments on active: ('calico-kube-controllers', 'coredns', 'tiller-deploy')
[2020-06-11 05:50:08,995] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:50:08,995] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:50:09,081] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:50:09,081] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 05:50:12,085] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:50:12,085] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:50:12,136] 426  DEBUG MainThread ssh.expect  :: Output: 
]0;root@forstarlingxtest:~/yuchengde/test/automated-pytest-suiteroot@forstarlingxtest$ 
[2020-06-11 05:50:12,136] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 05:50:12,137] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_kube_system_services.py::test_kube_system_services[active]
[2020-06-11 05:50:12,137] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 1: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 05:50:12,137] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:12,137] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:12,138] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:50:12,895] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:50:12,895] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:12,979] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:12,980] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:12,980] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:12,980] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:50:13,840] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:50:13,840] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:13,924] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:13,924] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 05:50:13,924] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:13,924] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:13,924] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:50:14,132] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d3h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d3h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:50:14,132] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:14,232] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:14,233] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 05:50:14,233] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:14,233] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:14,233] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 05:50:14,413] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE    IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 05:50:14,413] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:14,498] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:14,499] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 05:50:14,499] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_kube_system_services.py::test_kube_system_services[active] - Test Passed

[2020-06-11 05:50:14,501] 215  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Setup started for: testcases/functional/z_containers/test_kube_system_services.py::test_kube_system_services[standby]
[2020-06-11 05:50:14,502] 125  INFO  MainThread verify_fixtures.__get_alarms:: 
====================== Setup Step 1: (function) Gathering system health info before test function begins.
[2020-06-11 05:50:14,502] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:14,502] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:14,502] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:50:15,281] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:50:15,281] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:15,366] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:15,367] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:15,367] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:15,367] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:50:15,535] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d3h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d3h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:50:15,535] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:15,632] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:15,633] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:15,633] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:15,633] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:50:16,509] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:50:16,509] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:16,596] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:16,598] 213  DEBUG MainThread conftest.testcase_log:: 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Test steps started for: testcases/functional/z_containers/test_kube_system_services.py::test_kube_system_services[standby]
[2020-06-11 05:50:16,598] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:16,598] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:16,598] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne servicegroup-list'
[2020-06-11 05:50:17,746] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+-----------------------------+--------------+--------+
| uuid                                 | service_group_name          | hostname     | state  |
+--------------------------------------+-----------------------------+--------------+--------+
| 6d74599f-5aa5-4667-92b8-bcf065b97566 | cloud-services              | controller-0 | active |
| e6f0d8e5-8313-4c07-8597-06f87ff72758 | controller-services         | controller-0 | active |
| 0bd309c1-616b-4b3c-b0fa-588b5d59a6ed | directory-services          | controller-0 | active |
| db0096b5-0946-4518-ba8c-07676312736c | oam-services                | controller-0 | active |
| f2a7b29a-9e00-4c1a-ac9d-646d28b22e88 | patching-services           | controller-0 | active |
| 6ba173e0-cb68-44f8-817f-bc330ca41322 | storage-monitoring-services | controller-0 | active |
| ff7a236c-d5cb-4b2c-af44-4990e774fc76 | storage-services            | controller-0 | active |
| 39603c17-ae56-4106-bb4d-4e89c8bd14a6 | vim-services                | controller-0 | active |
| 1a32d85a-3abf-44c8-a5e5-9a6da9d88068 | web-services                | controller-0 | active |
+--------------------------------------+-----------------------------+--------------+--------+
controller-0:~$ 
[2020-06-11 05:50:17,746] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:17,825] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:17,826] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): ['controller-0']
[2020-06-11 05:50:17,826] 575  DEBUG MainThread table_parser.get_values:: Returning matching hostname value(s): []
[2020-06-11 05:50:17,826] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:17,826] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:17,826] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne host-list'
[2020-06-11 05:50:18,731] 426  DEBUG MainThread ssh.expect  :: Output: 
+----+--------------+-------------+----------------+-------------+--------------+
| id | hostname     | personality | administrative | operational | availability |
+----+--------------+-------------+----------------+-------------+--------------+
| 1  | controller-0 | controller  | unlocked       | enabled     | available    |
| 2  | worker-0     | worker      | unlocked       | enabled     | available    |
+----+--------------+-------------+----------------+-------------+--------------+
controller-0:~$ 
[2020-06-11 05:50:18,731] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:18,850] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:18,850] 244  DEBUG MainThread system_helper.get_hosts:: Filtered hosts: ['controller-0']
[2020-06-11 05:50:21,855] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:50:21,856] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:50:21,935] 426  DEBUG MainThread ssh.expect  :: Output: 
controller-0:~$ 
[2020-06-11 05:50:21,935] 139  DEBUG MainThread ssh.connect :: Already connected to 172.16.130.249. Do nothing.
[2020-06-11 05:50:24,939] 341  DEBUG MainThread ssh.flush   :: Buffer is flushed by reading out the rest of the output
[2020-06-11 05:50:24,939] 304  DEBUG MainThread ssh.send    :: Send ''
[2020-06-11 05:50:24,990] 426  DEBUG MainThread ssh.expect  :: Output: 
]0;root@forstarlingxtest:~/yuchengde/test/automated-pytest-suiteroot@forstarlingxtest$ 
[2020-06-11 05:50:24,990] 73   DEBUG MainThread local.connect :: Already connected to forstarlingxtest. Do nothing.
[2020-06-11 05:50:24,991] 217  DEBUG MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Teardown started for: testcases/functional/z_containers/test_kube_system_services.py::test_kube_system_services[standby]
[2020-06-11 05:50:24,991] 53   INFO  MainThread verify_fixtures.verify_ :: 
====================== Teardown Step 1: (function) Verify system alarms, applications and pods status after test function ended...
[2020-06-11 05:50:24,991] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:24,991] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:24,992] 304  DEBUG MainThread ssh.send    :: Send 'fm --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne alarm-list --nowrap --uuid'
[2020-06-11 05:50:25,738] 426  DEBUG MainThread ssh.expect  :: Output: 
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| UUID                                 | Alarm ID | Reason Text                                                                                                              | Entity ID                                                                        | Severity | Time Stamp                 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
| b4d3b5f7-c5b6-4aa7-9ad2-5c14406f9afa | 100.114  | NTP address 139.99.236.185 is not a valid or a reachable NTP server.                                                     | host=controller-0.ntp=139.99.236.185                                             | minor    | 2020-06-11T05:38:59.575179 |
| 50736227-65a0-4035-9a38-74186377bfbd | 400.002  | Service group controller-services loss of redundancy; expected 1 standby member but no standby members available         | service_domain=controller.service_group=controller-services                      | major    | 2020-06-11T05:38:42.369055 |
| d610eba4-ee57-4c56-a6f3-1ea57125c588 | 400.002  | Service group cloud-services loss of redundancy; expected 1 standby member but no standby members available              | service_domain=controller.service_group=cloud-services                           | major    | 2020-06-11T05:38:42.288081 |
| 0bbdcdea-a893-4793-a321-223e2e1ef0df | 400.002  | Service group vim-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=vim-services                             | major    | 2020-06-11T05:38:41.998063 |
| a5ca67b5-1fea-443b-8e0a-a8cb10996988 | 400.002  | Service group oam-services loss of redundancy; expected 1 standby member but no standby members available                | service_domain=controller.service_group=oam-services                             | major    | 2020-06-11T05:38:41.917044 |
| fc7eb0a0-382d-4c13-a5e9-a7b687cdc1bf | 400.002  | Service group patching-services loss of redundancy; expected 1 standby member but no standby members available           | service_domain=controller.service_group=patching-services                        | major    | 2020-06-11T05:38:41.593029 |
| 13e64eba-324f-451b-bdd8-ebcfe58e138c | 400.002  | Service group directory-services loss of redundancy; expected 2 active members but only 1 active member available        | service_domain=controller.service_group=directory-services                       | major    | 2020-06-11T05:38:41.511171 |
| e514b67e-376c-4dd4-84e7-50762920b93f | 400.002  | Service group web-services loss of redundancy; expected 2 active members but only 1 active member available              | service_domain=controller.service_group=web-services                             | major    | 2020-06-11T05:38:41.349065 |
| 8df3a0f6-417f-4791-a5bc-344171f76d39 | 400.002  | Service group storage-services loss of redundancy; expected 2 active members but only 1 active member available          | service_domain=controller.service_group=storage-services                         | major    | 2020-06-11T05:38:40.274305 |
| a049b0bf-2b49-4cc6-9300-f99ee9b1e931 | 400.002  | Service group storage-monitoring-services loss of redundancy; expected 1 standby member but no standby members available | service_domain=controller.service_group=storage-monitoring-services              | major    | 2020-06-11T05:38:40.111078 |
| 4e191807-46db-4157-9b14-550a7233a8a2 | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=mgmt                                                   | major    | 2020-06-11T05:38:38.883032 |
| 704d7cc8-f071-40a2-8e7e-c9635f658a62 | 400.005  | Communication failure detected with peer over port enp5s0 on host controller-0                                           | host=controller-0.network=oam                                                    | major    | 2020-06-11T05:38:38.719032 |
| 418716ed-a068-43cf-bc0e-2dd7b1b1dd8a | 400.005  | Communication failure detected with peer over port eno1 on host controller-0                                             | host=controller-0.network=cluster-host                                           | major    | 2020-06-11T05:38:38.471074 |
| 0d37b187-585e-4dab-b263-010ee6e07289 | 800.001  | Storage Alarm Condition: HEALTH_WARN [PGs are degraded/stuck or undersized]. Please check 'ceph -s' for more details.    | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a                                     | warning  | 2020-05-19T08:37:16.320718 |
| c89ecd3a-7956-4da5-aed5-bd1e3a3d388b | 800.011  | Loss of replication in replication group  group-0: no OSDs                                                               | cluster=0d357032-06df-4a4e-b11f-777a43c0bd6a.peergroup=group-0.host=controller-1 | major    | 2020-05-12T07:57:43.561426 |
+--------------------------------------+----------+--------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------+----------------------------+
controller-0:~$ 
[2020-06-11 05:50:25,738] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:25,840] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:25,840] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:25,840] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:25,841] 304  DEBUG MainThread ssh.send    :: Send 'system --os-username 'admin' --os-password '99cloud@SH' --os-project-name admin --os-auth-url http://192.168.104.1:5000/v3 --os-user-domain-name Default --os-project-domain-name Default  --os-endpoint-type internalURL --os-region-name RegionOne application-list'
[2020-06-11 05:50:26,699] 426  DEBUG MainThread ssh.expect  :: Output: 
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| application         | version | manifest name                 | manifest file    | status   | progress  |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
| platform-integ-apps | 1.0-8   | platform-integration-manifest | manifest.yaml    | applied  | completed |
| stx-monitor         | 1.0-1   | monitor-armada-manifest       | stx-monitor.yaml | uploaded | completed |
+---------------------+---------+-------------------------------+------------------+----------+-----------+
controller-0:~$ 
[2020-06-11 05:50:26,700] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:26,786] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:26,787] 755  INFO  MainThread kube_helper.wait_for_pods_healthy:: Wait for pods ready..
[2020-06-11 05:50:26,787] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:26,787] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:26,787] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase!=Running,status.phase!=Succeeded -o=wide'
[2020-06-11 05:50:26,964] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                              READY   STATUS             RESTARTS   AGE    IP             NODE       NOMINATED NODE   READINESS GATES
default       server-pod-dep-7fc68f8c47-85drg   0/1     ImagePullBackOff   0          2d3h   172.16.43.9    worker-0   <none>           <none>
default       server-pod-dep-7fc68f8c47-957dw   0/1     ImagePullBackOff   0          2d3h   172.16.43.10   worker-0   <none>           <none>
kube-system   coredns-6bc668cd76-vlc6w          0/1     Pending            0          47h    <none>         <none>     <none>           <none>
controller-0:~$ 
[2020-06-11 05:50:26,964] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:27,050] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:27,051] 773  INFO  MainThread kube_helper.wait_for_pods_healthy:: Pods are Completed or Running.
[2020-06-11 05:50:27,051] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:27,051] 469  DEBUG MainThread ssh.exec_cmd:: Executing command...
[2020-06-11 05:50:27,051] 304  DEBUG MainThread ssh.send    :: Send 'kubectl get pod --all-namespaces --field-selector=status.phase=Running -o=wide | grep --color=never -v 1/1'
[2020-06-11 05:50:27,221] 426  DEBUG MainThread ssh.expect  :: Output: 
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE    IP                NODE           NOMINATED NODE   READINESS GATES
controller-0:~$ 
[2020-06-11 05:50:27,221] 304  DEBUG MainThread ssh.send    :: Send 'echo $?'
[2020-06-11 05:50:27,309] 426  DEBUG MainThread ssh.expect  :: Output: 
0
controller-0:~$ 
[2020-06-11 05:50:27,310] 63   INFO  MainThread verify_fixtures.verify_ :: Check applications status...
[2020-06-11 05:50:27,316] 211  INFO  MainThread conftest.testcase_log:: 
-----------------------------------------------------------------
Test Result for: testcases/functional/z_containers/test_kube_system_services.py::test_kube_system_services[standby] - Test Skipped
Reason: Standby controller does not exist or not in good state

[2020-06-11 05:50:27,446] 799  DEBUG MainThread ssh.close   :: connection closed. host: forstarlingxtest, user: root. Object ID: 140181486312976
[2020-06-11 05:50:27,447] 512  INFO  MainThread conftest.pytest_unconfigure:: Test Results saved to: /root/AUTOMATION_LOGS/172.16.130.249/202006111330/test_results.log
[2020-06-11 05:50:27,447] 1540 DEBUG MainThread ssh.get_active_controller:: Getting active controller client for no_name
[2020-06-11 05:50:27,877] 799  DEBUG MainThread ssh.close   :: connection closed. host: 172.16.130.249, user: sysadmin. Object ID: 140181504263896
